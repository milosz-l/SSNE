{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Mainakdeb/Wine_Quality_Prediction/blob/master/Wine_Connoisseur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sX-tdVX9D9t3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from tqdm import tnrange\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MPS_GPU = False  # device whether to use MPS gpu if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connect to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.has_mps and USE_MPS_GPU:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "if torch.backends.mps.is_available() and USE_MPS_GPU:\n",
    "    torch.backends.mps.manual_seed(42)\n",
    "    torch.backends.mps.manual_seed_all(42)\n",
    "    \n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.mps.deterministic = True\n",
    "torch.backends.mps.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piTKw4F7tYra"
   },
   "source": [
    "### The Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "iIq7JlmpDmYb",
    "outputId": "6caf5c72-e226-448f-fa24-db89f72c996c"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', delimiter=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7DadPEKo6qg"
   },
   "source": [
    "### Convert all values into float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlIuuGRTDmE4"
   },
   "outputs": [],
   "source": [
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UwfkR4_pXcV"
   },
   "source": [
    "### Scale all values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJOYAEUTDmAb"
   },
   "outputs": [],
   "source": [
    "quality_backup = df[\"quality\"]\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n",
    "df_scaled['quality'] = quality_backup #restore quality values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LncrQoQJz68E"
   },
   "source": [
    "### Balance Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['quality'].value_counts().sort_index().plot(kind='bar', sort_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "HQ-ct2doz2v2",
    "outputId": "c0444ccf-9d7f-4dda-ed60-9a73cb70aacb"
   },
   "outputs": [],
   "source": [
    "# max_size = df_scaled['quality'].value_counts().max()\n",
    "# lst = [df_scaled]\n",
    "# for class_index, group in df_scaled.groupby('quality'):\n",
    "#     lst.append(group.sample(max_size-len(group), replace=True))\n",
    "# frame_new = pd.concat(lst)\n",
    "# df_scaled2=frame_new\n",
    "# df_scaled2[\"quality\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tDlYrJ0OpqRs"
   },
   "source": [
    "### Shuffle Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GeElgCspi1Z"
   },
   "outputs": [],
   "source": [
    "# df_scaled3=df_scaled2.sample(frac=1)\n",
    "# df_scaled3 = df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYSzFF6qpuak"
   },
   "source": [
    "### Split into train, test and val set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCqHM5XxDl9l"
   },
   "outputs": [],
   "source": [
    "train = df_scaled.iloc[:3686]\n",
    "val = df_scaled.iloc[3686:3886]\n",
    "test = df_scaled.iloc[3886:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=df.sample(frac=0.8,random_state=23) #random state is a seed value\n",
    "# test=df.drop(train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRPWhLGhrYOt"
   },
   "source": [
    "### Split features and labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CS5SFLwUDl5t",
    "outputId": "0b77d1da-aacb-47e3-d567-c944f138bdb2"
   },
   "outputs": [],
   "source": [
    "X_train, y_train= train.drop('quality', axis=1), train['quality']\n",
    "print(X_train.shape)\n",
    "\n",
    "X_val, y_val = val.drop('quality', axis=1), val['quality']\n",
    "print(X_val.shape)\n",
    "\n",
    "X_test, y_test = test.drop(\"quality\", axis=1), test[\"quality\"]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xU3PZCxgsNmC"
   },
   "source": [
    "### Split into batches :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AsLdat2eE2OO",
    "outputId": "7d958f9c-1709-4810-b1ff-f6ca9c53a247"
   },
   "outputs": [],
   "source": [
    "f=20 # no. of batches\n",
    "\n",
    "train_batch = np.array_split(X_train, f) \n",
    "label_batch = np.array_split(y_train, f) # 50 sections/batches\n",
    "\n",
    "val_batch = np.array_split(X_val, f)\n",
    "val_label_batch = np.array_split(y_val, f)\n",
    "\n",
    "test_batch = np.array_split(X_test,f) \n",
    "test_label_batch  = np.array_split(y_test, f)\n",
    "\n",
    "\n",
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "for i in range(len(val_batch)):\n",
    "    val_batch[i] = torch.from_numpy(val_batch[i].values).float()\n",
    "for i in range(len(val_label_batch)):\n",
    "    val_label_batch[i] = torch.from_numpy(val_label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "    \n",
    "for i in range(len(test_batch)):\n",
    "    test_batch[i] = torch.from_numpy(test_batch[i].values).float()\n",
    "for i in range(len(test_label_batch)):\n",
    "    test_label_batch[i] = torch.from_numpy(test_label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "print(\"Batch size:\", len(train_batch[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqMR4-U5sVcE"
   },
   "source": [
    "### The Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ml1BgIpxE2L9"
   },
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(11, 22)\n",
    "        self.fc2 = nn.Linear(22, 44)\n",
    "        self.fc3 = nn.Linear(44, 88)\n",
    "        self.fc4 = nn.Linear(88, 176)\n",
    "        self.fc5 = nn.Linear(176, 88)\n",
    "        self.fc6 = nn.Linear(88, 22)\n",
    "        self.fc7 = nn.Linear(22, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.20)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc7(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Regressor()\n",
    "train_losses, val_losses = [], []\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # 0.015 87\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1, patience=15) \n",
    "total_epochs=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ov0p9t03saO6"
   },
   "source": [
    "### The training loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = copy.deepcopy(model)\n",
    "best_val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvTIRas7E2Ku"
   },
   "outputs": [],
   "source": [
    "#print(model)\n",
    "epochs = 1000\n",
    "total_epochs+=epochs\n",
    "\n",
    "model.train()\n",
    "\n",
    "for e in tnrange(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss_1 = 0\n",
    "    val_loss_sum = 0\n",
    "    \n",
    "    for i in range(len(train_batch)):\n",
    "        \n",
    "        output = model(train_batch[i])\n",
    "        loss = criterion(output, label_batch[i])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for j in range(len(val_batch)):\n",
    "                \n",
    "                val_output = model(val_batch[j])\n",
    "                val_loss =  criterion(val_output, val_label_batch[j])\n",
    "                val_loss_1+=val_loss.item()\n",
    "        val_loss_sum=val_loss_1/len(val_batch)\n",
    "        \n",
    "    # saving best model\n",
    "    val_loss_divided = val_loss_sum/len(val_batch)\n",
    "    if best_val_loss is None or val_loss_divided < best_val_loss:\n",
    "        print('Model replaced')\n",
    "        best_val_loss = val_loss_divided\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(\"Epoch :\", e, \"train_loss :\", train_loss/len(train_batch), \"Val loss: \", val_loss_divided)    \n",
    "    val_losses.append(val_loss_divided)    \n",
    "    train_losses.append(train_loss/len(train_batch))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGxKLiV0suA8"
   },
   "source": [
    "### Training Metrics :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "g3uZMDqrE2Iu",
    "outputId": "b4191f91-8f2e-4ff4-d86b-b1b2b58dc15b"
   },
   "outputs": [],
   "source": [
    "frm=10 # does not \n",
    "plt.plot(train_losses[frm:], label='Training loss')\n",
    "plt.plot(val_losses[frm:], label='Validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdxctDz8tECj"
   },
   "source": [
    "### How does the model perform on the test-set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "correct=0\n",
    "i=0\n",
    "res=[]\n",
    "with torch.no_grad():\n",
    "    for batch in test_batch :\n",
    "        for j in range(len(batch)):\n",
    "            x = best_model(batch[j])\n",
    "            res.append(round(x.item()))\n",
    "\n",
    "true_labels = list(test[\"quality\"])\n",
    "\n",
    "for i in range(len(res)):\n",
    "    if res[i]==int(true_labels[i]):\n",
    "        correct+=1\n",
    "        \n",
    "print(\"Accuracy:\", 100 * (correct/len(res)), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model from last iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CRm4OS3IRq22",
    "outputId": "0a7b802e-e68a-4af0-dd6b-25a64bef8bfb"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct=0\n",
    "i=0\n",
    "res=[]\n",
    "with torch.no_grad():\n",
    "    for batch in test_batch :\n",
    "        for j in range(len(batch)):\n",
    "            x = model(batch[j])\n",
    "            #print(round(x.item()))\n",
    "            res.append(round(x.item()))\n",
    "\n",
    "true_labels= list(test[\"quality\"])\n",
    "\n",
    "for i in range(len(res)):\n",
    "    if res[i]==int(true_labels[i]):\n",
    "        correct+=1\n",
    "        \n",
    "print(\"Accuracy:\", 100*(correct/len(res)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Wine_Connoisseur.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a7aaad0630ad3230b7b955ab34166b9352cf35c53f01b298fd4177c5371ed1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
