{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Mainakdeb/Wine_Quality_Prediction/blob/master/Wine_Connoisseur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sX-tdVX9D9t3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MPS_GPU = False  # device whether to use MPS gpu if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connect to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'macOS-12.6-arm64-arm-64bit'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.has_mps and USE_MPS_GPU:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "if torch.backends.mps.is_available() and USE_MPS_GPU:\n",
    "    torch.backends.mps.manual_seed(42)\n",
    "    torch.backends.mps.manual_seed_all(42)\n",
    "    \n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.mps.deterministic = True\n",
    "torch.backends.mps.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piTKw4F7tYra"
   },
   "source": [
    "### The Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "iIq7JlmpDmYb",
    "outputId": "6caf5c72-e226-448f-fa24-db89f72c996c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', delimiter=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7DadPEKo6qg"
   },
   "source": [
    "### Convert all values into float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlIuuGRTDmE4"
   },
   "outputs": [],
   "source": [
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UwfkR4_pXcV"
   },
   "source": [
    "### Scale all values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJOYAEUTDmAb"
   },
   "outputs": [],
   "source": [
    "quality_backup = df[\"quality\"]\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n",
    "df_scaled['quality'] = quality_backup #restore quality values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.293730</td>\n",
       "      <td>0.194354</td>\n",
       "      <td>0.201320</td>\n",
       "      <td>0.088825</td>\n",
       "      <td>0.109117</td>\n",
       "      <td>0.116056</td>\n",
       "      <td>0.300141</td>\n",
       "      <td>0.133360</td>\n",
       "      <td>0.425697</td>\n",
       "      <td>0.313775</td>\n",
       "      <td>0.405527</td>\n",
       "      <td>5.877909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.081141</td>\n",
       "      <td>0.098818</td>\n",
       "      <td>0.072903</td>\n",
       "      <td>0.077792</td>\n",
       "      <td>0.064831</td>\n",
       "      <td>0.059258</td>\n",
       "      <td>0.098603</td>\n",
       "      <td>0.057662</td>\n",
       "      <td>0.137273</td>\n",
       "      <td>0.132704</td>\n",
       "      <td>0.198487</td>\n",
       "      <td>0.885639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.162651</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>0.080119</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.229698</td>\n",
       "      <td>0.088924</td>\n",
       "      <td>0.336364</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.070552</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.111498</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.127820</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.336538</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.234940</td>\n",
       "      <td>0.142638</td>\n",
       "      <td>0.121662</td>\n",
       "      <td>0.153310</td>\n",
       "      <td>0.366589</td>\n",
       "      <td>0.173318</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
       "mean        0.293730          0.194354     0.201320        0.088825   \n",
       "std         0.081141          0.098818     0.072903        0.077792   \n",
       "min         0.000000          0.000000     0.000000        0.000000   \n",
       "25%         0.240385          0.127451     0.162651        0.016871   \n",
       "50%         0.288462          0.176471     0.192771        0.070552   \n",
       "75%         0.336538          0.235294     0.234940        0.142638   \n",
       "max         1.000000          1.000000     1.000000        1.000000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
       "mean      0.109117             0.116056              0.300141     0.133360   \n",
       "std       0.064831             0.059258              0.098603     0.057662   \n",
       "min       0.000000             0.000000              0.000000     0.000000   \n",
       "25%       0.080119             0.073171              0.229698     0.088924   \n",
       "50%       0.100890             0.111498              0.290023     0.127820   \n",
       "75%       0.121662             0.153310              0.366589     0.173318   \n",
       "max       1.000000             1.000000              1.000000     1.000000   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
       "mean      0.425697     0.313775     0.405527     5.877909  \n",
       "std       0.137273     0.132704     0.198487     0.885639  \n",
       "min       0.000000     0.000000     0.000000     3.000000  \n",
       "25%       0.336364     0.220930     0.241935     5.000000  \n",
       "50%       0.418182     0.290698     0.387097     6.000000  \n",
       "75%       0.509091     0.383721     0.548387     6.000000  \n",
       "max       1.000000     1.000000     1.000000     9.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LncrQoQJz68E"
   },
   "source": [
    "### Balance Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['quality'].value_counts().sort_index().plot(kind='bar', sort_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "HQ-ct2doz2v2",
    "outputId": "c0444ccf-9d7f-4dda-ed60-9a73cb70aacb"
   },
   "outputs": [],
   "source": [
    "# max_size = df_scaled['quality'].value_counts().max()\n",
    "# lst = [df_scaled]\n",
    "# for class_index, group in df_scaled.groupby('quality'):\n",
    "#     lst.append(group.sample(max_size-len(group), replace=True))\n",
    "# frame_new = pd.concat(lst)\n",
    "# df_scaled2=frame_new\n",
    "# df_scaled2[\"quality\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tDlYrJ0OpqRs"
   },
   "source": [
    "### Shuffle Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GeElgCspi1Z"
   },
   "outputs": [],
   "source": [
    "# df_scaled3=df_scaled2.sample(frac=1)\n",
    "# df_scaled3 = df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYSzFF6qpuak"
   },
   "source": [
    "### Split into train, test and val set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCqHM5XxDl9l"
   },
   "outputs": [],
   "source": [
    "train = df_scaled.iloc[:3686]\n",
    "val = df_scaled.iloc[3686:3886]\n",
    "test = df_scaled.iloc[3886:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=df.sample(frac=0.8,random_state=23) #random state is a seed value\n",
    "# test=df.drop(train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRPWhLGhrYOt"
   },
   "source": [
    "### Split features and labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CS5SFLwUDl5t",
    "outputId": "0b77d1da-aacb-47e3-d567-c944f138bdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3686, 11)\n",
      "(200, 11)\n",
      "(1012, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train= train.drop('quality', axis=1), train['quality']\n",
    "print(X_train.shape)\n",
    "\n",
    "X_val, y_val = val.drop('quality', axis=1), val['quality']\n",
    "print(X_val.shape)\n",
    "\n",
    "X_test, y_test = test.drop(\"quality\", axis=1), test[\"quality\"]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xU3PZCxgsNmC"
   },
   "source": [
    "### Split into batches :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AsLdat2eE2OO",
    "outputId": "7d958f9c-1709-4810-b1ff-f6ca9c53a247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milosz/opt/miniconda3/envs/pt/lib/python3.9/site-packages/numpy/lib/shape_base.py:790: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  sub_arys.append(_nx.swapaxes(sary[st:end], axis, 0))\n"
     ]
    }
   ],
   "source": [
    "f=10 # no. of batches\n",
    "\n",
    "train_batch = np.array_split(X_train, f) \n",
    "label_batch = np.array_split(y_train, f) # 50 sections/batches\n",
    "\n",
    "val_batch = np.array_split(X_val, f)\n",
    "val_label_batch = np.array_split(y_val, f)\n",
    "\n",
    "test_batch = np.array_split(X_test,f) \n",
    "test_label_batch  = np.array_split(y_test, f)\n",
    "\n",
    "\n",
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "for i in range(len(val_batch)):\n",
    "    val_batch[i] = torch.from_numpy(val_batch[i].values).float()\n",
    "for i in range(len(val_label_batch)):\n",
    "    val_label_batch[i] = torch.from_numpy(val_label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "    \n",
    "for i in range(len(test_batch)):\n",
    "    test_batch[i] = torch.from_numpy(test_batch[i].values).float()\n",
    "for i in range(len(test_label_batch)):\n",
    "    test_label_batch[i] = torch.from_numpy(test_label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "print(\"Batch size:\", len(train_batch[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqMR4-U5sVcE"
   },
   "source": [
    "### The Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ml1BgIpxE2L9"
   },
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(11, 22)\n",
    "        self.fc2 = nn.Linear(22, 44)\n",
    "        self.fc3 = nn.Linear(44, 88)\n",
    "        self.fc4 = nn.Linear(88, 176)\n",
    "        self.fc5 = nn.Linear(176, 88)\n",
    "        self.fc6 = nn.Linear(88, 22)\n",
    "        self.fc7 = nn.Linear(22, 1)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Regressor()\n",
    "train_losses, val_losses = [], []\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # 0.015 87\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1, patience=15) \n",
    "total_epochs=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ov0p9t03saO6"
   },
   "source": [
    "### The training loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvTIRas7E2Ku"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v7/7b5vyv797kg9pf47h5zx_wxr0000gn/T/ipykernel_58292/1813283738.py:7: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for e in tnrange(epochs):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d5846486964428ad3d4ddf48a5be2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 train_loss : 34.19920139312744 Val loss:  31.963402061462403\n",
      "Epoch : 1 train_loss : 31.447653007507324 Val loss:  29.011281909942625\n",
      "Epoch : 2 train_loss : 22.393559646606445 Val loss:  19.35148220062256\n",
      "Epoch : 3 train_loss : 4.598309648036957 Val loss:  3.750403568446636\n",
      "Epoch : 4 train_loss : 2.757319176197052 Val loss:  2.1484723809361457\n",
      "Epoch : 5 train_loss : 1.6185725688934327 Val loss:  1.2372394162416458\n",
      "Epoch : 6 train_loss : 0.9862229764461518 Val loss:  0.8794218146800995\n",
      "Epoch : 7 train_loss : 0.9487866520881653 Val loss:  0.7311244583129882\n",
      "Epoch : 8 train_loss : 0.8596832573413848 Val loss:  0.6745230266451836\n",
      "Epoch : 9 train_loss : 0.8542646706104279 Val loss:  0.7026233765482902\n",
      "Epoch : 10 train_loss : 0.8155039966106414 Val loss:  0.6489502665400505\n",
      "Epoch : 11 train_loss : 0.798980051279068 Val loss:  0.6598306107521057\n",
      "Epoch : 12 train_loss : 0.7812564611434937 Val loss:  0.6419626861810684\n",
      "Epoch : 13 train_loss : 0.7631302058696747 Val loss:  0.637849018573761\n",
      "Epoch : 14 train_loss : 0.7479013621807098 Val loss:  0.6308388616144657\n",
      "Epoch : 15 train_loss : 0.7319481432437897 Val loss:  0.6254276420176029\n",
      "Epoch : 16 train_loss : 0.7181853830814362 Val loss:  0.621880489885807\n",
      "Epoch : 17 train_loss : 0.7050135850906372 Val loss:  0.6186445125937462\n",
      "Epoch : 18 train_loss : 0.6933945000171662 Val loss:  0.6169585099816322\n",
      "Epoch : 19 train_loss : 0.6829068839550019 Val loss:  0.6159070484340191\n",
      "Epoch : 20 train_loss : 0.6737573146820068 Val loss:  0.6158312387764454\n",
      "Epoch : 21 train_loss : 0.6658048152923584 Val loss:  0.6161804817616939\n",
      "Epoch : 22 train_loss : 0.658931416273117 Val loss:  0.6168397878110409\n",
      "Epoch : 23 train_loss : 0.6530395388603211 Val loss:  0.6175907230377197\n",
      "Epoch : 24 train_loss : 0.6479622006416321 Val loss:  0.6183342464268208\n",
      "Epoch : 25 train_loss : 0.6435742676258087 Val loss:  0.6189590531587601\n",
      "Epoch : 26 train_loss : 0.6397410750389099 Val loss:  0.6193422012031078\n",
      "Epoch : 27 train_loss : 0.6363291561603546 Val loss:  0.619345637857914\n",
      "Epoch : 28 train_loss : 0.6332626700401306 Val loss:  0.6190379102528095\n",
      "Epoch : 29 train_loss : 0.6305015802383422 Val loss:  0.6183727356791496\n",
      "Epoch : 30 train_loss : 0.6279186546802521 Val loss:  0.6173654344677926\n",
      "Epoch : 31 train_loss : 0.6255537748336792 Val loss:  0.6161331337690353\n",
      "Epoch : 32 train_loss : 0.6233798623085022 Val loss:  0.6148788201808929\n",
      "Epoch : 33 train_loss : 0.6213643252849579 Val loss:  0.6135780566930771\n",
      "Epoch : 34 train_loss : 0.6194910883903504 Val loss:  0.6123054717481137\n",
      "Epoch : 35 train_loss : 0.6177427262067795 Val loss:  0.6109019839763642\n",
      "Epoch : 36 train_loss : 0.6161456137895585 Val loss:  0.6096197178959846\n",
      "Epoch : 37 train_loss : 0.6145750701427459 Val loss:  0.6081081902980804\n",
      "Epoch : 38 train_loss : 0.6131316959857941 Val loss:  0.6067705534398555\n",
      "Epoch : 39 train_loss : 0.6117976933717728 Val loss:  0.6053956268727779\n",
      "Epoch : 40 train_loss : 0.6105730682611465 Val loss:  0.6042045581340789\n",
      "Epoch : 41 train_loss : 0.609444984793663 Val loss:  0.6031941191107035\n",
      "Epoch : 42 train_loss : 0.6084209978580475 Val loss:  0.6022515192627906\n",
      "Epoch : 43 train_loss : 0.6074274867773056 Val loss:  0.6014018351584672\n",
      "Epoch : 44 train_loss : 0.6065277814865112 Val loss:  0.600659207329154\n",
      "Epoch : 45 train_loss : 0.6056728303432465 Val loss:  0.5998635476827621\n",
      "Epoch : 46 train_loss : 0.6048939406871796 Val loss:  0.599212312027812\n",
      "Epoch : 47 train_loss : 0.6041961908340454 Val loss:  0.5985486152023076\n",
      "Epoch : 48 train_loss : 0.6035288780927658 Val loss:  0.5979368752986193\n",
      "Epoch : 49 train_loss : 0.6029377311468125 Val loss:  0.5973431849479676\n",
      "Epoch : 50 train_loss : 0.6023709595203399 Val loss:  0.5967124550044536\n",
      "Epoch : 51 train_loss : 0.6019125550985336 Val loss:  0.5963158410787582\n",
      "Epoch : 52 train_loss : 0.6014901518821716 Val loss:  0.595983005091548\n",
      "Epoch : 53 train_loss : 0.6010759741067886 Val loss:  0.5958000153303147\n",
      "Epoch : 54 train_loss : 0.6007513910531997 Val loss:  0.5957222212851048\n",
      "Epoch : 55 train_loss : 0.6004685670137405 Val loss:  0.5955639243125915\n",
      "Epoch : 56 train_loss : 0.6001351356506348 Val loss:  0.5951962244510651\n",
      "Epoch : 57 train_loss : 0.5998695671558381 Val loss:  0.5947810255736112\n",
      "Epoch : 58 train_loss : 0.5996549397706985 Val loss:  0.5943200346827507\n",
      "Epoch : 59 train_loss : 0.5994623839855194 Val loss:  0.5939253550767899\n",
      "Epoch : 60 train_loss : 0.5993278920650482 Val loss:  0.5936556774377822\n",
      "Epoch : 61 train_loss : 0.5992141246795655 Val loss:  0.5934282103180886\n",
      "Epoch : 62 train_loss : 0.5991204857826233 Val loss:  0.5931089704483747\n",
      "Epoch : 63 train_loss : 0.5989804804325104 Val loss:  0.5926763543486595\n",
      "Epoch : 64 train_loss : 0.5988555282354355 Val loss:  0.5923598232120275\n",
      "Epoch : 65 train_loss : 0.598705580830574 Val loss:  0.5919282785803079\n",
      "Epoch : 66 train_loss : 0.5985880255699157 Val loss:  0.5917060121893882\n",
      "Epoch : 67 train_loss : 0.5984180748462677 Val loss:  0.5910662995278836\n",
      "Epoch : 68 train_loss : 0.5983050763607025 Val loss:  0.5908657715469599\n",
      "Epoch : 69 train_loss : 0.59832464158535 Val loss:  0.5910709888488055\n",
      "Epoch : 70 train_loss : 0.5983985155820847 Val loss:  0.591377180442214\n",
      "Epoch : 71 train_loss : 0.5984859019517899 Val loss:  0.591575566828251\n",
      "Epoch : 72 train_loss : 0.59855976998806 Val loss:  0.5915907008200885\n",
      "Epoch : 73 train_loss : 0.598645493388176 Val loss:  0.5916755377501249\n",
      "Epoch : 74 train_loss : 0.598762035369873 Val loss:  0.5919453740119934\n",
      "Epoch : 75 train_loss : 0.598934817314148 Val loss:  0.5922433156520128\n",
      "Epoch : 76 train_loss : 0.5991254091262818 Val loss:  0.5921850890666247\n",
      "Epoch : 77 train_loss : 0.5991753906011581 Val loss:  0.5919561728835105\n",
      "Epoch : 78 train_loss : 0.5992239207029343 Val loss:  0.5917637064307928\n",
      "Epoch : 79 train_loss : 0.5993655830621719 Val loss:  0.5918494947999716\n",
      "Epoch : 80 train_loss : 0.5995756655931472 Val loss:  0.5918137878924609\n",
      "Epoch : 81 train_loss : 0.5996416002511978 Val loss:  0.5914785677939653\n",
      "Epoch : 82 train_loss : 0.5996622502803802 Val loss:  0.5913299368321896\n",
      "Epoch : 83 train_loss : 0.5996671199798584 Val loss:  0.5909663539379835\n",
      "Epoch : 84 train_loss : 0.5997145593166351 Val loss:  0.5908146777004004\n",
      "Epoch : 85 train_loss : 0.5999345064163208 Val loss:  0.5913380752503872\n",
      "Epoch : 86 train_loss : 0.6001213908195495 Val loss:  0.5916560467332601\n",
      "Epoch : 87 train_loss : 0.6001187294721604 Val loss:  0.5920740826427936\n",
      "Epoch : 88 train_loss : 0.6002413600683212 Val loss:  0.5928176663815975\n",
      "Epoch : 89 train_loss : 0.6004835575819015 Val loss:  0.5935551887005568\n",
      "Epoch : 90 train_loss : 0.6006679773330689 Val loss:  0.5940430617332458\n",
      "Epoch : 91 train_loss : 0.6007830917835235 Val loss:  0.5942738266289235\n",
      "Epoch : 92 train_loss : 0.6008387774229049 Val loss:  0.5943313956260681\n",
      "Epoch : 93 train_loss : 0.6008500456809998 Val loss:  0.5944794038683175\n",
      "Epoch : 94 train_loss : 0.6009126514196396 Val loss:  0.5948469592630863\n",
      "Epoch : 95 train_loss : 0.6010253220796585 Val loss:  0.5952533548325301\n",
      "Epoch : 96 train_loss : 0.6011136531829834 Val loss:  0.5955818293243647\n",
      "Epoch : 97 train_loss : 0.6011739283800125 Val loss:  0.5957282566279173\n",
      "Epoch : 98 train_loss : 0.6011597007513046 Val loss:  0.5956315185874701\n",
      "Epoch : 99 train_loss : 0.601137000322342 Val loss:  0.5956142602860928\n",
      "Epoch : 100 train_loss : 0.6011223316192627 Val loss:  0.5957006979733706\n",
      "Epoch : 101 train_loss : 0.6011846125125885 Val loss:  0.595889649912715\n",
      "Epoch : 102 train_loss : 0.6012765824794769 Val loss:  0.5961351311206817\n",
      "Epoch : 103 train_loss : 0.6013494819402695 Val loss:  0.5961577483266592\n",
      "Epoch : 104 train_loss : 0.6013097316026688 Val loss:  0.596044832020998\n",
      "Epoch : 105 train_loss : 0.6011502057313919 Val loss:  0.5958660291135311\n",
      "Epoch : 106 train_loss : 0.6010834276676178 Val loss:  0.5956983952224254\n",
      "Epoch : 107 train_loss : 0.6010261505842209 Val loss:  0.595595160573721\n",
      "Epoch : 108 train_loss : 0.6010056257247924 Val loss:  0.5953920784592628\n",
      "Epoch : 109 train_loss : 0.6009516805410385 Val loss:  0.5951902107894421\n",
      "Epoch : 110 train_loss : 0.6008697152137756 Val loss:  0.5951114542782306\n",
      "Epoch : 111 train_loss : 0.6008607685565949 Val loss:  0.5951721148192883\n",
      "Epoch : 112 train_loss : 0.6008557200431823 Val loss:  0.5952176165580749\n",
      "Epoch : 113 train_loss : 0.6008353412151337 Val loss:  0.5952673787623644\n",
      "Epoch : 114 train_loss : 0.6008038759231568 Val loss:  0.5952631383389234\n",
      "Epoch : 115 train_loss : 0.6007786989212036 Val loss:  0.5952396335452794\n",
      "Epoch : 116 train_loss : 0.6007704168558121 Val loss:  0.5952743852883577\n",
      "Epoch : 117 train_loss : 0.6007501870393753 Val loss:  0.5952980935573577\n",
      "Epoch : 118 train_loss : 0.6007462829351425 Val loss:  0.5953100608289242\n",
      "Epoch : 119 train_loss : 0.6006894201040268 Val loss:  0.5951248816400766\n",
      "Epoch : 120 train_loss : 0.6005668044090271 Val loss:  0.5949872756004334\n",
      "Epoch : 121 train_loss : 0.6005194842815399 Val loss:  0.5949089150130749\n",
      "Epoch : 122 train_loss : 0.600470119714737 Val loss:  0.5947326646000147\n",
      "Epoch : 123 train_loss : 0.6003812581300736 Val loss:  0.5945839107781649\n",
      "Epoch : 124 train_loss : 0.6003191888332366 Val loss:  0.5944879543036222\n",
      "Epoch : 125 train_loss : 0.6002945244312287 Val loss:  0.5944596429914236\n",
      "Epoch : 126 train_loss : 0.6002884358167648 Val loss:  0.5944511057436467\n",
      "Epoch : 127 train_loss : 0.6002037733793258 Val loss:  0.5942421354353428\n",
      "Epoch : 128 train_loss : 0.5998482197523117 Val loss:  0.5926293728500605\n",
      "Epoch : 129 train_loss : 0.6000660926103591 Val loss:  0.5953945702314376\n",
      "Epoch : 130 train_loss : 0.6010331511497498 Val loss:  0.5950762047618627\n",
      "Epoch : 131 train_loss : 0.6004167944192886 Val loss:  0.5939933332055807\n",
      "Epoch : 132 train_loss : 0.5996448367834091 Val loss:  0.5933896585553884\n",
      "Epoch : 133 train_loss : 0.5993991613388061 Val loss:  0.5933529035747052\n",
      "Epoch : 134 train_loss : 0.59947389960289 Val loss:  0.5934087730199098\n",
      "Epoch : 135 train_loss : 0.5993779838085175 Val loss:  0.592960748821497\n",
      "Epoch : 136 train_loss : 0.5990311861038208 Val loss:  0.5919797126948834\n",
      "Epoch : 137 train_loss : 0.5990543484687805 Val loss:  0.5937463140487671\n",
      "Epoch : 138 train_loss : 0.5997024744749069 Val loss:  0.5938800495862961\n",
      "Epoch : 139 train_loss : 0.5993892848491669 Val loss:  0.593095661252737\n",
      "Epoch : 140 train_loss : 0.5987346142530441 Val loss:  0.5923359863460064\n",
      "Epoch : 141 train_loss : 0.5985105246305465 Val loss:  0.5928893747925759\n",
      "Epoch : 142 train_loss : 0.5987255901098252 Val loss:  0.5929795604944229\n",
      "Epoch : 143 train_loss : 0.5986769884824753 Val loss:  0.5929290287196636\n",
      "Epoch : 144 train_loss : 0.5984659522771836 Val loss:  0.5926810280233622\n",
      "Epoch : 145 train_loss : 0.5981836020946503 Val loss:  0.5922427247464657\n",
      "Epoch : 146 train_loss : 0.5980643957853318 Val loss:  0.5925735712796449\n",
      "Epoch : 147 train_loss : 0.59809490442276 Val loss:  0.5923688881099224\n",
      "Epoch : 148 train_loss : 0.5979500859975815 Val loss:  0.5923673912882805\n",
      "Epoch : 149 train_loss : 0.5978270798921586 Val loss:  0.5922384588420391\n",
      "Epoch : 150 train_loss : 0.5974921554327011 Val loss:  0.591009272634983\n",
      "Epoch : 151 train_loss : 0.5974787563085556 Val loss:  0.5927141182869673\n",
      "Epoch : 152 train_loss : 0.598051768541336 Val loss:  0.5926134080439807\n",
      "Epoch : 153 train_loss : 0.5975410252809524 Val loss:  0.5915473802387714\n",
      "Epoch : 154 train_loss : 0.5966498494148255 Val loss:  0.5898084107786417\n",
      "Epoch : 155 train_loss : 0.596424663066864 Val loss:  0.5911407373100519\n",
      "Epoch : 156 train_loss : 0.5971963435411454 Val loss:  0.5920972445607185\n",
      "Epoch : 157 train_loss : 0.5972093552350998 Val loss:  0.59143136754632\n",
      "Epoch : 158 train_loss : 0.5965207815170288 Val loss:  0.5901674520969391\n",
      "Epoch : 159 train_loss : 0.5961498439311981 Val loss:  0.5909835752099752\n",
      "Epoch : 160 train_loss : 0.5963363617658615 Val loss:  0.5909031062573195\n",
      "Epoch : 161 train_loss : 0.5961300402879715 Val loss:  0.5904010331630707\n",
      "Epoch : 162 train_loss : 0.5957879960536957 Val loss:  0.5904612705856562\n",
      "Epoch : 163 train_loss : 0.5956918329000473 Val loss:  0.5903117977082729\n",
      "Epoch : 164 train_loss : 0.5955752760171891 Val loss:  0.5906791266798973\n",
      "Epoch : 165 train_loss : 0.5954156666994095 Val loss:  0.5898985192924738\n",
      "Epoch : 166 train_loss : 0.5952363640069962 Val loss:  0.5903032388538122\n",
      "Epoch : 167 train_loss : 0.5953659743070603 Val loss:  0.5903308387100696\n",
      "Epoch : 168 train_loss : 0.5950736314058304 Val loss:  0.5901824847608805\n",
      "Epoch : 169 train_loss : 0.5948587030172348 Val loss:  0.589855573028326\n",
      "Epoch : 170 train_loss : 0.5946711361408233 Val loss:  0.589603590965271\n",
      "Epoch : 171 train_loss : 0.5945567697286606 Val loss:  0.5900313524156809\n",
      "Epoch : 172 train_loss : 0.5945348858833313 Val loss:  0.5899170558154583\n",
      "Epoch : 173 train_loss : 0.5943623661994935 Val loss:  0.5898284742236137\n",
      "Epoch : 174 train_loss : 0.5941958904266358 Val loss:  0.589628328755498\n",
      "Epoch : 175 train_loss : 0.5940303564071655 Val loss:  0.5894215184450149\n",
      "Epoch : 176 train_loss : 0.5938974261283875 Val loss:  0.5893492410331964\n",
      "Epoch : 177 train_loss : 0.593706750869751 Val loss:  0.5892655967921019\n",
      "Epoch : 178 train_loss : 0.5935656368732453 Val loss:  0.58946683883667\n",
      "Epoch : 179 train_loss : 0.593459677696228 Val loss:  0.5896450603008271\n",
      "Epoch : 180 train_loss : 0.59341581761837 Val loss:  0.5891584654152393\n",
      "Epoch : 181 train_loss : 0.5931120097637177 Val loss:  0.5890499567240477\n",
      "Epoch : 182 train_loss : 0.5929398894309997 Val loss:  0.5889708965271712\n",
      "Epoch : 183 train_loss : 0.5927138984203338 Val loss:  0.5887969043850899\n",
      "Epoch : 184 train_loss : 0.5926266938447953 Val loss:  0.5885449150949716\n",
      "Epoch : 185 train_loss : 0.592342272400856 Val loss:  0.5885775783658028\n",
      "Epoch : 186 train_loss : 0.5923322916030884 Val loss:  0.5883398520201444\n",
      "Epoch : 187 train_loss : 0.5921160697937011 Val loss:  0.5883825689554214\n",
      "Epoch : 188 train_loss : 0.5919761061668396 Val loss:  0.587770554125309\n",
      "Epoch : 189 train_loss : 0.5916807264089584 Val loss:  0.5878390146046877\n",
      "Epoch : 190 train_loss : 0.5915415793657303 Val loss:  0.5873318300396204\n",
      "Epoch : 191 train_loss : 0.5913011997938156 Val loss:  0.5874509280174971\n",
      "Epoch : 192 train_loss : 0.5912374347448349 Val loss:  0.5871204856038094\n",
      "Epoch : 193 train_loss : 0.5909792929887772 Val loss:  0.586996049284935\n",
      "Epoch : 194 train_loss : 0.5908862262964248 Val loss:  0.5865294539928436\n",
      "Epoch : 195 train_loss : 0.5905316650867463 Val loss:  0.5866166248172522\n",
      "Epoch : 196 train_loss : 0.5904556721448898 Val loss:  0.5862222370505333\n",
      "Epoch : 197 train_loss : 0.5901952981948853 Val loss:  0.5862636414170266\n",
      "Epoch : 198 train_loss : 0.5900471895933151 Val loss:  0.58578027009964\n",
      "Epoch : 199 train_loss : 0.5896983802318573 Val loss:  0.58581972271204\n",
      "Epoch : 200 train_loss : 0.5895854711532593 Val loss:  0.5854007852077484\n",
      "Epoch : 201 train_loss : 0.5892683804035187 Val loss:  0.5855958168208599\n",
      "Epoch : 202 train_loss : 0.5891801685094833 Val loss:  0.5851180458068848\n",
      "Epoch : 203 train_loss : 0.5888736784458161 Val loss:  0.5852141498029232\n",
      "Epoch : 204 train_loss : 0.5886943578720093 Val loss:  0.5847097818553448\n",
      "Epoch : 205 train_loss : 0.5884545981884003 Val loss:  0.5850236339867114\n",
      "Epoch : 206 train_loss : 0.5883455574512482 Val loss:  0.5846341390907764\n",
      "Epoch : 207 train_loss : 0.587976050376892 Val loss:  0.5845152343809604\n",
      "Epoch : 208 train_loss : 0.5877343624830246 Val loss:  0.5835268245637417\n",
      "Epoch : 209 train_loss : 0.5874647945165634 Val loss:  0.5851296509802342\n",
      "Epoch : 210 train_loss : 0.5882422834634781 Val loss:  0.5852993392944337\n",
      "Epoch : 211 train_loss : 0.5878767281770706 Val loss:  0.5843435187637807\n",
      "Epoch : 212 train_loss : 0.5870229691267014 Val loss:  0.5844700354337692\n",
      "Epoch : 213 train_loss : 0.5870638281106949 Val loss:  0.5845069786906242\n",
      "Epoch : 214 train_loss : 0.5870729893445968 Val loss:  0.5855182334780693\n",
      "Epoch : 215 train_loss : 0.5873303771018982 Val loss:  0.5851949407160282\n",
      "Epoch : 216 train_loss : 0.5866763532161713 Val loss:  0.584590545296669\n",
      "Epoch : 217 train_loss : 0.5862527638673782 Val loss:  0.5839521174132825\n",
      "Epoch : 218 train_loss : 0.5859975785017013 Val loss:  0.584724268168211\n",
      "Epoch : 219 train_loss : 0.586145555973053 Val loss:  0.5844442743062973\n",
      "Epoch : 220 train_loss : 0.5857334583997726 Val loss:  0.5837785561382771\n",
      "Epoch : 221 train_loss : 0.5851919412612915 Val loss:  0.5838954778015614\n",
      "Epoch : 222 train_loss : 0.5853554397821427 Val loss:  0.5840429918467999\n",
      "Epoch : 223 train_loss : 0.5853478133678436 Val loss:  0.5840271194279194\n",
      "Epoch : 224 train_loss : 0.5850282490253449 Val loss:  0.5836303539574146\n",
      "Epoch : 225 train_loss : 0.5845549464225769 Val loss:  0.5831978242099285\n",
      "Epoch : 226 train_loss : 0.5840989828109742 Val loss:  0.5831600299477577\n",
      "Epoch : 227 train_loss : 0.5839827865362167 Val loss:  0.5830157135426999\n",
      "Epoch : 228 train_loss : 0.5839109510183335 Val loss:  0.5833890332281589\n",
      "Epoch : 229 train_loss : 0.5839679092168808 Val loss:  0.583111897855997\n",
      "Epoch : 230 train_loss : 0.5834850311279297 Val loss:  0.5829389813542366\n",
      "Epoch : 231 train_loss : 0.5833256036043167 Val loss:  0.5825846762955189\n",
      "Epoch : 232 train_loss : 0.583032238483429 Val loss:  0.5827908296883106\n",
      "Epoch : 233 train_loss : 0.5831246167421341 Val loss:  0.5827155588567257\n",
      "Epoch : 234 train_loss : 0.5827768325805665 Val loss:  0.5826524759829044\n",
      "Epoch : 235 train_loss : 0.5827134877443314 Val loss:  0.5826350009441376\n",
      "Epoch : 236 train_loss : 0.5824073791503906 Val loss:  0.5825490517914295\n",
      "Epoch : 237 train_loss : 0.5824120312929153 Val loss:  0.5824340993165971\n",
      "Epoch : 238 train_loss : 0.582137468457222 Val loss:  0.5823686619102955\n",
      "Epoch : 239 train_loss : 0.5820702999830246 Val loss:  0.5826758506894112\n",
      "Epoch : 240 train_loss : 0.5820055633783341 Val loss:  0.5822777907550335\n",
      "Epoch : 241 train_loss : 0.5814835518598557 Val loss:  0.5822889693081379\n",
      "Epoch : 242 train_loss : 0.5815422773361206 Val loss:  0.5822015836834907\n",
      "Epoch : 243 train_loss : 0.5811526864767075 Val loss:  0.5823793634772301\n",
      "Epoch : 244 train_loss : 0.5812038898468017 Val loss:  0.5821531386673451\n",
      "Epoch : 245 train_loss : 0.5810610532760621 Val loss:  0.5825653402507305\n",
      "Epoch : 246 train_loss : 0.5808422207832337 Val loss:  0.5820663252472877\n",
      "Epoch : 247 train_loss : 0.5804277211427689 Val loss:  0.5822962091863155\n",
      "Epoch : 248 train_loss : 0.5805559575557708 Val loss:  0.5822068178653718\n",
      "Epoch : 249 train_loss : 0.5802326679229737 Val loss:  0.5822203701734543\n",
      "Epoch : 250 train_loss : 0.5800152122974396 Val loss:  0.5819446916878224\n",
      "Epoch : 251 train_loss : 0.5795182168483735 Val loss:  0.5818434837460518\n",
      "Epoch : 252 train_loss : 0.5795443803071976 Val loss:  0.582544584274292\n",
      "Epoch : 253 train_loss : 0.5797294110059739 Val loss:  0.582144597619772\n",
      "Epoch : 254 train_loss : 0.5792611569166184 Val loss:  0.5825028502941132\n",
      "Epoch : 255 train_loss : 0.5794218748807907 Val loss:  0.581860534697771\n",
      "Epoch : 256 train_loss : 0.5788774758577346 Val loss:  0.5827368196845055\n",
      "Epoch : 257 train_loss : 0.5793232619762421 Val loss:  0.5819919057190418\n",
      "Epoch : 258 train_loss : 0.578956463932991 Val loss:  0.5825874020159245\n",
      "Epoch : 259 train_loss : 0.5788302928209305 Val loss:  0.581875231564045\n",
      "Epoch : 260 train_loss : 0.5782608062028884 Val loss:  0.5817662048339843\n",
      "Epoch : 261 train_loss : 0.5779745787382126 Val loss:  0.5810165300965309\n",
      "Epoch : 262 train_loss : 0.577621391415596 Val loss:  0.5817342598736286\n",
      "Epoch : 263 train_loss : 0.5780668795108795 Val loss:  0.5815562677383423\n",
      "Epoch : 264 train_loss : 0.5777914464473725 Val loss:  0.5815168139338494\n",
      "Epoch : 265 train_loss : 0.5773631602525711 Val loss:  0.5811177891492844\n",
      "Epoch : 266 train_loss : 0.5772095561027527 Val loss:  0.5810980333387852\n",
      "Epoch : 267 train_loss : 0.5767557591199874 Val loss:  0.5809521186351776\n",
      "Epoch : 268 train_loss : 0.5767589181661605 Val loss:  0.5811631739139557\n",
      "Epoch : 269 train_loss : 0.5763752937316895 Val loss:  0.5809128499031067\n",
      "Epoch : 270 train_loss : 0.5763352036476135 Val loss:  0.5809843601286412\n",
      "Epoch : 271 train_loss : 0.5763308078050613 Val loss:  0.5810907092690468\n",
      "Epoch : 272 train_loss : 0.5760719954967499 Val loss:  0.5810817570984363\n",
      "Epoch : 273 train_loss : 0.576140108704567 Val loss:  0.5809262047708035\n",
      "Epoch : 274 train_loss : 0.5756728827953339 Val loss:  0.5807708501815796\n",
      "Epoch : 275 train_loss : 0.5756640732288361 Val loss:  0.5805736437439919\n",
      "Epoch : 276 train_loss : 0.5753603965044022 Val loss:  0.5808619809150696\n",
      "Epoch : 277 train_loss : 0.575342521071434 Val loss:  0.5804922020435332\n",
      "Epoch : 278 train_loss : 0.5751868963241578 Val loss:  0.5806015440821648\n",
      "Epoch : 279 train_loss : 0.5751340299844742 Val loss:  0.5806522274017334\n",
      "Epoch : 280 train_loss : 0.5747746616601944 Val loss:  0.5806978479027748\n",
      "Epoch : 281 train_loss : 0.5749705463647843 Val loss:  0.5804036065936089\n",
      "Epoch : 282 train_loss : 0.5745211094617844 Val loss:  0.5807891376316547\n",
      "Epoch : 283 train_loss : 0.5746939599514007 Val loss:  0.5803375844657421\n",
      "Epoch : 284 train_loss : 0.5742013484239579 Val loss:  0.5805005131661891\n",
      "Epoch : 285 train_loss : 0.5745266765356064 Val loss:  0.5805930057168007\n",
      "Epoch : 286 train_loss : 0.5739329069852829 Val loss:  0.5806297263503074\n",
      "Epoch : 287 train_loss : 0.5742364317178726 Val loss:  0.5803396929800511\n",
      "Epoch : 288 train_loss : 0.5734653025865555 Val loss:  0.5804199519753456\n",
      "Epoch : 289 train_loss : 0.5736701935529709 Val loss:  0.5802969868481159\n",
      "Epoch : 290 train_loss : 0.5733858585357666 Val loss:  0.5804365220665931\n",
      "Epoch : 291 train_loss : 0.5735506325960159 Val loss:  0.5805247530341149\n",
      "Epoch : 292 train_loss : 0.5730863630771637 Val loss:  0.5801601448655129\n",
      "Epoch : 293 train_loss : 0.5730951428413391 Val loss:  0.5802351525425911\n",
      "Epoch : 294 train_loss : 0.5729132801294327 Val loss:  0.5804450023174286\n",
      "Epoch : 295 train_loss : 0.572904396057129 Val loss:  0.5802174372971057\n",
      "Epoch : 296 train_loss : 0.5726092368364334 Val loss:  0.5799583250284195\n",
      "Epoch : 297 train_loss : 0.5723198920488357 Val loss:  0.5799072512984276\n",
      "Epoch : 298 train_loss : 0.5722961068153382 Val loss:  0.5803416149318219\n",
      "Epoch : 299 train_loss : 0.5724578619003295 Val loss:  0.5803698506951332\n",
      "Epoch : 300 train_loss : 0.572121873497963 Val loss:  0.5800961536169053\n",
      "Epoch : 301 train_loss : 0.5720704287290573 Val loss:  0.5800765126943588\n",
      "Epoch : 302 train_loss : 0.571763563156128 Val loss:  0.580132815092802\n",
      "Epoch : 303 train_loss : 0.5715879052877426 Val loss:  0.5801323872804642\n",
      "Epoch : 304 train_loss : 0.57136450111866 Val loss:  0.5800071385502815\n",
      "Epoch : 305 train_loss : 0.5712636053562165 Val loss:  0.5803000685572625\n",
      "Epoch : 306 train_loss : 0.5710657805204391 Val loss:  0.5802506832778453\n",
      "Epoch : 307 train_loss : 0.5710837751626968 Val loss:  0.5803119330108165\n",
      "Epoch : 308 train_loss : 0.5707287579774857 Val loss:  0.5801862420141697\n",
      "Epoch : 309 train_loss : 0.570664843916893 Val loss:  0.5802758887410164\n",
      "Epoch : 310 train_loss : 0.5705328732728958 Val loss:  0.5802488309144974\n",
      "Epoch : 311 train_loss : 0.5704761266708374 Val loss:  0.580305024087429\n",
      "Epoch : 312 train_loss : 0.5703859716653824 Val loss:  0.5804200084507466\n",
      "Epoch : 313 train_loss : 0.5703382134437561 Val loss:  0.5799632261693477\n",
      "Epoch : 314 train_loss : 0.5702029585838317 Val loss:  0.5801143363118172\n",
      "Epoch : 315 train_loss : 0.5701670199632645 Val loss:  0.5802006515860557\n",
      "Epoch : 316 train_loss : 0.5698178797960282 Val loss:  0.5801897633075714\n",
      "Epoch : 317 train_loss : 0.5700277954339981 Val loss:  0.5801761108636856\n",
      "Epoch : 318 train_loss : 0.5695393413305283 Val loss:  0.5797969816625118\n",
      "Epoch : 319 train_loss : 0.5693914204835892 Val loss:  0.5802145911753177\n",
      "Epoch : 320 train_loss : 0.5694977074861527 Val loss:  0.5800526157021523\n",
      "Epoch : 321 train_loss : 0.5693010598421097 Val loss:  0.5801963372528552\n",
      "Epoch : 322 train_loss : 0.569143196940422 Val loss:  0.5800751362740993\n",
      "Epoch : 323 train_loss : 0.5691563010215759 Val loss:  0.5799988850951194\n",
      "Epoch : 324 train_loss : 0.5688870996236801 Val loss:  0.580526399165392\n",
      "Epoch : 325 train_loss : 0.5691029906272889 Val loss:  0.5800653676688671\n",
      "Epoch : 326 train_loss : 0.5684873759746552 Val loss:  0.5798736578226089\n",
      "Epoch : 327 train_loss : 0.5684396207332612 Val loss:  0.5802717462182045\n",
      "Epoch : 328 train_loss : 0.5685795575380326 Val loss:  0.5801517163217068\n",
      "Epoch : 329 train_loss : 0.5683415174484253 Val loss:  0.5800536406040192\n",
      "Epoch : 330 train_loss : 0.5680966436862945 Val loss:  0.5800911550223827\n",
      "Epoch : 331 train_loss : 0.5679142147302627 Val loss:  0.5800223955512047\n",
      "Epoch : 332 train_loss : 0.567905130982399 Val loss:  0.5800910863280296\n",
      "Epoch : 333 train_loss : 0.5680283814668655 Val loss:  0.5804731202125549\n",
      "Epoch : 334 train_loss : 0.5682375401258468 Val loss:  0.5803793770074844\n",
      "Epoch : 335 train_loss : 0.5675325900316238 Val loss:  0.5798388412594795\n",
      "Epoch : 336 train_loss : 0.5673323780298233 Val loss:  0.5802507638931275\n",
      "Epoch : 337 train_loss : 0.5674079596996308 Val loss:  0.5802496533095837\n",
      "Epoch : 338 train_loss : 0.5672804653644562 Val loss:  0.5805196531116963\n",
      "Epoch : 339 train_loss : 0.5670926511287689 Val loss:  0.5802864515781403\n",
      "Epoch : 340 train_loss : 0.566891485452652 Val loss:  0.5803404021263122\n",
      "Epoch : 341 train_loss : 0.566852667927742 Val loss:  0.5805568690598011\n",
      "Epoch : 342 train_loss : 0.5666814148426056 Val loss:  0.5804627975821495\n",
      "Epoch : 343 train_loss : 0.5666601121425628 Val loss:  0.5806273436546325\n",
      "Epoch : 344 train_loss : 0.5665250658988953 Val loss:  0.5808737653493881\n",
      "Epoch : 345 train_loss : 0.5665254086256027 Val loss:  0.5807006713747979\n",
      "Epoch : 346 train_loss : 0.5664951026439666 Val loss:  0.5812301675975322\n",
      "Epoch : 347 train_loss : 0.566726216673851 Val loss:  0.5812504579126835\n",
      "Epoch : 348 train_loss : 0.5659337431192398 Val loss:  0.5805863526463508\n",
      "Epoch : 349 train_loss : 0.5663626343011856 Val loss:  0.5814433851838112\n",
      "Epoch : 350 train_loss : 0.5667041361331939 Val loss:  0.5815326803922654\n",
      "Epoch : 351 train_loss : 0.565656241774559 Val loss:  0.5812524293363095\n",
      "Epoch : 352 train_loss : 0.565852016210556 Val loss:  0.5816436068713665\n",
      "Epoch : 353 train_loss : 0.5660731643438339 Val loss:  0.5812024007737636\n",
      "Epoch : 354 train_loss : 0.5660040080547333 Val loss:  0.5820146818459034\n",
      "Epoch : 355 train_loss : 0.5664204508066177 Val loss:  0.5811550265550613\n",
      "Epoch : 356 train_loss : 0.5655727028846741 Val loss:  0.5816937293112278\n",
      "Epoch : 357 train_loss : 0.5658548146486282 Val loss:  0.5815075540542602\n",
      "Epoch : 358 train_loss : 0.5647054135799408 Val loss:  0.5814832128584385\n",
      "Epoch : 359 train_loss : 0.5648268073797226 Val loss:  0.5817271772027015\n",
      "Epoch : 360 train_loss : 0.5651083916425705 Val loss:  0.5821167662739754\n",
      "Epoch : 361 train_loss : 0.5647086501121521 Val loss:  0.5818596845865249\n",
      "Epoch : 362 train_loss : 0.5645861387252807 Val loss:  0.5824472305178643\n",
      "Epoch : 363 train_loss : 0.5648940324783325 Val loss:  0.5820562569797039\n",
      "Epoch : 364 train_loss : 0.5645615130662918 Val loss:  0.5818090748786926\n",
      "Epoch : 365 train_loss : 0.5640420854091645 Val loss:  0.5810445103049278\n",
      "Epoch : 366 train_loss : 0.5638294130563736 Val loss:  0.582359376102686\n",
      "Epoch : 367 train_loss : 0.564790153503418 Val loss:  0.5823438657820225\n",
      "Epoch : 368 train_loss : 0.5639025688171386 Val loss:  0.5817572376132011\n",
      "Epoch : 369 train_loss : 0.5635498851537705 Val loss:  0.5821892859041691\n",
      "Epoch : 370 train_loss : 0.5637888312339783 Val loss:  0.5826704348623752\n",
      "Epoch : 371 train_loss : 0.5638168960809707 Val loss:  0.5822449085116387\n",
      "Epoch : 372 train_loss : 0.5635760933160782 Val loss:  0.5819147151708604\n",
      "Epoch : 373 train_loss : 0.5629032135009766 Val loss:  0.5818900048732758\n",
      "Epoch : 374 train_loss : 0.5630829423666001 Val loss:  0.5821647496521474\n",
      "Epoch : 375 train_loss : 0.5630968660116196 Val loss:  0.5820073303580284\n",
      "Epoch : 376 train_loss : 0.562695762515068 Val loss:  0.581765605211258\n",
      "Epoch : 377 train_loss : 0.56282639503479 Val loss:  0.5822999280691147\n",
      "Epoch : 378 train_loss : 0.5628758877515793 Val loss:  0.5824014826118946\n",
      "Epoch : 379 train_loss : 0.5628151386976242 Val loss:  0.5821723926067353\n",
      "Epoch : 380 train_loss : 0.5625218987464905 Val loss:  0.5818854346871376\n",
      "Epoch : 381 train_loss : 0.5621503382921219 Val loss:  0.5819572012126446\n",
      "Epoch : 382 train_loss : 0.5625208407640457 Val loss:  0.5829163019359112\n",
      "Epoch : 383 train_loss : 0.5626952439546585 Val loss:  0.5822072832286358\n",
      "Epoch : 384 train_loss : 0.5617916494607925 Val loss:  0.5824699668586254\n",
      "Epoch : 385 train_loss : 0.5623848289251328 Val loss:  0.5822966392338276\n",
      "Epoch : 386 train_loss : 0.5617439448833466 Val loss:  0.5823072747886181\n",
      "Epoch : 387 train_loss : 0.5616972774267197 Val loss:  0.5824390876293182\n",
      "Epoch : 388 train_loss : 0.5616736948490143 Val loss:  0.5828811338543891\n",
      "Epoch : 389 train_loss : 0.561590701341629 Val loss:  0.5824298571050167\n",
      "Epoch : 390 train_loss : 0.5614882469177246 Val loss:  0.5826042313873768\n",
      "Epoch : 391 train_loss : 0.5611725360155105 Val loss:  0.5826276002824307\n",
      "Epoch : 392 train_loss : 0.5613671898841858 Val loss:  0.5828279721736908\n",
      "Epoch : 393 train_loss : 0.5611144572496414 Val loss:  0.5825848744809627\n",
      "Epoch : 394 train_loss : 0.5610881090164185 Val loss:  0.5832209658622742\n",
      "Epoch : 395 train_loss : 0.5611681133508682 Val loss:  0.5824219438433647\n",
      "Epoch : 396 train_loss : 0.5608507126569748 Val loss:  0.5824596467614174\n",
      "Epoch : 397 train_loss : 0.5606327086687088 Val loss:  0.5817212490737438\n",
      "Epoch : 398 train_loss : 0.5606697529554368 Val loss:  0.5836405234038831\n",
      "Epoch : 399 train_loss : 0.5615626305341721 Val loss:  0.5838481472432614\n",
      "Epoch : 400 train_loss : 0.561696857213974 Val loss:  0.5836847276985646\n",
      "Epoch : 401 train_loss : 0.5606628745794296 Val loss:  0.5817085734009743\n",
      "Epoch : 402 train_loss : 0.559766098856926 Val loss:  0.5829584033787251\n",
      "Epoch : 403 train_loss : 0.5607721596956253 Val loss:  0.5829203769564628\n",
      "Epoch : 404 train_loss : 0.5602153569459916 Val loss:  0.5826629799604416\n",
      "Epoch : 405 train_loss : 0.5596530050039291 Val loss:  0.5820403622090817\n",
      "Epoch : 406 train_loss : 0.5598062694072723 Val loss:  0.5829824931919575\n",
      "Epoch : 407 train_loss : 0.5603985577821732 Val loss:  0.5830691252648831\n",
      "Epoch : 408 train_loss : 0.5610026448965073 Val loss:  0.5820330162346363\n",
      "Epoch : 409 train_loss : 0.5587860256433487 Val loss:  0.5811856289207935\n",
      "Epoch : 410 train_loss : 0.5586322605609894 Val loss:  0.5828393974900246\n",
      "Epoch : 411 train_loss : 0.5599851936101914 Val loss:  0.5828640399873256\n",
      "Epoch : 412 train_loss : 0.5594087600708008 Val loss:  0.5828031040728092\n",
      "Epoch : 413 train_loss : 0.5590934574604034 Val loss:  0.5819402879476547\n",
      "Epoch : 414 train_loss : 0.5590291917324066 Val loss:  0.5829889865219593\n",
      "Epoch : 415 train_loss : 0.5591846376657486 Val loss:  0.582536780834198\n",
      "Epoch : 416 train_loss : 0.5591473549604415 Val loss:  0.5822295448184013\n",
      "Epoch : 417 train_loss : 0.5584017753601074 Val loss:  0.58120745241642\n",
      "Epoch : 418 train_loss : 0.5581255912780761 Val loss:  0.5834600453078747\n",
      "Epoch : 419 train_loss : 0.5599701225757598 Val loss:  0.5826914359629154\n",
      "Epoch : 420 train_loss : 0.5582396268844605 Val loss:  0.5817411275207997\n",
      "Epoch : 421 train_loss : 0.5582067370414734 Val loss:  0.5830672559142113\n",
      "Epoch : 422 train_loss : 0.5591671943664551 Val loss:  0.5825766623020172\n",
      "Epoch : 423 train_loss : 0.55776187479496 Val loss:  0.581486060321331\n",
      "Epoch : 424 train_loss : 0.5574309140443802 Val loss:  0.582555132508278\n",
      "Epoch : 425 train_loss : 0.5582807689905167 Val loss:  0.582191188186407\n",
      "Epoch : 426 train_loss : 0.557710587978363 Val loss:  0.5823626962304116\n",
      "Epoch : 427 train_loss : 0.5577197819948196 Val loss:  0.5816959457099438\n",
      "Epoch : 428 train_loss : 0.557299742102623 Val loss:  0.5825509905815125\n",
      "Epoch : 429 train_loss : 0.5580840289592743 Val loss:  0.5831974723935127\n",
      "Epoch : 430 train_loss : 0.5577693849802017 Val loss:  0.5827268946170807\n",
      "Epoch : 431 train_loss : 0.5571267306804657 Val loss:  0.5816641671955585\n",
      "Epoch : 432 train_loss : 0.5572144776582718 Val loss:  0.5830487680435181\n",
      "Epoch : 433 train_loss : 0.5576913625001907 Val loss:  0.5826079766452312\n",
      "Epoch : 434 train_loss : 0.5568040192127228 Val loss:  0.5831162831187248\n",
      "Epoch : 435 train_loss : 0.5569991797208786 Val loss:  0.5826711311936379\n",
      "Epoch : 436 train_loss : 0.5566214382648468 Val loss:  0.5828282944858074\n",
      "Epoch : 437 train_loss : 0.556681501865387 Val loss:  0.5817520672082901\n",
      "Epoch : 438 train_loss : 0.5567627996206284 Val loss:  0.5847554847598075\n",
      "Epoch : 439 train_loss : 0.5585976839065552 Val loss:  0.5833049872517586\n",
      "Epoch : 440 train_loss : 0.556005883216858 Val loss:  0.5826693207025528\n",
      "Epoch : 441 train_loss : 0.5558699339628219 Val loss:  0.5827699139714241\n",
      "Epoch : 442 train_loss : 0.5558633178472518 Val loss:  0.583190131187439\n",
      "Epoch : 443 train_loss : 0.5559558510780335 Val loss:  0.583162539601326\n",
      "Epoch : 444 train_loss : 0.5560807198286056 Val loss:  0.5826905871927738\n",
      "Epoch : 445 train_loss : 0.5554730117321014 Val loss:  0.5837355460226535\n",
      "Epoch : 446 train_loss : 0.5564800053834915 Val loss:  0.5832668378949165\n",
      "Epoch : 447 train_loss : 0.5551945626735687 Val loss:  0.5831974332034588\n",
      "Epoch : 448 train_loss : 0.5548730283975601 Val loss:  0.583228739053011\n",
      "Epoch : 449 train_loss : 0.5553122758865356 Val loss:  0.5822062775492668\n",
      "Epoch : 450 train_loss : 0.5547396570444107 Val loss:  0.5842237158119679\n",
      "Epoch : 451 train_loss : 0.5560152381658554 Val loss:  0.5845169146358967\n",
      "Epoch : 452 train_loss : 0.5554346799850464 Val loss:  0.5832162667810917\n",
      "Epoch : 453 train_loss : 0.553912353515625 Val loss:  0.5822838899493218\n",
      "Epoch : 454 train_loss : 0.5543987721204757 Val loss:  0.5850441005825997\n",
      "Epoch : 455 train_loss : 0.5559442609548568 Val loss:  0.5835643619298935\n",
      "Epoch : 456 train_loss : 0.5544654250144958 Val loss:  0.5834348896145821\n",
      "Epoch : 457 train_loss : 0.5535221248865128 Val loss:  0.5827505107223987\n",
      "Epoch : 458 train_loss : 0.5539716601371765 Val loss:  0.5840775142610073\n",
      "Epoch : 459 train_loss : 0.5541916817426682 Val loss:  0.5826337815821171\n",
      "Epoch : 460 train_loss : 0.5535814225673675 Val loss:  0.5849264144897461\n",
      "Epoch : 461 train_loss : 0.5549299985170364 Val loss:  0.5838861030340194\n",
      "Epoch : 462 train_loss : 0.5539042264223099 Val loss:  0.5819719617068768\n",
      "Epoch : 463 train_loss : 0.5531693249940872 Val loss:  0.5845243339240551\n",
      "Epoch : 464 train_loss : 0.5544203788042068 Val loss:  0.583339858353138\n",
      "Epoch : 465 train_loss : 0.5532926559448242 Val loss:  0.583060025125742\n",
      "Epoch : 466 train_loss : 0.5527692466974259 Val loss:  0.5821621197462081\n",
      "Epoch : 467 train_loss : 0.553077119588852 Val loss:  0.5830800335109234\n",
      "Epoch : 468 train_loss : 0.5533408999443055 Val loss:  0.5829028561711311\n",
      "Epoch : 469 train_loss : 0.5527103960514068 Val loss:  0.5838374184072018\n",
      "Epoch : 470 train_loss : 0.5528501659631729 Val loss:  0.5829167518019676\n",
      "Epoch : 471 train_loss : 0.5529050946235656 Val loss:  0.5828319638967514\n",
      "Epoch : 472 train_loss : 0.552443215250969 Val loss:  0.5823687878251076\n",
      "Epoch : 473 train_loss : 0.5521601140499115 Val loss:  0.5820943948626518\n",
      "Epoch : 474 train_loss : 0.551901775598526 Val loss:  0.5828299468755722\n",
      "Epoch : 475 train_loss : 0.5524806380271912 Val loss:  0.5828664991259574\n",
      "Epoch : 476 train_loss : 0.5518383771181107 Val loss:  0.5819739681482315\n",
      "Epoch : 477 train_loss : 0.5512696325778961 Val loss:  0.5851403646171093\n",
      "Epoch : 478 train_loss : 0.5534450858831406 Val loss:  0.5828254356980324\n",
      "Epoch : 479 train_loss : 0.5511260837316513 Val loss:  0.5816118013858795\n",
      "Epoch : 480 train_loss : 0.5501780718564987 Val loss:  0.5820661298930645\n",
      "Epoch : 481 train_loss : 0.5507841646671295 Val loss:  0.5835343462228775\n",
      "Epoch : 482 train_loss : 0.5514947742223739 Val loss:  0.583357790261507\n",
      "Epoch : 483 train_loss : 0.5511436522006988 Val loss:  0.5840447568893433\n",
      "Epoch : 484 train_loss : 0.5511943638324738 Val loss:  0.5821463014185428\n",
      "Epoch : 485 train_loss : 0.5501917183399201 Val loss:  0.5822455203533172\n",
      "Epoch : 486 train_loss : 0.5499665796756744 Val loss:  0.5834403042495251\n",
      "Epoch : 487 train_loss : 0.5508663356304169 Val loss:  0.5829623931646347\n",
      "Epoch : 488 train_loss : 0.5504793971776962 Val loss:  0.5826346069574356\n",
      "Epoch : 489 train_loss : 0.5496792316436767 Val loss:  0.5830815030634403\n",
      "Epoch : 490 train_loss : 0.550215557217598 Val loss:  0.5827822385728358\n",
      "Epoch : 491 train_loss : 0.5504300713539123 Val loss:  0.5837685830891133\n",
      "Epoch : 492 train_loss : 0.5503001242876053 Val loss:  0.582280383259058\n",
      "Epoch : 493 train_loss : 0.5493914633989334 Val loss:  0.5813969515264035\n",
      "Epoch : 494 train_loss : 0.5486697763204574 Val loss:  0.5822714646160603\n",
      "Epoch : 495 train_loss : 0.5493320316076279 Val loss:  0.581853574514389\n",
      "Epoch : 496 train_loss : 0.5493388593196868 Val loss:  0.5813956850767136\n",
      "Epoch : 497 train_loss : 0.5486170142889023 Val loss:  0.5825001513957977\n",
      "Epoch : 498 train_loss : 0.5489410161972046 Val loss:  0.5829899652302265\n",
      "Epoch : 499 train_loss : 0.5493878573179245 Val loss:  0.5829482117295266\n",
      "Epoch : 500 train_loss : 0.5493282705545426 Val loss:  0.5816879714280366\n",
      "Epoch : 501 train_loss : 0.5481578409671783 Val loss:  0.58089668802917\n",
      "Epoch : 502 train_loss : 0.5477191984653473 Val loss:  0.581675426363945\n",
      "Epoch : 503 train_loss : 0.5481422364711761 Val loss:  0.5818404868990183\n",
      "Epoch : 504 train_loss : 0.5483014971017838 Val loss:  0.5829281140863896\n",
      "Epoch : 505 train_loss : 0.5487882614135742 Val loss:  0.5826248887181282\n",
      "Epoch : 506 train_loss : 0.5483702987432479 Val loss:  0.5818450225889682\n",
      "Epoch : 507 train_loss : 0.5482075870037079 Val loss:  0.5819166678190231\n",
      "Epoch : 508 train_loss : 0.5483865141868591 Val loss:  0.5802277752012015\n",
      "Epoch : 509 train_loss : 0.5471820414066315 Val loss:  0.580627510547638\n",
      "Epoch : 510 train_loss : 0.5476173907518387 Val loss:  0.5812426573783159\n",
      "Epoch : 511 train_loss : 0.5475966691970825 Val loss:  0.5811874954402446\n",
      "Epoch : 512 train_loss : 0.547448393702507 Val loss:  0.5807425072044134\n",
      "Epoch : 513 train_loss : 0.5474064826965332 Val loss:  0.5809408225119114\n",
      "Epoch : 514 train_loss : 0.5471526741981506 Val loss:  0.5813292223960161\n",
      "Epoch : 515 train_loss : 0.5470658719539643 Val loss:  0.5805362701416016\n",
      "Epoch : 516 train_loss : 0.547102278470993 Val loss:  0.5808476748317479\n",
      "Epoch : 517 train_loss : 0.5470386356115341 Val loss:  0.5804989387840033\n",
      "Epoch : 518 train_loss : 0.5468185752630234 Val loss:  0.5800584691017866\n",
      "Epoch : 519 train_loss : 0.5465894848108291 Val loss:  0.5802084333449602\n",
      "Epoch : 520 train_loss : 0.5464731693267822 Val loss:  0.5793870104849338\n",
      "Epoch : 521 train_loss : 0.5460422784090042 Val loss:  0.5795004939287901\n",
      "Epoch : 522 train_loss : 0.5460186064243316 Val loss:  0.5806695982068777\n",
      "Epoch : 523 train_loss : 0.5473729491233825 Val loss:  0.5808607595413923\n",
      "Epoch : 524 train_loss : 0.5468460530042648 Val loss:  0.5789569719135761\n",
      "Epoch : 525 train_loss : 0.5459491044282914 Val loss:  0.5798746306449175\n",
      "Epoch : 526 train_loss : 0.5461917042732238 Val loss:  0.58002875238657\n",
      "Epoch : 527 train_loss : 0.5463390678167344 Val loss:  0.5792012283205986\n",
      "Epoch : 528 train_loss : 0.5459689319133758 Val loss:  0.5795729388296604\n",
      "Epoch : 529 train_loss : 0.5460015505552291 Val loss:  0.5790493090450763\n",
      "Epoch : 530 train_loss : 0.5456813305616379 Val loss:  0.5778155067563058\n",
      "Epoch : 531 train_loss : 0.5446885645389556 Val loss:  0.579011669754982\n",
      "Epoch : 532 train_loss : 0.5455323368310928 Val loss:  0.5788899238407612\n",
      "Epoch : 533 train_loss : 0.5458152770996094 Val loss:  0.5781878089904785\n",
      "Epoch : 534 train_loss : 0.5449642598628998 Val loss:  0.5798071741312742\n",
      "Epoch : 535 train_loss : 0.5458364188671112 Val loss:  0.5785421796888113\n",
      "Epoch : 536 train_loss : 0.5454231262207031 Val loss:  0.5779100402444601\n",
      "Epoch : 537 train_loss : 0.5441238552331924 Val loss:  0.5777963353693485\n",
      "Epoch : 538 train_loss : 0.544201722741127 Val loss:  0.57779128074646\n",
      "Epoch : 539 train_loss : 0.5448386639356613 Val loss:  0.5788452979922295\n",
      "Epoch : 540 train_loss : 0.5455165803432465 Val loss:  0.5784123525768519\n",
      "Epoch : 541 train_loss : 0.5448208451271057 Val loss:  0.577814171910286\n",
      "Epoch : 542 train_loss : 0.5450115293264389 Val loss:  0.5765689468383789\n",
      "Epoch : 543 train_loss : 0.5433322012424469 Val loss:  0.5769773814082145\n",
      "Epoch : 544 train_loss : 0.5444767892360687 Val loss:  0.5779683006554842\n",
      "Epoch : 545 train_loss : 0.5442799359560013 Val loss:  0.5754033749550581\n",
      "Epoch : 546 train_loss : 0.5433210462331772 Val loss:  0.5773241014033557\n",
      "Epoch : 547 train_loss : 0.54504134953022 Val loss:  0.5766271548718214\n",
      "Epoch : 548 train_loss : 0.5435973644256592 Val loss:  0.5756937935948372\n",
      "Epoch : 549 train_loss : 0.5432035773992538 Val loss:  0.5758469185233116\n",
      "Epoch : 550 train_loss : 0.5429782569408417 Val loss:  0.5745027782768011\n",
      "Epoch : 551 train_loss : 0.5430088400840759 Val loss:  0.5745210854709148\n",
      "Epoch : 552 train_loss : 0.5428753823041916 Val loss:  0.5732626120001078\n",
      "Epoch : 553 train_loss : 0.5422982633113861 Val loss:  0.5735847720503807\n",
      "Epoch : 554 train_loss : 0.5420998364686966 Val loss:  0.57360230602324\n",
      "Epoch : 555 train_loss : 0.5428656309843063 Val loss:  0.5742856457084418\n",
      "Epoch : 556 train_loss : 0.5430758625268937 Val loss:  0.573135318160057\n",
      "Epoch : 557 train_loss : 0.5416039437055588 Val loss:  0.5721262077987194\n",
      "Epoch : 558 train_loss : 0.5410563200712204 Val loss:  0.5735195145010948\n",
      "Epoch : 559 train_loss : 0.5427828103303909 Val loss:  0.5733530213683844\n",
      "Epoch : 560 train_loss : 0.5423408448696136 Val loss:  0.5722715818881989\n",
      "Epoch : 561 train_loss : 0.5413082510232925 Val loss:  0.5718744353950024\n",
      "Epoch : 562 train_loss : 0.541203236579895 Val loss:  0.5711723764240741\n",
      "Epoch : 563 train_loss : 0.5408679991960526 Val loss:  0.5722373018413782\n",
      "Epoch : 564 train_loss : 0.5416640788316727 Val loss:  0.5718426568806171\n",
      "Epoch : 565 train_loss : 0.5408166587352753 Val loss:  0.5707927999645472\n",
      "Epoch : 566 train_loss : 0.5407132148742676 Val loss:  0.5717829286307097\n",
      "Epoch : 567 train_loss : 0.5419550985097885 Val loss:  0.5711301057040691\n",
      "Epoch : 568 train_loss : 0.540598201751709 Val loss:  0.5700308300554753\n",
      "Epoch : 569 train_loss : 0.5399948239326477 Val loss:  0.5716570743173361\n",
      "Epoch : 570 train_loss : 0.5414085060358047 Val loss:  0.5696709963679314\n",
      "Epoch : 571 train_loss : 0.5398883581161499 Val loss:  0.5698100204020738\n",
      "Epoch : 572 train_loss : 0.540039575099945 Val loss:  0.5707542426884175\n",
      "Epoch : 573 train_loss : 0.5413118779659272 Val loss:  0.57030128210783\n",
      "Epoch : 574 train_loss : 0.5401769310235978 Val loss:  0.5681164070218802\n",
      "Epoch : 575 train_loss : 0.5387648195028305 Val loss:  0.569553650021553\n",
      "Epoch : 576 train_loss : 0.5400230795145035 Val loss:  0.5700069279223681\n",
      "Epoch : 577 train_loss : 0.5396934270858764 Val loss:  0.5686551452428102\n",
      "Epoch : 578 train_loss : 0.539075282216072 Val loss:  0.5689089769124985\n",
      "Epoch : 579 train_loss : 0.5395297408103943 Val loss:  0.5685898640751839\n",
      "Epoch : 580 train_loss : 0.539422744512558 Val loss:  0.5690173727273942\n",
      "Epoch : 581 train_loss : 0.5389767944812774 Val loss:  0.5676876028627158\n",
      "Epoch : 582 train_loss : 0.5385492026805878 Val loss:  0.5689271656423808\n",
      "Epoch : 583 train_loss : 0.5393532693386078 Val loss:  0.56846764549613\n",
      "Epoch : 584 train_loss : 0.5397266268730163 Val loss:  0.5685830240696669\n",
      "Epoch : 585 train_loss : 0.538092628121376 Val loss:  0.5672372882068157\n",
      "Epoch : 586 train_loss : 0.5386603385210037 Val loss:  0.5681562926620245\n",
      "Epoch : 587 train_loss : 0.5402614533901214 Val loss:  0.5689277628064155\n",
      "Epoch : 588 train_loss : 0.5385416120290756 Val loss:  0.5669614341855049\n",
      "Epoch : 589 train_loss : 0.5370672911405563 Val loss:  0.5685067074745893\n",
      "Epoch : 590 train_loss : 0.5382085382938385 Val loss:  0.5688102350383997\n",
      "Epoch : 591 train_loss : 0.5399808943271637 Val loss:  0.5688674647361041\n",
      "Epoch : 592 train_loss : 0.5379469126462937 Val loss:  0.5664060097187757\n",
      "Epoch : 593 train_loss : 0.5371645539999008 Val loss:  0.5683021433651447\n",
      "Epoch : 594 train_loss : 0.538372614979744 Val loss:  0.568512666001916\n",
      "Epoch : 595 train_loss : 0.5377910166978837 Val loss:  0.567724844366312\n",
      "Epoch : 596 train_loss : 0.5380299150943756 Val loss:  0.5677605099976063\n",
      "Epoch : 597 train_loss : 0.5377230942249298 Val loss:  0.567184432297945\n",
      "Epoch : 598 train_loss : 0.5370605140924454 Val loss:  0.5678570325672626\n",
      "Epoch : 599 train_loss : 0.537521955370903 Val loss:  0.5679846363514661\n",
      "Epoch : 600 train_loss : 0.5387398213148117 Val loss:  0.568918029293418\n",
      "Epoch : 601 train_loss : 0.5375505775213242 Val loss:  0.566768281981349\n",
      "Epoch : 602 train_loss : 0.5370431959629058 Val loss:  0.5679243355989456\n",
      "Epoch : 603 train_loss : 0.5368738949298859 Val loss:  0.5662602639943362\n",
      "Epoch : 604 train_loss : 0.5359073609113694 Val loss:  0.567287866100669\n",
      "Epoch : 605 train_loss : 0.5368590295314789 Val loss:  0.56810995914042\n",
      "Epoch : 606 train_loss : 0.5384891241788864 Val loss:  0.5687090431153774\n",
      "Epoch : 607 train_loss : 0.5370601058006287 Val loss:  0.5647825030982494\n",
      "Epoch : 608 train_loss : 0.5356029659509659 Val loss:  0.5666104704141617\n",
      "Epoch : 609 train_loss : 0.5355109632015228 Val loss:  0.5662517443299293\n",
      "Epoch : 610 train_loss : 0.5352411717176437 Val loss:  0.5669094171375036\n",
      "Epoch : 611 train_loss : 0.5371646165847779 Val loss:  0.5691875754296779\n",
      "Epoch : 612 train_loss : 0.5379024296998978 Val loss:  0.5672301311790944\n",
      "Epoch : 613 train_loss : 0.5355786144733429 Val loss:  0.5649941916018724\n",
      "Epoch : 614 train_loss : 0.534367498755455 Val loss:  0.5663871742784977\n",
      "Epoch : 615 train_loss : 0.535415130853653 Val loss:  0.5666714829206467\n",
      "Epoch : 616 train_loss : 0.5354287832975387 Val loss:  0.5668903545290231\n",
      "Epoch : 617 train_loss : 0.5353735715150834 Val loss:  0.5664365365356206\n",
      "Epoch : 618 train_loss : 0.5347318381071091 Val loss:  0.5659971771389246\n",
      "Epoch : 619 train_loss : 0.5348895728588104 Val loss:  0.5660925775021315\n",
      "Epoch : 620 train_loss : 0.5350555270910263 Val loss:  0.5659481269866229\n",
      "Epoch : 621 train_loss : 0.534550940990448 Val loss:  0.5661433043330908\n",
      "Epoch : 622 train_loss : 0.5343814700841903 Val loss:  0.5655463288724423\n",
      "Epoch : 623 train_loss : 0.5342260241508484 Val loss:  0.5661006872355938\n",
      "Epoch : 624 train_loss : 0.534891602396965 Val loss:  0.5660983606427907\n",
      "Epoch : 625 train_loss : 0.534979322552681 Val loss:  0.5653565958887339\n",
      "Epoch : 626 train_loss : 0.5334362208843231 Val loss:  0.5641439059376717\n",
      "Epoch : 627 train_loss : 0.5334025174379349 Val loss:  0.5646790499985218\n",
      "Epoch : 628 train_loss : 0.5336788922548295 Val loss:  0.5652081679552794\n",
      "Epoch : 629 train_loss : 0.5337868243455887 Val loss:  0.5653578466176986\n",
      "Epoch : 630 train_loss : 0.5340420186519623 Val loss:  0.5654324214905501\n",
      "Epoch : 631 train_loss : 0.5339954316616058 Val loss:  0.5641796647012234\n",
      "Epoch : 632 train_loss : 0.5326590061187744 Val loss:  0.5633738921582699\n",
      "Epoch : 633 train_loss : 0.5322644233703613 Val loss:  0.5645290214568377\n",
      "Epoch : 634 train_loss : 0.5336299568414689 Val loss:  0.5667318780720234\n",
      "Epoch : 635 train_loss : 0.5350879818201065 Val loss:  0.5667549664527177\n",
      "Epoch : 636 train_loss : 0.5351717203855515 Val loss:  0.5635650159418584\n",
      "Epoch : 637 train_loss : 0.5314626634120941 Val loss:  0.5617194425314664\n",
      "Epoch : 638 train_loss : 0.5314120888710022 Val loss:  0.563900083526969\n",
      "Epoch : 639 train_loss : 0.5326966166496276 Val loss:  0.5646121244877577\n",
      "Epoch : 640 train_loss : 0.5325607597827912 Val loss:  0.5631030387431383\n",
      "Epoch : 641 train_loss : 0.5315559089183808 Val loss:  0.564508104994893\n",
      "Epoch : 642 train_loss : 0.5325928062200547 Val loss:  0.5658489515632391\n",
      "Epoch : 643 train_loss : 0.53475501537323 Val loss:  0.5650611754506827\n",
      "Epoch : 644 train_loss : 0.5318114489316941 Val loss:  0.5603047410398722\n",
      "Epoch : 645 train_loss : 0.5304702401161194 Val loss:  0.5623371088504792\n",
      "Epoch : 646 train_loss : 0.5312545627355576 Val loss:  0.5633875384181738\n",
      "Epoch : 647 train_loss : 0.532040399312973 Val loss:  0.5630509459227324\n",
      "Epoch : 648 train_loss : 0.5313213050365448 Val loss:  0.5625380040705205\n",
      "Epoch : 649 train_loss : 0.5309796154499054 Val loss:  0.5626641923934221\n",
      "Epoch : 650 train_loss : 0.5316754817962647 Val loss:  0.5636032292991876\n",
      "Epoch : 651 train_loss : 0.5312859117984772 Val loss:  0.5621215625852346\n",
      "Epoch : 652 train_loss : 0.5307569533586503 Val loss:  0.561748198568821\n",
      "Epoch : 653 train_loss : 0.5306098133325576 Val loss:  0.5613792425394057\n",
      "Epoch : 654 train_loss : 0.5304213285446167 Val loss:  0.5619324228912592\n",
      "Epoch : 655 train_loss : 0.530611002445221 Val loss:  0.5617039356380701\n",
      "Epoch : 656 train_loss : 0.5302049398422242 Val loss:  0.5616677758097649\n",
      "Epoch : 657 train_loss : 0.5303688198328018 Val loss:  0.5616187069565057\n",
      "Epoch : 658 train_loss : 0.5305966377258301 Val loss:  0.5612932384759188\n",
      "Epoch : 659 train_loss : 0.5301874220371247 Val loss:  0.561578845307231\n",
      "Epoch : 660 train_loss : 0.5299104034900666 Val loss:  0.5595330270379781\n",
      "Epoch : 661 train_loss : 0.5291885673999787 Val loss:  0.5599119371920824\n",
      "Epoch : 662 train_loss : 0.5293612778186798 Val loss:  0.5611511621624231\n",
      "Epoch : 663 train_loss : 0.5297222942113876 Val loss:  0.5612286069244147\n",
      "Epoch : 664 train_loss : 0.5309382021427155 Val loss:  0.560236989930272\n",
      "Epoch : 665 train_loss : 0.5299365639686584 Val loss:  0.5599830522388219\n",
      "Epoch : 666 train_loss : 0.5286894291639328 Val loss:  0.5576562243700027\n",
      "Epoch : 667 train_loss : 0.5278801381587982 Val loss:  0.560749026313424\n",
      "Epoch : 668 train_loss : 0.5295455068349838 Val loss:  0.5603830006718635\n",
      "Epoch : 669 train_loss : 0.5296819567680359 Val loss:  0.5593969932943582\n",
      "Epoch : 670 train_loss : 0.5287454515695572 Val loss:  0.5593819058686494\n",
      "Epoch : 671 train_loss : 0.5281630694866181 Val loss:  0.5597594714909792\n",
      "Epoch : 672 train_loss : 0.5290266543626785 Val loss:  0.5609132286161185\n",
      "Epoch : 673 train_loss : 0.5297766596078872 Val loss:  0.560622231066227\n",
      "Epoch : 674 train_loss : 0.528739932179451 Val loss:  0.5585725776106119\n",
      "Epoch : 675 train_loss : 0.5270970016717911 Val loss:  0.557452719733119\n",
      "Epoch : 676 train_loss : 0.5271872371435166 Val loss:  0.5597736056894064\n",
      "Epoch : 677 train_loss : 0.5292899310588837 Val loss:  0.5598951596021652\n",
      "Epoch : 678 train_loss : 0.5275602072477341 Val loss:  0.5574311131983996\n",
      "Epoch : 679 train_loss : 0.5267445653676986 Val loss:  0.5587914434075356\n",
      "Epoch : 680 train_loss : 0.5272630661725998 Val loss:  0.5581030713021755\n",
      "Epoch : 681 train_loss : 0.5280694782733917 Val loss:  0.558664588034153\n",
      "Epoch : 682 train_loss : 0.5271266520023346 Val loss:  0.5571984819322824\n",
      "Epoch : 683 train_loss : 0.5257676810026168 Val loss:  0.5572676477581263\n",
      "Epoch : 684 train_loss : 0.5264344662427902 Val loss:  0.5575934125483035\n",
      "Epoch : 685 train_loss : 0.5270754367113113 Val loss:  0.5584682124108076\n",
      "Epoch : 686 train_loss : 0.527155065536499 Val loss:  0.5567722278833389\n",
      "Epoch : 687 train_loss : 0.5256412178277969 Val loss:  0.5566877404600381\n",
      "Epoch : 688 train_loss : 0.5258597195148468 Val loss:  0.5560970032960176\n",
      "Epoch : 689 train_loss : 0.5260126382112503 Val loss:  0.5572966554760933\n",
      "Epoch : 690 train_loss : 0.5261316895484924 Val loss:  0.556803015768528\n",
      "Epoch : 691 train_loss : 0.5253678292036057 Val loss:  0.5568480136245488\n",
      "Epoch : 692 train_loss : 0.5251465409994125 Val loss:  0.5557536496222019\n",
      "Epoch : 693 train_loss : 0.5257014214992524 Val loss:  0.5585044427216053\n",
      "Epoch : 694 train_loss : 0.5273228704929351 Val loss:  0.5581583743542433\n",
      "Epoch : 695 train_loss : 0.526106882095337 Val loss:  0.5551388481259346\n",
      "Epoch : 696 train_loss : 0.5239100903272629 Val loss:  0.5544728174060582\n",
      "Epoch : 697 train_loss : 0.523289829492569 Val loss:  0.5552835700660944\n",
      "Epoch : 698 train_loss : 0.5244716823101043 Val loss:  0.5573227947205306\n",
      "Epoch : 699 train_loss : 0.5266291797161102 Val loss:  0.5579089445620775\n",
      "Epoch : 700 train_loss : 0.5263593286275864 Val loss:  0.5545187996327877\n",
      "Epoch : 701 train_loss : 0.5237348139286041 Val loss:  0.555950379371643\n",
      "Epoch : 702 train_loss : 0.5240845620632172 Val loss:  0.5552491118013859\n",
      "Epoch : 703 train_loss : 0.5243114054203033 Val loss:  0.5572798565030098\n",
      "Epoch : 704 train_loss : 0.5256149470806122 Val loss:  0.5562635994702577\n",
      "Epoch : 705 train_loss : 0.5248412638902664 Val loss:  0.5542284438759089\n",
      "Epoch : 706 train_loss : 0.523019090294838 Val loss:  0.5544378263503313\n",
      "Epoch : 707 train_loss : 0.5229697972536087 Val loss:  0.5544341155886651\n",
      "Epoch : 708 train_loss : 0.5234473794698715 Val loss:  0.5547506085783243\n",
      "Epoch : 709 train_loss : 0.5231743901968002 Val loss:  0.5554440841823817\n",
      "Epoch : 710 train_loss : 0.5229711920022965 Val loss:  0.5542569870501757\n",
      "Epoch : 711 train_loss : 0.5227203786373138 Val loss:  0.5549027923494577\n",
      "Epoch : 712 train_loss : 0.5228884190320968 Val loss:  0.5548287577927112\n",
      "Epoch : 713 train_loss : 0.5223657220602036 Val loss:  0.5541496085375547\n",
      "Epoch : 714 train_loss : 0.523040896654129 Val loss:  0.5553004693239927\n",
      "Epoch : 715 train_loss : 0.523311710357666 Val loss:  0.5541004491597414\n",
      "Epoch : 716 train_loss : 0.5221812099218368 Val loss:  0.5534383389353752\n",
      "Epoch : 717 train_loss : 0.5213276326656342 Val loss:  0.5531172811985016\n",
      "Epoch : 718 train_loss : 0.521748161315918 Val loss:  0.5551011484116316\n",
      "Epoch : 719 train_loss : 0.5238216698169709 Val loss:  0.5548771239817143\n",
      "Epoch : 720 train_loss : 0.5228965371847153 Val loss:  0.5536374672502279\n",
      "Epoch : 721 train_loss : 0.5207773178815842 Val loss:  0.5513598405569792\n",
      "Epoch : 722 train_loss : 0.5194006562232971 Val loss:  0.5535630648583174\n",
      "Epoch : 723 train_loss : 0.5214506089687347 Val loss:  0.5544944839179516\n",
      "Epoch : 724 train_loss : 0.5230647146701812 Val loss:  0.5546203444898128\n",
      "Epoch : 725 train_loss : 0.5216227024793625 Val loss:  0.5517922804504634\n",
      "Epoch : 726 train_loss : 0.5197767347097397 Val loss:  0.5521796499192715\n",
      "Epoch : 727 train_loss : 0.5203222274780274 Val loss:  0.5525889825820923\n",
      "Epoch : 728 train_loss : 0.5204540252685547 Val loss:  0.553441588357091\n",
      "Epoch : 729 train_loss : 0.5214092403650283 Val loss:  0.5534631517529488\n",
      "Epoch : 730 train_loss : 0.5209004163742066 Val loss:  0.5527075024694204\n",
      "Epoch : 731 train_loss : 0.5208445638418198 Val loss:  0.552105124220252\n",
      "Epoch : 732 train_loss : 0.5199241697788238 Val loss:  0.5522465787827968\n",
      "Epoch : 733 train_loss : 0.5194628685712814 Val loss:  0.5515441002696753\n",
      "Epoch : 734 train_loss : 0.519478651881218 Val loss:  0.5515845000743866\n",
      "Epoch : 735 train_loss : 0.5196760803461075 Val loss:  0.5525027932971716\n",
      "Epoch : 736 train_loss : 0.519828787446022 Val loss:  0.5505810785293579\n",
      "Epoch : 737 train_loss : 0.5184638887643814 Val loss:  0.5515203885734081\n",
      "Epoch : 738 train_loss : 0.5189473778009415 Val loss:  0.5515051069110631\n",
      "Epoch : 739 train_loss : 0.5191489785909653 Val loss:  0.5518999698013067\n",
      "Epoch : 740 train_loss : 0.5198257565498352 Val loss:  0.5520477607101202\n",
      "Epoch : 741 train_loss : 0.5200764238834381 Val loss:  0.550912994965911\n",
      "Epoch : 742 train_loss : 0.5184304237365722 Val loss:  0.5493582431972027\n",
      "Epoch : 743 train_loss : 0.5170080959796906 Val loss:  0.5485252361744642\n",
      "Epoch : 744 train_loss : 0.5162816733121872 Val loss:  0.5501671381294727\n",
      "Epoch : 745 train_loss : 0.5182148784399032 Val loss:  0.5502536076307296\n",
      "Epoch : 746 train_loss : 0.5180340677499771 Val loss:  0.5501086090505123\n",
      "Epoch : 747 train_loss : 0.5178718864917755 Val loss:  0.5501520413905382\n",
      "Epoch : 748 train_loss : 0.5174771517515182 Val loss:  0.5502130418270826\n",
      "Epoch : 749 train_loss : 0.5178749412298203 Val loss:  0.5493073745071888\n",
      "Epoch : 750 train_loss : 0.5161958426237107 Val loss:  0.547746265605092\n",
      "Epoch : 751 train_loss : 0.5154935568571091 Val loss:  0.5480214918404818\n",
      "Epoch : 752 train_loss : 0.5156045764684677 Val loss:  0.5489118684083223\n",
      "Epoch : 753 train_loss : 0.5165212392807007 Val loss:  0.5495783004164696\n",
      "Epoch : 754 train_loss : 0.5178434908390045 Val loss:  0.5514859947562217\n",
      "Epoch : 755 train_loss : 0.517880865931511 Val loss:  0.5498182103037834\n",
      "Epoch : 756 train_loss : 0.5166800379753113 Val loss:  0.5474599149078131\n",
      "Epoch : 757 train_loss : 0.5151081800460815 Val loss:  0.5480496857315302\n",
      "Epoch : 758 train_loss : 0.5159750282764435 Val loss:  0.5464912053197623\n",
      "Epoch : 759 train_loss : 0.5139343410730361 Val loss:  0.5456330598890782\n",
      "Epoch : 760 train_loss : 0.5143076181411743 Val loss:  0.546389922350645\n",
      "Epoch : 761 train_loss : 0.5139985680580139 Val loss:  0.5464004049450158\n",
      "Epoch : 762 train_loss : 0.5142609030008316 Val loss:  0.5460902583599091\n",
      "Epoch : 763 train_loss : 0.5137711465358734 Val loss:  0.5469755478948355\n",
      "Epoch : 764 train_loss : 0.5154598325490951 Val loss:  0.5469259478151798\n",
      "Epoch : 765 train_loss : 0.514396607875824 Val loss:  0.5444038350135088\n",
      "Epoch : 766 train_loss : 0.5131792187690735 Val loss:  0.5439301836490631\n",
      "Epoch : 767 train_loss : 0.5122743725776673 Val loss:  0.5452422000467777\n",
      "Epoch : 768 train_loss : 0.5134961068630218 Val loss:  0.5451419015228749\n",
      "Epoch : 769 train_loss : 0.5133544951677322 Val loss:  0.5453641539812087\n",
      "Epoch : 770 train_loss : 0.513889342546463 Val loss:  0.5445670403540135\n",
      "Epoch : 771 train_loss : 0.5118750512599946 Val loss:  0.5434035037457943\n",
      "Epoch : 772 train_loss : 0.5125386893749238 Val loss:  0.5448278684169054\n",
      "Epoch : 773 train_loss : 0.5130540877580643 Val loss:  0.5453892878443003\n",
      "Epoch : 774 train_loss : 0.513557282090187 Val loss:  0.5437621287256479\n",
      "Epoch : 775 train_loss : 0.5114512354135513 Val loss:  0.5420141217112542\n",
      "Epoch : 776 train_loss : 0.511051332950592 Val loss:  0.5437673184275627\n",
      "Epoch : 777 train_loss : 0.511423996090889 Val loss:  0.5437882711738349\n",
      "Epoch : 778 train_loss : 0.511960044503212 Val loss:  0.5442293708771467\n",
      "Epoch : 779 train_loss : 0.5120663851499557 Val loss:  0.5457358325272799\n",
      "Epoch : 780 train_loss : 0.5133320957422256 Val loss:  0.5435188496857881\n",
      "Epoch : 781 train_loss : 0.5103685945272446 Val loss:  0.5408815865218639\n",
      "Epoch : 782 train_loss : 0.5102570712566376 Val loss:  0.5449895354360341\n",
      "Epoch : 783 train_loss : 0.511174526810646 Val loss:  0.5417027634382248\n",
      "Epoch : 784 train_loss : 0.5101723819971085 Val loss:  0.5443682914972305\n",
      "Epoch : 785 train_loss : 0.5107622236013413 Val loss:  0.5437642271071672\n",
      "Epoch : 786 train_loss : 0.5107533097267151 Val loss:  0.5445289810746908\n",
      "Epoch : 787 train_loss : 0.5112766981124878 Val loss:  0.5447734444588422\n",
      "Epoch : 788 train_loss : 0.5103688359260559 Val loss:  0.5422481241822242\n",
      "Epoch : 789 train_loss : 0.5093258380889892 Val loss:  0.5432633898407221\n",
      "Epoch : 790 train_loss : 0.5098105013370514 Val loss:  0.5423599827289581\n",
      "Epoch : 791 train_loss : 0.5077227562665939 Val loss:  0.5416794510185718\n",
      "Epoch : 792 train_loss : 0.5083233505487442 Val loss:  0.5451074646413326\n",
      "Epoch : 793 train_loss : 0.510509318113327 Val loss:  0.5429775716364384\n",
      "Epoch : 794 train_loss : 0.5092863410711288 Val loss:  0.543383722230792\n",
      "Epoch : 795 train_loss : 0.5085004270076752 Val loss:  0.5414919696003199\n",
      "Epoch : 796 train_loss : 0.5073346316814422 Val loss:  0.5447971115261316\n",
      "Epoch : 797 train_loss : 0.5097936570644379 Val loss:  0.5443338228762149\n",
      "Epoch : 798 train_loss : 0.5082147687673568 Val loss:  0.5443311718851328\n",
      "Epoch : 799 train_loss : 0.5093252301216126 Val loss:  0.5437930800765753\n",
      "Epoch : 800 train_loss : 0.5071049422025681 Val loss:  0.541014469563961\n",
      "Epoch : 801 train_loss : 0.5066198259592056 Val loss:  0.5407855151593685\n",
      "Epoch : 802 train_loss : 0.50719433426857 Val loss:  0.5455062145739793\n",
      "Epoch : 803 train_loss : 0.5093752473592759 Val loss:  0.5466722519695759\n",
      "Epoch : 804 train_loss : 0.5087295830249786 Val loss:  0.5440031208097935\n",
      "Epoch : 805 train_loss : 0.508328166604042 Val loss:  0.5442163827270269\n",
      "Epoch : 806 train_loss : 0.507834005355835 Val loss:  0.5423445177078248\n",
      "Epoch : 807 train_loss : 0.5069569885730744 Val loss:  0.5414487806707621\n",
      "Epoch : 808 train_loss : 0.5059610843658447 Val loss:  0.5398955371975899\n",
      "Epoch : 809 train_loss : 0.5035666286945343 Val loss:  0.5382573713362218\n",
      "Epoch : 810 train_loss : 0.5033968299627304 Val loss:  0.5427695109695196\n",
      "Epoch : 811 train_loss : 0.5059055745601654 Val loss:  0.5445228044688701\n",
      "Epoch : 812 train_loss : 0.5071384310722351 Val loss:  0.5442894288897515\n",
      "Epoch : 813 train_loss : 0.5068536698818207 Val loss:  0.542090672403574\n",
      "Epoch : 814 train_loss : 0.5043552368879318 Val loss:  0.5403979089856148\n",
      "Epoch : 815 train_loss : 0.5039076507091522 Val loss:  0.5404974702000618\n",
      "Epoch : 816 train_loss : 0.503679284453392 Val loss:  0.5410178568214178\n",
      "Epoch : 817 train_loss : 0.5037558794021606 Val loss:  0.5419889871776105\n",
      "Epoch : 818 train_loss : 0.5041697561740875 Val loss:  0.5423737818747758\n",
      "Epoch : 819 train_loss : 0.5041209250688553 Val loss:  0.5422337347269058\n",
      "Epoch : 820 train_loss : 0.5043202877044678 Val loss:  0.5430469714850188\n",
      "Epoch : 821 train_loss : 0.5046382457017898 Val loss:  0.5425105949491262\n",
      "Epoch : 822 train_loss : 0.5045364737510681 Val loss:  0.5436332276463508\n",
      "Epoch : 823 train_loss : 0.5052357912063599 Val loss:  0.5414988183230162\n",
      "Epoch : 824 train_loss : 0.5037149637937546 Val loss:  0.5399345014244318\n",
      "Epoch : 825 train_loss : 0.5013596028089523 Val loss:  0.5383028572052717\n",
      "Epoch : 826 train_loss : 0.5006698489189148 Val loss:  0.540545699596405\n",
      "Epoch : 827 train_loss : 0.5016009986400605 Val loss:  0.5407051260024309\n",
      "Epoch : 828 train_loss : 0.5025742322206497 Val loss:  0.5431533275544643\n",
      "Epoch : 829 train_loss : 0.5034838229417801 Val loss:  0.5422036126255989\n",
      "Epoch : 830 train_loss : 0.5026591718196869 Val loss:  0.5411330185085534\n",
      "Epoch : 831 train_loss : 0.502103841304779 Val loss:  0.5415564493834972\n",
      "Epoch : 832 train_loss : 0.5024021297693253 Val loss:  0.5430223606526852\n",
      "Epoch : 833 train_loss : 0.5031768053770065 Val loss:  0.5388859301060438\n",
      "Epoch : 834 train_loss : 0.49978227317333224 Val loss:  0.5382297796010971\n",
      "Epoch : 835 train_loss : 0.4987380027770996 Val loss:  0.5394947282969952\n",
      "Epoch : 836 train_loss : 0.49957861602306364 Val loss:  0.5407855516672134\n",
      "Epoch : 837 train_loss : 0.500510185956955 Val loss:  0.5430684296786785\n",
      "Epoch : 838 train_loss : 0.5011467725038529 Val loss:  0.5415643804520369\n",
      "Epoch : 839 train_loss : 0.5013176321983337 Val loss:  0.5441275651007891\n",
      "Epoch : 840 train_loss : 0.5024891316890716 Val loss:  0.542734341174364\n",
      "Epoch : 841 train_loss : 0.502810025215149 Val loss:  0.5415564497560263\n",
      "Epoch : 842 train_loss : 0.5007634401321411 Val loss:  0.5381820753961801\n",
      "Epoch : 843 train_loss : 0.4974673867225647 Val loss:  0.5365072497725487\n",
      "Epoch : 844 train_loss : 0.49660969376564024 Val loss:  0.5398235939443111\n",
      "Epoch : 845 train_loss : 0.4991901457309723 Val loss:  0.5409773574024439\n",
      "Epoch : 846 train_loss : 0.4994694650173187 Val loss:  0.5413514059782029\n",
      "Epoch : 847 train_loss : 0.4991774380207062 Val loss:  0.5424216461926699\n",
      "Epoch : 848 train_loss : 0.5001634240150452 Val loss:  0.5408701117336749\n",
      "Epoch : 849 train_loss : 0.5000150859355926 Val loss:  0.5403685488551855\n",
      "Epoch : 850 train_loss : 0.4995782345533371 Val loss:  0.5422678855061531\n",
      "Epoch : 851 train_loss : 0.5012866139411927 Val loss:  0.5394551225006581\n",
      "Epoch : 852 train_loss : 0.49771909415721893 Val loss:  0.5380191104859113\n",
      "Epoch : 853 train_loss : 0.4948622167110443 Val loss:  0.5381145108491182\n",
      "Epoch : 854 train_loss : 0.49663200974464417 Val loss:  0.5414371967315674\n",
      "Epoch : 855 train_loss : 0.49939484894275665 Val loss:  0.5440345057845116\n",
      "Epoch : 856 train_loss : 0.4996677368879318 Val loss:  0.5410486983507872\n",
      "Epoch : 857 train_loss : 0.49810652136802674 Val loss:  0.5394858283549547\n",
      "Epoch : 858 train_loss : 0.4974579006433487 Val loss:  0.5382258532196283\n",
      "Epoch : 859 train_loss : 0.49574809670448305 Val loss:  0.5379606646299362\n",
      "Epoch : 860 train_loss : 0.4943613976240158 Val loss:  0.5387071993201971\n",
      "Epoch : 861 train_loss : 0.49540485441684723 Val loss:  0.5387811259925366\n",
      "Epoch : 862 train_loss : 0.49598942399024964 Val loss:  0.5413595978170633\n",
      "Epoch : 863 train_loss : 0.497066855430603 Val loss:  0.5413658306002617\n",
      "Epoch : 864 train_loss : 0.49809440076351164 Val loss:  0.5431014147400857\n",
      "Epoch : 865 train_loss : 0.497358512878418 Val loss:  0.5398595092445613\n",
      "Epoch : 866 train_loss : 0.4964096188545227 Val loss:  0.5380585896968841\n",
      "Epoch : 867 train_loss : 0.4953573614358902 Val loss:  0.5383274503052234\n",
      "Epoch : 868 train_loss : 0.4946949541568756 Val loss:  0.5377265459299088\n",
      "Epoch : 869 train_loss : 0.4932137459516525 Val loss:  0.537679605782032\n",
      "Epoch : 870 train_loss : 0.49382862746715545 Val loss:  0.5378619353473186\n",
      "Epoch : 871 train_loss : 0.494321221113205 Val loss:  0.5394036470353603\n",
      "Epoch : 872 train_loss : 0.49544537663459776 Val loss:  0.541810325756669\n",
      "Epoch : 873 train_loss : 0.4967738538980484 Val loss:  0.5397766041755676\n",
      "Epoch : 874 train_loss : 0.4952589362859726 Val loss:  0.538170564994216\n",
      "Epoch : 875 train_loss : 0.49343139827251437 Val loss:  0.5389081759005785\n",
      "Epoch : 876 train_loss : 0.494450780749321 Val loss:  0.5387490775436163\n",
      "Epoch : 877 train_loss : 0.49395984709262847 Val loss:  0.5372396349906922\n",
      "Epoch : 878 train_loss : 0.49308125078678133 Val loss:  0.5371517590433359\n",
      "Epoch : 879 train_loss : 0.49182386994361876 Val loss:  0.5355947290360927\n",
      "Epoch : 880 train_loss : 0.4915039956569672 Val loss:  0.5380681674927473\n",
      "Epoch : 881 train_loss : 0.49285484850406647 Val loss:  0.5415961456298828\n",
      "Epoch : 882 train_loss : 0.49447010457515717 Val loss:  0.5404584749788046\n",
      "Epoch : 883 train_loss : 0.4940750926733017 Val loss:  0.5381270668655633\n",
      "Epoch : 884 train_loss : 0.4938118696212769 Val loss:  0.5399976555258036\n",
      "Epoch : 885 train_loss : 0.49141891300678253 Val loss:  0.5349231518059969\n",
      "Epoch : 886 train_loss : 0.48886869549751283 Val loss:  0.5373885570466518\n",
      "Epoch : 887 train_loss : 0.4911072373390198 Val loss:  0.5380946431308985\n",
      "Epoch : 888 train_loss : 0.49323914051055906 Val loss:  0.5419711227715015\n",
      "Epoch : 889 train_loss : 0.49408293068408965 Val loss:  0.5379481343924999\n",
      "Epoch : 890 train_loss : 0.49140702486038207 Val loss:  0.5372477388381958\n",
      "Epoch : 891 train_loss : 0.4892262488603592 Val loss:  0.5357527534663677\n",
      "Epoch : 892 train_loss : 0.4884596675634384 Val loss:  0.5392569749802351\n",
      "Epoch : 893 train_loss : 0.48944192826747895 Val loss:  0.5391530504077673\n",
      "Epoch : 894 train_loss : 0.49038081169128417 Val loss:  0.5407333064824342\n",
      "Epoch : 895 train_loss : 0.4914354205131531 Val loss:  0.5422162755578757\n",
      "Epoch : 896 train_loss : 0.49162234365940094 Val loss:  0.5415812073647975\n",
      "Epoch : 897 train_loss : 0.4915908306837082 Val loss:  0.5403946402668953\n",
      "Epoch : 898 train_loss : 0.49036669433116914 Val loss:  0.538678376302123\n",
      "Epoch : 899 train_loss : 0.48709464967250826 Val loss:  0.5379894328117371\n",
      "Epoch : 900 train_loss : 0.4873703420162201 Val loss:  0.536915623024106\n",
      "Epoch : 901 train_loss : 0.48744249939918516 Val loss:  0.5424148270487785\n",
      "Epoch : 902 train_loss : 0.488560152053833 Val loss:  0.541479146629572\n",
      "Epoch : 903 train_loss : 0.4886013329029083 Val loss:  0.5419501989334822\n",
      "Epoch : 904 train_loss : 0.4895666539669037 Val loss:  0.5431099277734757\n",
      "Epoch : 905 train_loss : 0.4899022549390793 Val loss:  0.542617283090949\n",
      "Epoch : 906 train_loss : 0.4893310010433197 Val loss:  0.5384155093878508\n",
      "Epoch : 907 train_loss : 0.4861453413963318 Val loss:  0.5423971356451511\n",
      "Epoch : 908 train_loss : 0.48735426366329193 Val loss:  0.5396707863360644\n",
      "Epoch : 909 train_loss : 0.48824826776981356 Val loss:  0.5410113567113877\n",
      "Epoch : 910 train_loss : 0.48671684265136717 Val loss:  0.5431679808348417\n",
      "Epoch : 911 train_loss : 0.4867607206106186 Val loss:  0.5409753227978944\n",
      "Epoch : 912 train_loss : 0.48678828179836275 Val loss:  0.5416975145041942\n",
      "Epoch : 913 train_loss : 0.4858712702989578 Val loss:  0.5438590160757303\n",
      "Epoch : 914 train_loss : 0.486410790681839 Val loss:  0.5425521440804004\n",
      "Epoch : 915 train_loss : 0.4884423345327377 Val loss:  0.5452373298257589\n",
      "Epoch : 916 train_loss : 0.48801462948322294 Val loss:  0.540183619633317\n",
      "Epoch : 917 train_loss : 0.4849310576915741 Val loss:  0.5433034007996321\n",
      "Epoch : 918 train_loss : 0.485160231590271 Val loss:  0.5386672750860453\n",
      "Epoch : 919 train_loss : 0.48367178738117217 Val loss:  0.5414419350028038\n",
      "Epoch : 920 train_loss : 0.4834171861410141 Val loss:  0.5427234577387571\n",
      "Epoch : 921 train_loss : 0.48551382422447203 Val loss:  0.5449011386930943\n",
      "Epoch : 922 train_loss : 0.48734375536441804 Val loss:  0.5429964531958104\n",
      "Epoch : 923 train_loss : 0.4856323808431625 Val loss:  0.5461064774543047\n",
      "Epoch : 924 train_loss : 0.4853509783744812 Val loss:  0.5397911095619202\n",
      "Epoch : 925 train_loss : 0.4847181588411331 Val loss:  0.5414527150988578\n",
      "Epoch : 926 train_loss : 0.4834033876657486 Val loss:  0.5423704548925161\n",
      "Epoch : 927 train_loss : 0.4835672497749329 Val loss:  0.539804801568389\n",
      "Epoch : 928 train_loss : 0.4823206961154938 Val loss:  0.5419682333618402\n",
      "Epoch : 929 train_loss : 0.4819152504205704 Val loss:  0.5414203441143035\n",
      "Epoch : 930 train_loss : 0.4825450122356415 Val loss:  0.544622090011835\n",
      "Epoch : 931 train_loss : 0.4855115205049515 Val loss:  0.5451780342310667\n",
      "Epoch : 932 train_loss : 0.48588269054889677 Val loss:  0.5452272418886424\n",
      "Epoch : 933 train_loss : 0.4832494556903839 Val loss:  0.5422956043481827\n",
      "Epoch : 934 train_loss : 0.48327834606170655 Val loss:  0.544836496040225\n",
      "Epoch : 935 train_loss : 0.4829722762107849 Val loss:  0.5416130793094636\n",
      "Epoch : 936 train_loss : 0.48108078837394713 Val loss:  0.542460235953331\n",
      "Epoch : 937 train_loss : 0.48118226826190946 Val loss:  0.5450636643916369\n",
      "Epoch : 938 train_loss : 0.48144911229610443 Val loss:  0.5430747167021036\n",
      "Epoch : 939 train_loss : 0.481112414598465 Val loss:  0.5428044399619102\n",
      "Epoch : 940 train_loss : 0.4808677911758423 Val loss:  0.5465280735492707\n",
      "Epoch : 941 train_loss : 0.4831450581550598 Val loss:  0.5432672920823097\n",
      "Epoch : 942 train_loss : 0.48086486756801605 Val loss:  0.5458700965344906\n",
      "Epoch : 943 train_loss : 0.4803353130817413 Val loss:  0.5433833049237728\n",
      "Epoch : 944 train_loss : 0.4813184320926666 Val loss:  0.5471177797764539\n",
      "Epoch : 945 train_loss : 0.48277988433837893 Val loss:  0.5448996877670288\n",
      "Epoch : 946 train_loss : 0.47976216971874236 Val loss:  0.5452180340141057\n",
      "Epoch : 947 train_loss : 0.4796624004840851 Val loss:  0.5451784809678794\n",
      "Epoch : 948 train_loss : 0.4805462688207626 Val loss:  0.5444216102361679\n",
      "Epoch : 949 train_loss : 0.47890336215496065 Val loss:  0.5450348385423422\n",
      "Epoch : 950 train_loss : 0.4796704202890396 Val loss:  0.5444045688956975\n",
      "Epoch : 951 train_loss : 0.47997336089611053 Val loss:  0.5452259620279074\n",
      "Epoch : 952 train_loss : 0.47821771800518037 Val loss:  0.5450950600206852\n",
      "Epoch : 953 train_loss : 0.47837704718112944 Val loss:  0.5437351597845554\n",
      "Epoch : 954 train_loss : 0.4778547018766403 Val loss:  0.5469473600387573\n",
      "Epoch : 955 train_loss : 0.48051137924194337 Val loss:  0.5491524589061737\n",
      "Epoch : 956 train_loss : 0.48038345873355864 Val loss:  0.5474234990775585\n",
      "Epoch : 957 train_loss : 0.47856751680374143 Val loss:  0.5454427345097065\n",
      "Epoch : 958 train_loss : 0.4775232344865799 Val loss:  0.5445907213538885\n",
      "Epoch : 959 train_loss : 0.4763140618801117 Val loss:  0.5458266647160054\n",
      "Epoch : 960 train_loss : 0.4761772871017456 Val loss:  0.5452854545414448\n",
      "Epoch : 961 train_loss : 0.47644409239292146 Val loss:  0.5468826008588076\n",
      "Epoch : 962 train_loss : 0.476300510764122 Val loss:  0.5467851976305246\n",
      "Epoch : 963 train_loss : 0.4763945579528809 Val loss:  0.5457395520806313\n",
      "Epoch : 964 train_loss : 0.47694024443626404 Val loss:  0.5489633347094058\n",
      "Epoch : 965 train_loss : 0.4794918268918991 Val loss:  0.5487822998315096\n",
      "Epoch : 966 train_loss : 0.47700334191322324 Val loss:  0.5482297446578741\n",
      "Epoch : 967 train_loss : 0.47594005763530733 Val loss:  0.5458702548593283\n",
      "Epoch : 968 train_loss : 0.4751087099313736 Val loss:  0.5465986551344395\n",
      "Epoch : 969 train_loss : 0.4739916682243347 Val loss:  0.5464130847901105\n",
      "Epoch : 970 train_loss : 0.47639753520488737 Val loss:  0.5495993851870298\n",
      "Epoch : 971 train_loss : 0.4775985836982727 Val loss:  0.5466584799438715\n",
      "Epoch : 972 train_loss : 0.474346724152565 Val loss:  0.5452921722084284\n",
      "Epoch : 973 train_loss : 0.4729104518890381 Val loss:  0.5454624137282371\n",
      "Epoch : 974 train_loss : 0.4742733329534531 Val loss:  0.5485217883437872\n",
      "Epoch : 975 train_loss : 0.47652829587459566 Val loss:  0.5495483904331923\n",
      "Epoch : 976 train_loss : 0.474960646033287 Val loss:  0.5465011245757341\n",
      "Epoch : 977 train_loss : 0.47215483486652376 Val loss:  0.5460263554751873\n",
      "Epoch : 978 train_loss : 0.4726619839668274 Val loss:  0.5493075251579285\n",
      "Epoch : 979 train_loss : 0.47440142929553986 Val loss:  0.550770990550518\n",
      "Epoch : 980 train_loss : 0.4751286655664444 Val loss:  0.550749396905303\n",
      "Epoch : 981 train_loss : 0.4745139330625534 Val loss:  0.5507078011333942\n",
      "Epoch : 982 train_loss : 0.47410197257995607 Val loss:  0.5465348669141531\n",
      "Epoch : 983 train_loss : 0.4723184198141098 Val loss:  0.5471488495171071\n",
      "Epoch : 984 train_loss : 0.4704256057739258 Val loss:  0.5461314508318902\n",
      "Epoch : 985 train_loss : 0.47019979953765867 Val loss:  0.5471029543876649\n",
      "Epoch : 986 train_loss : 0.47078106105327605 Val loss:  0.55033712297678\n",
      "Epoch : 987 train_loss : 0.4740608841180801 Val loss:  0.5537858476489783\n",
      "Epoch : 988 train_loss : 0.47386392652988435 Val loss:  0.5516054859757423\n",
      "Epoch : 989 train_loss : 0.4724434643983841 Val loss:  0.5506825840473175\n",
      "Epoch : 990 train_loss : 0.47180417478084563 Val loss:  0.5486894664168358\n",
      "Epoch : 991 train_loss : 0.47160540521144867 Val loss:  0.5479514858871698\n",
      "Epoch : 992 train_loss : 0.4698499858379364 Val loss:  0.5474727285653354\n",
      "Epoch : 993 train_loss : 0.46819038689136505 Val loss:  0.5457719128578902\n",
      "Epoch : 994 train_loss : 0.4683683753013611 Val loss:  0.5493826974183321\n",
      "Epoch : 995 train_loss : 0.4702025502920151 Val loss:  0.552570349946618\n",
      "Epoch : 996 train_loss : 0.4724378287792206 Val loss:  0.5564633391797542\n",
      "Epoch : 997 train_loss : 0.4730776250362396 Val loss:  0.5545018927007914\n",
      "Epoch : 998 train_loss : 0.4710457384586334 Val loss:  0.5494539190083743\n",
      "Epoch : 999 train_loss : 0.4692995518445969 Val loss:  0.5478882527351379\n"
     ]
    }
   ],
   "source": [
    "#print(model)\n",
    "epochs = 1000\n",
    "total_epochs+=epochs\n",
    "\n",
    "model.train()\n",
    "\n",
    "for e in tnrange(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss_1 = 0\n",
    "    val_loss_sum = 0\n",
    "    \n",
    "    for i in range(len(train_batch)):\n",
    "        \n",
    "        output = model(train_batch[i])\n",
    "        loss = criterion(output, label_batch[i])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for j in range(len(val_batch)):\n",
    "                \n",
    "                val_output = model(val_batch[j])\n",
    "                val_loss =  criterion(val_output, val_label_batch[j])\n",
    "                val_loss_1+=val_loss.item()\n",
    "        val_loss_sum=val_loss_1/len(val_batch)\n",
    "        \n",
    "        \n",
    "    print(\"Epoch :\", e, \"train_loss :\", train_loss/len(train_batch), \"Val loss: \", val_loss_sum/len(val_batch))    \n",
    "    val_losses.append(val_loss_sum/len(val_batch))    \n",
    "    train_losses.append(train_loss/len(train_batch))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGxKLiV0suA8"
   },
   "source": [
    "### Training Metrics :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "g3uZMDqrE2Iu",
    "outputId": "b4191f91-8f2e-4ff4-d86b-b1b2b58dc15b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14cfde490>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frm=10 # does not \n",
    "plt.plot(train_losses[frm:], label='Training loss')\n",
    "plt.plot(val_losses[frm:], label='Validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdxctDz8tECj"
   },
   "source": [
    "### How does the model perform on the test-set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CRm4OS3IRq22",
    "outputId": "0a7b802e-e68a-4af0-dd6b-25a64bef8bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.22529644268774 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct=0\n",
    "i=0\n",
    "res=[]\n",
    "with torch.no_grad():\n",
    "    for batch in test_batch :\n",
    "        for j in range(len(batch)):\n",
    "            x = model(batch[j])\n",
    "            #print(round(x.item()))\n",
    "            res.append(round(x.item()))\n",
    "\n",
    "true_labels= list(test[\"quality\"])\n",
    "\n",
    "for i in range(len(res)):\n",
    "    if res[i]==int(true_labels[i]):\n",
    "        correct+=1\n",
    "        \n",
    "print(\"Accuracy:\", 100*(correct/len(res)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Wine_Connoisseur.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a7aaad0630ad3230b7b955ab34166b9352cf35c53f01b298fd4177c5371ed1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
