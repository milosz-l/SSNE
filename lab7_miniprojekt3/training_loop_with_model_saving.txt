%%time

epochs, losses, train_acc, val_acc = [], [], [], []

best_model = None
best_valid_acc = None

NUM_EPOCHS = 5
for epoch in range(NUM_EPOCHS): 
    train_correct_count = 0
    train_data_len = 0
    running_loss = 0.0
    for i, data in enumerate(train_dataloader, 0):
        # print(i)
        net.train()
        
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        

        # print statistics
        outputs_for_train_correct_count = outputs.max(1, keepdim=True)[1].squeeze()
        train_correct_count += (outputs_for_train_correct_count == labels).sum()
        train_data_len += len(labels)

        # # clear memory
        # del inputs, labels, outputs
        # torch.mps.empty_cache()

        running_loss_single_iter = loss.item()
        running_loss += running_loss_single_iter
        
    

    new_train_acc = train_correct_count / train_data_len
    train_acc.append(new_train_acc)
    
    running_loss /= 1000
    losses.append(running_loss)
    new_val_acc = get_accuracy()
    val_acc.append(new_val_acc)
    epochs.append(epoch+1)
    
    # replacing best model if necessary
    if best_model == None or best_valid_acc < new_val_acc:
        best_model = net
        best_valid_acc = new_val_acc
        
        # saving model to file
        state_dict = best_model.state_dict()
        torch.save(state_dict, "neural_net.tar")

    print(f'epoch {epoch+1} train running accuracy = {new_train_acc}, val_accuracy = {new_val_acc}')
    print(f'[{epoch+1}/{NUM_EPOCHS}] running loss: {round(running_loss, 4)}, macro avg accuracy: {round(get_accuracy_per_class(), 4)}\n')
    running_loss = 0.0

print('Finished Training')