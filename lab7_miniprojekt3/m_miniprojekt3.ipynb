{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, setting up device and seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(1)\n",
    "# device = torch.device(\"cuda\")\n",
    "# device = torch.device('mps')\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa94b7",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d478aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_COLAB_VERSION = False\n",
    "VALIDATION_PERCENTAGE = 0.10\n",
    "TRAIN_PATH = \"./train\"\n",
    "TEST_PATH = \"./test_all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainFolder = torchvision.datasets.ImageFolder(root=TRAIN_PATH,\n",
    "                                               transform=transform)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainFolder, batch_size=batch_size,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root=TEST_PATH, train=False,\n",
    "#                                        download=False, transform=transform)\n",
    "\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "#                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and validation\n",
    "train_size = int((1 - VALIDATION_PERCENTAGE) * len(trainFolder))\n",
    "val_size = len(trainFolder) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(trainFolder, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79209"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_subset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8802"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_subset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_subset, batch_size=256) # TODO: change to max, który się szybko liczy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Mobdule):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, 2)\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, 2)\n",
    "#         self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv4 = nn.Conv2d(256, 512, 2)\n",
    "#         self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.conv5 = nn.Conv2d(64, 128, 2)\n",
    "#         self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv6 = nn.Conv2d(128, 128, 2)\n",
    "#         self.pool6 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "#         self.conv7 = nn.Conv2d(128, 128, 2)\n",
    "#         self.pool7 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "#         self.conv8 = nn.Conv2d(128, 128, 2)\n",
    "#         self.pool8 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "#         self.fc1 = nn.Linear(4608, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 50)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.pool4(F.relu(self.conv4(x)))\n",
    "\n",
    "#         # x = self.pool5(F.relu(self.conv5(x)))\n",
    "#         # x = self.pool6(F.relu(self.conv6(x)))\n",
    "#         # x = self.pool7(F.relu(self.conv7(x)))\n",
    "#         # x = self.pool8(F.relu(self.conv8(x)))\n",
    "\n",
    "#         # print(x.shape, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LongNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         ## Warstwa konwolucyjna\n",
    "#         self.n_convs =  4\n",
    "#         n_channels = 32\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_channels, kernel_size=5, stride=1, padding=0)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=5, stride=1, padding=0)\n",
    "#         self.convs = [nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2) for _ in range(self.n_convs)]\n",
    "#         self.bns = [nn.BatchNorm2d(n_channels) for _ in range(self.n_convs)]\n",
    "     \n",
    "#         ## Warstwa max pooling \n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.fc1 = nn.Linear(n_channels * 28 * 28, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 50)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x_2 = F.relu(self.conv2(x))\n",
    "#         x = x_2\n",
    "#         for i in range(self.n_convs):    # tutaj w pętli jest chyba skip connection, skipujemy wyjścia drugiej warstwy do kazdej kolejnej\n",
    "#             x = torch.cat([x,x_2],dim=1)\n",
    "#             x = F.relu(self.convs[i](x))\n",
    "#             x = self.bns[i](x)\n",
    "#         x = self.pool1(x)\n",
    "# #         print(x.size())\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# net = LongNet().to(device)\n",
    "# for conv in net.convs:\n",
    "#     conv.to(device)\n",
    "# for bn in net.bns:\n",
    "#     bn.to(device)\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         ## Warstwa konwolucyjna\n",
    "#         self.n_convs =  8\n",
    "#         n_channels = 10\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_channels, kernel_size=5, stride=1, padding=0)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn3 = nn.BatchNorm2d(n_channels)\n",
    "#         self.conv4 = nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn4 = nn.BatchNorm2d(n_channels)\n",
    "#         self.conv5 = nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn5 = nn.BatchNorm2d(n_channels)\n",
    "#         self.conv6 = nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn6 = nn.BatchNorm2d(n_channels)\n",
    "#         self.conv7 = nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn7 = nn.BatchNorm2d(n_channels)\n",
    "#         self.conv8 = nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn8 = nn.BatchNorm2d(n_channels)\n",
    "#         self.conv9 = nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn9 = nn.BatchNorm2d(n_channels)\n",
    "#         self.conv10 = nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn10 = nn.BatchNorm2d(n_channels)\n",
    "\n",
    "#         self.convs = [self.conv3, self.conv4, self.conv5, self.conv6, self.conv7, self.conv8, self.conv9, self.conv10]\n",
    "#         self.bns = [self.bn3, self.bn4, self.bn5, self.bn6, self.bn7, self.bn8, self.bn9, self.bn10]\n",
    "#         # self.convs = [nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2) for _ in range(self.n_convs)]\n",
    "#         # self.bns = [nn.BatchNorm2d(n_channels) for _ in range(self.n_convs)]\n",
    "\n",
    "#         ## Warstwa max pooling \n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.fc1 = nn.Linear(n_channels * 28 * 28, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 50)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x_2 = F.relu(self.conv2(x))\n",
    "#         x = x_2\n",
    "#         for i in range(self.n_convs):    # tutaj w pętli jest chyba skip connection, skipujemy wyjścia drugiej warstwy do kazdej kolejnej\n",
    "#             x = torch.cat([x,x_2],dim=1)\n",
    "#             x = F.relu(self.convs[i](x))\n",
    "#             x = self.bns[i](x)\n",
    "#         x = self.pool1(x)\n",
    "# #         print(x.size())\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels=64):\n",
    "#         super().__init__()\n",
    "#         ## Warstwa konwolucyjna\n",
    "#         out_channels = in_channels\n",
    "#         self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.conv3 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "#         self.conv4 = nn.Conv2d(in_channels=in_channels*2, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn4 = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x_1 = self.bn1(F.relu(self.conv1(x)))\n",
    "#         # x = x_1\n",
    "\n",
    "#         x_2 = self.bn2(F.relu(self.conv2(x_1)))\n",
    "#         # x = x_2\n",
    "\n",
    "#         x_3 = self.bn3(F.relu(self.conv3(x_2)))\n",
    "#         x_3 = torch.cat([x_3, x],dim=1)\n",
    "#         # x = torch.cat([x_3, x],dim=1)\n",
    "\n",
    "#         x_4 = self.bn3(F.relu(self.conv4(x_3)))\n",
    "\n",
    "#         x_5 = torch.cat([x_4, x_2],dim=1)\n",
    "#         # x = x_2\n",
    "#         # for i in range(self.n_convs):    # tutaj w pętli jest chyba skip connection, skipujemy wyjścia drugiej warstwy do kazdej kolejnej\n",
    "#         #     x = torch.cat([x,x_2],dim=1)\n",
    "#         #     x = F.relu(self.convs[i](x))\n",
    "#         #     x = self.bns[i](x)\n",
    "#         # x = self.pool1(x)\n",
    "# #         print(x.size())\n",
    "#         # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         # x = F.relu(self.fc1(x))\n",
    "#         # x = F.relu(self.fc2(x))\n",
    "#         # x = self.fc3(x)\n",
    "#         return x_5\n",
    "\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         ## Warstwa konwolucyjna\n",
    "#         n_channels = 64\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(n_channels)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "#         self.conv_block1 = ConvBlock(in_channels=n_channels)\n",
    "\n",
    "#         # self.convs = [self.conv3, self.conv4, self.conv5, self.conv6, self.conv7, self.conv8, self.conv9, self.conv10]\n",
    "#         # self.bns = [self.bn3, self.bn4, self.bn5, self.bn6, self.bn7, self.bn8, self.bn9, self.bn10]\n",
    "#         # self.convs = [nn.Conv2d(in_channels=n_channels*2, out_channels=n_channels, kernel_size=5, stride=1, padding=2) for _ in range(self.n_convs)]\n",
    "#         # self.bns = [nn.BatchNorm2d(n_channels) for _ in range(self.n_convs)]\n",
    "\n",
    "#         ## Warstwa max pooling \n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.fc1 = nn.Linear(n_channels * 16 * 16 * 2, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 50)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.bn1(self.conv1(x)))\n",
    "#         x = F.relu(self.conv_block1(x))\n",
    "#         x = self.pool2(x)\n",
    "# #         print(x.size())\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias=False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias=False),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)    # TODO: try to remove this relu, # TODO: compare bias=False and True\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 50):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3, bias=False),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU())\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        # self.avgpool = nn.AvgPool2d(2, stride=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            \n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # print(x.shape)\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# n_channels_test = 64\n",
    "# input_test = torch.rand(32, 3, 32, 32)\n",
    "\n",
    "# conv1_test = torch.nn.Conv2d(in_channels=3, out_channels=n_channels_test, kernel_size=3, stride=1, padding=1)\n",
    "# x_test = conv1_test(input_test)\n",
    "# print(x_test.shape)\n",
    "\n",
    "# conv2_test = nn.Conv2d(in_channels=n_channels_test, out_channels=n_channels_test, kernel_size=3, stride=1, padding=1)\n",
    "# x_test = conv2_test(x_test)\n",
    "# print(x_test.shape)\n",
    "\n",
    "# conv3_test = ConvBlock(in_channels=n_channels_test)\n",
    "# x_test = conv3_test(x_test)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer0): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# net = Net().to(device)\n",
    "# net = models.resnet18().to(device)\n",
    "# net = models.densenet161().to(device)\n",
    "# net = Net().to(device)\n",
    "net = ResNet(ResidualBlock, [2, 2, 2, 2]).to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare achitectures:\n",
    "# with open('architecture_ResNet_implemented.txt', 'w') as f:\n",
    "#     print(net, file=f)  # Python 3.x\n",
    "\n",
    "# with open('architecture_resnet18.txt', 'w') as f:\n",
    "#     print(models.resnet18().to(device), file=f)  # Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-0.2078, -0.2392, -0.3020,  ..., -0.1765, -0.1373, -0.1686],\n",
      "          [-0.1765, -0.2000, -0.2706,  ..., -0.2706, -0.1843, -0.1765],\n",
      "          [-0.1373, -0.1608, -0.2235,  ..., -0.2549, -0.1608, -0.1451],\n",
      "          ...,\n",
      "          [-0.3961, -0.3725, -0.3647,  ..., -0.4588, -0.4510, -0.4824],\n",
      "          [-0.4431, -0.4039, -0.3961,  ..., -0.5137, -0.5451, -0.4980],\n",
      "          [-0.5059, -0.4667, -0.4510,  ..., -0.5373, -0.6157, -0.4824]],\n",
      "\n",
      "         [[-0.4118, -0.4196, -0.4353,  ...,  0.0118,  0.0431,  0.0118],\n",
      "          [-0.3882, -0.4039, -0.4118,  ..., -0.0824, -0.0039,  0.0039],\n",
      "          [-0.3804, -0.3882, -0.3961,  ..., -0.0667,  0.0275,  0.0353],\n",
      "          ...,\n",
      "          [-0.1608, -0.1373, -0.1294,  ..., -0.4196, -0.4196, -0.4588],\n",
      "          [-0.2235, -0.1843, -0.1608,  ..., -0.4510, -0.5137, -0.4667],\n",
      "          [-0.2863, -0.2471, -0.2314,  ..., -0.4745, -0.5843, -0.4510]],\n",
      "\n",
      "         [[-0.5451, -0.5608, -0.5922,  ..., -0.2549, -0.1843, -0.2078],\n",
      "          [-0.5216, -0.5373, -0.5843,  ..., -0.3490, -0.2314, -0.2157],\n",
      "          [-0.5216, -0.5294, -0.5765,  ..., -0.3333, -0.2235, -0.1922],\n",
      "          ...,\n",
      "          [-0.5529, -0.5451, -0.5373,  ..., -0.5765, -0.5529, -0.5922],\n",
      "          [-0.6235, -0.5843, -0.5686,  ..., -0.6157, -0.6471, -0.6000],\n",
      "          [-0.6863, -0.6471, -0.6314,  ..., -0.6392, -0.7176, -0.5843]]],\n",
      "\n",
      "\n",
      "        [[[-0.3490, -0.4196, -0.5137,  ..., -0.0510, -0.3412, -0.7725],\n",
      "          [-0.3490, -0.4275, -0.5137,  ..., -0.2157, -0.4510, -0.6941],\n",
      "          [-0.3725, -0.4353, -0.5059,  ..., -0.5294, -0.6863, -0.7176],\n",
      "          ...,\n",
      "          [ 0.5686,  0.5922,  0.6235,  ...,  0.3490,  0.3490,  0.3412],\n",
      "          [ 0.4588,  0.4667,  0.4980,  ...,  0.3569,  0.3412,  0.3412],\n",
      "          [ 0.5529,  0.4980,  0.4510,  ...,  0.3333,  0.3098,  0.3098]],\n",
      "\n",
      "         [[-0.6471, -0.6314, -0.6157,  ...,  0.1216, -0.2235, -0.7176],\n",
      "          [-0.6471, -0.6314, -0.6157,  ..., -0.0431, -0.3333, -0.6392],\n",
      "          [-0.6627, -0.6392, -0.6078,  ..., -0.3725, -0.5765, -0.6784],\n",
      "          ...,\n",
      "          [ 0.4824,  0.5137,  0.5529,  ...,  0.2549,  0.2549,  0.2471],\n",
      "          [ 0.3804,  0.3961,  0.4353,  ...,  0.2627,  0.2471,  0.2471],\n",
      "          [ 0.4824,  0.4353,  0.3961,  ...,  0.2392,  0.2157,  0.2157]],\n",
      "\n",
      "         [[-0.7961, -0.8431, -0.8824,  ..., -0.1843, -0.5608, -1.0000],\n",
      "          [-0.7961, -0.8431, -0.8824,  ..., -0.3255, -0.6392, -0.9608],\n",
      "          [-0.8039, -0.8353, -0.8588,  ..., -0.5922, -0.8353, -0.9294],\n",
      "          ...,\n",
      "          [ 0.4353,  0.4353,  0.4196,  ...,  0.1294,  0.1294,  0.1216],\n",
      "          [ 0.2863,  0.2627,  0.2549,  ...,  0.1529,  0.1373,  0.1373],\n",
      "          [ 0.3490,  0.2863,  0.1922,  ...,  0.1294,  0.1059,  0.1059]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8196,  0.9216,  0.8745,  ...,  0.5686,  0.6784,  1.0000],\n",
      "          [ 0.9216,  0.8431,  0.9216,  ...,  0.5294,  0.6078,  0.9529],\n",
      "          [ 0.9059,  0.8824,  0.9451,  ...,  0.4275,  0.4667,  0.8039],\n",
      "          ...,\n",
      "          [ 0.9922,  0.9922,  0.9529,  ...,  0.0745,  0.2235,  0.3725],\n",
      "          [ 1.0000,  0.9686,  0.9608,  ...,  0.1137,  0.2314,  0.3176],\n",
      "          [ 1.0000,  0.9451,  0.9843,  ...,  0.1686,  0.2392,  0.2157]],\n",
      "\n",
      "         [[ 0.6392,  0.7333,  0.6706,  ..., -0.1373,  0.2784,  0.9216],\n",
      "          [ 0.7412,  0.6549,  0.7176,  ..., -0.1765,  0.1686,  0.8196],\n",
      "          [ 0.7490,  0.6941,  0.7333,  ..., -0.2706, -0.0118,  0.5765],\n",
      "          ...,\n",
      "          [ 0.7020,  0.7176,  0.7098,  ..., -0.5451, -0.3255, -0.1294],\n",
      "          [ 0.7333,  0.6784,  0.7176,  ..., -0.4588, -0.2549, -0.1529],\n",
      "          [ 0.7882,  0.6392,  0.7412,  ..., -0.3725, -0.2314, -0.2471]],\n",
      "\n",
      "         [[ 0.3961,  0.5451,  0.5373,  ..., -0.0431,  0.3333,  0.9059],\n",
      "          [ 0.4980,  0.4510,  0.5843,  ..., -0.0980,  0.1922,  0.7569],\n",
      "          [ 0.4902,  0.4745,  0.5686,  ..., -0.2235, -0.0510,  0.4353],\n",
      "          ...,\n",
      "          [ 0.5608,  0.5686,  0.5529,  ..., -0.8824, -0.8118, -0.7098],\n",
      "          [ 0.6235,  0.5373,  0.5608,  ..., -0.7333, -0.7020, -0.7020],\n",
      "          [ 0.6863,  0.5059,  0.5765,  ..., -0.6157, -0.6392, -0.7647]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4745, -0.2863, -0.4667,  ..., -0.9765, -0.9765, -0.9686],\n",
      "          [-0.3725, -0.2863, -0.4510,  ..., -0.9922, -0.9765, -0.9529],\n",
      "          [-0.4275, -0.3804, -0.4275,  ..., -0.8667, -0.8431, -0.8196],\n",
      "          ...,\n",
      "          [-0.3255, -0.5137, -0.6471,  ..., -0.4980, -0.1373, -0.1294],\n",
      "          [-0.6157, -0.6706, -0.6627,  ..., -0.4824, -0.4980, -0.6157],\n",
      "          [-0.3490, -0.3098, -0.2784,  ..., -0.5059, -0.8275, -0.8353]],\n",
      "\n",
      "         [[-0.4275, -0.2549, -0.4353,  ..., -0.9608, -0.9608, -0.9529],\n",
      "          [-0.3255, -0.2392, -0.4039,  ..., -0.9843, -0.9529, -0.9294],\n",
      "          [-0.3647, -0.3176, -0.3647,  ..., -0.8588, -0.8353, -0.8118],\n",
      "          ...,\n",
      "          [-0.2863, -0.4745, -0.6078,  ..., -0.4431, -0.0824, -0.0745],\n",
      "          [-0.5686, -0.6235, -0.6157,  ..., -0.4275, -0.4431, -0.5608],\n",
      "          [-0.3020, -0.2627, -0.2314,  ..., -0.4510, -0.7725, -0.7804]],\n",
      "\n",
      "         [[-0.2235, -0.0431, -0.2235,  ..., -0.9843, -0.9843, -0.9765],\n",
      "          [-0.1373, -0.0510, -0.2314,  ..., -0.9451, -0.9137, -0.8902],\n",
      "          [-0.1843, -0.1529, -0.2000,  ..., -0.7176, -0.6784, -0.6549],\n",
      "          ...,\n",
      "          [-0.1294, -0.3176, -0.4510,  ..., -0.3176,  0.0431,  0.0510],\n",
      "          [-0.4431, -0.4980, -0.4902,  ..., -0.2863, -0.3176, -0.4353],\n",
      "          [-0.1922, -0.1529, -0.1216,  ..., -0.3098, -0.6314, -0.6549]]],\n",
      "\n",
      "\n",
      "        [[[-0.9216, -0.8588, -0.9373,  ...,  0.0745,  0.0980,  0.1137],\n",
      "          [-0.9294, -0.9137, -0.9451,  ...,  0.0824,  0.0980,  0.1059],\n",
      "          [-0.9059, -0.9529, -0.9059,  ...,  0.0980,  0.0902,  0.0980],\n",
      "          ...,\n",
      "          [ 0.3333,  0.3412,  0.3804,  ...,  0.2078,  0.2078,  0.1843],\n",
      "          [ 0.4275,  0.4980,  0.4510,  ...,  0.2314,  0.2784,  0.2471],\n",
      "          [ 0.2549,  0.3647,  0.2784,  ...,  0.2000,  0.2784,  0.2392]],\n",
      "\n",
      "         [[-0.9451, -0.8824, -0.9686,  ..., -0.0824, -0.0980, -0.1059],\n",
      "          [-0.9529, -0.9373, -0.9529,  ..., -0.0745, -0.0980, -0.1137],\n",
      "          [-0.9294, -0.9765, -0.9137,  ..., -0.0588, -0.0980, -0.1216],\n",
      "          ...,\n",
      "          [ 0.3490,  0.3569,  0.3804,  ...,  0.2549,  0.2706,  0.2627],\n",
      "          [ 0.4431,  0.5137,  0.4510,  ...,  0.2784,  0.3412,  0.3255],\n",
      "          [ 0.2706,  0.3804,  0.2784,  ...,  0.2471,  0.3412,  0.3098]],\n",
      "\n",
      "         [[-0.9843, -0.9216, -0.9922,  ..., -0.5451, -0.6000, -0.6157],\n",
      "          [-0.9922, -0.9765, -0.9843,  ..., -0.5529, -0.6000, -0.6235],\n",
      "          [-0.9686, -1.0000, -0.9451,  ..., -0.5373, -0.6000, -0.6314],\n",
      "          ...,\n",
      "          [ 0.4431,  0.4510,  0.4588,  ...,  0.3490,  0.3725,  0.3569],\n",
      "          [ 0.5373,  0.6078,  0.5294,  ...,  0.3725,  0.4431,  0.4196],\n",
      "          [ 0.3647,  0.4745,  0.3569,  ...,  0.3412,  0.4431,  0.4275]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9529,  0.9686,  0.9686,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 0.9686,  0.9686,  0.9765,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 0.9765,  0.9765,  0.9765,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 0.9765,  0.9765,  0.9765,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 0.9686,  0.9686,  0.9686,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9922,  1.0000,  0.9922],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  0.9922],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  0.9922,  0.9922],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9922,  0.9922,  0.9922]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9451,  0.9294,  0.9137],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9686,  0.9608,  0.9608],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9686],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9686,  0.9686],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9686,  0.9686,  0.9686]]]]), tensor([ 5, 44, 34, 17, 21, 20,  6, 41, 32, 43, 12, 47,  7,  5, 11, 10, 47,  5,\n",
      "        12,  8,  1, 49, 22,  9, 34,  5, 46,  3, 35, 25, 12, 10])]\n",
      "<class 'list'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(test_batch)\n",
    "print(type(test_batch))\n",
    "print(len(test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "# from datetime import datetime\n",
    "\n",
    "# # uncomment this cell to visualize a net (do it on the cpu - change device from 'mps' to 'cpu')\n",
    "# yhat = net(test_batch[0])\n",
    "# current_datetime_formatted = f'{datetime.now()}'.replace('-', '_').replace(' ', '_').replace(':', '_')[:-4]\n",
    "# make_dot(yhat, params=dict(list(net.named_parameters()))).render(f\"net_visualizations/net_visualization{current_datetime_formatted}\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_parameters(net=models.resnet18()):\n",
    "    params_sum = 0\n",
    "    for params in net.parameters():\n",
    "        params_sum+=params.view(-1).size(0)\n",
    "    return params_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in created net = 11206962\n",
      "number of parameters in resnet18    = 11689512\n"
     ]
    }
   ],
   "source": [
    "# number of parameters in Net - compare to resnet18\n",
    "print(f'number of parameters in created net = {number_of_parameters(net=net)}')\n",
    "print(f'number of parameters in resnet18    = {number_of_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(epochs, losses, train_acc, val_acc):\n",
    "    # plotting\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Training Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(val_dataloader=val_dataloader):\n",
    "    # copy and create new get_accuracy() func - per class\n",
    "    val_correct_count = 0\n",
    "    val_data_len = 0\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "\n",
    "    for i, data in enumerate(val_dataloader, 0):\n",
    "        net.eval()\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        outputs_for_val_correct_count = outputs.max(1, keepdim=True)[1].squeeze()\n",
    "        val_correct_count += (outputs_for_val_correct_count == labels).sum()\n",
    "        val_data_len += len(labels)\n",
    "\n",
    "    new_val_acc = val_correct_count / val_data_len\n",
    "    return new_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dcPPvHRO1JWA"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)   # TODO: try to add very small weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0hGHHfwJ4mUC",
    "outputId": "479aa43b-3b99-4f9f-d9f4-558eb29b8edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train accuracy = 0.16448888182640076, val_accuracy = 0.2522154152393341\n",
      "[1/20] loss: inf\n",
      "epoch 2 train accuracy = 0.3140678405761719, val_accuracy = 0.371733695268631\n",
      "[2/20] loss: 6.266556383609772\n",
      "epoch 3 train accuracy = 0.4003332853317261, val_accuracy = 0.4112701714038849\n",
      "[3/20] loss: 5.4115651686191555\n",
      "epoch 4 train accuracy = 0.46145007014274597, val_accuracy = 0.456714391708374\n",
      "[4/20] loss: 4.81114781844616\n",
      "epoch 5 train accuracy = 0.5161408185958862, val_accuracy = 0.46875709295272827\n",
      "[5/20] loss: 4.26347658276558\n",
      "epoch 6 train accuracy = 0.5724728107452393, val_accuracy = 0.4860258996486664\n",
      "[6/20] loss: 3.7212274577617643\n",
      "epoch 7 train accuracy = 0.6271635890007019, val_accuracy = 0.5221540331840515\n",
      "[7/20] loss: 3.191125111758709\n",
      "epoch 8 train accuracy = 0.683407187461853, val_accuracy = 0.5022721886634827\n",
      "[8/20] loss: 2.6566578947901727\n",
      "epoch 9 train accuracy = 0.7370753288269043, val_accuracy = 0.502726674079895\n",
      "[9/20] loss: 2.160032504186034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:22\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pt/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pt/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pt/lib/python3.9/site-packages/torch/optim/adam.py:162\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    160\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    163\u001b[0m          grads,\n\u001b[1;32m    164\u001b[0m          exp_avgs,\n\u001b[1;32m    165\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    167\u001b[0m          state_steps,\n\u001b[1;32m    168\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    170\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    171\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    177\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    179\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pt/lib/python3.9/site-packages/torch/optim/adam.py:220\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 220\u001b[0m func(params,\n\u001b[1;32m    221\u001b[0m      grads,\n\u001b[1;32m    222\u001b[0m      exp_avgs,\n\u001b[1;32m    223\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    224\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    225\u001b[0m      state_steps,\n\u001b[1;32m    226\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    227\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    228\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    229\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    230\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    231\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    232\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    233\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    234\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pt/lib/python3.9/site-packages/torch/optim/adam.py:324\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    322\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    326\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs, losses, train_acc, val_acc = [], [], [], []\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "for epoch in range(NUM_EPOCHS): \n",
    "    train_correct_count = 0\n",
    "    train_data_len = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # print(i)\n",
    "        net.train()\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        outputs_for_train_correct_count = outputs.max(1, keepdim=True)[1].squeeze()\n",
    "        train_correct_count += (outputs_for_train_correct_count == labels).sum()\n",
    "        train_data_len += len(labels)\n",
    "\n",
    "        # # clear memory\n",
    "        # del inputs, labels, outputs\n",
    "        # torch.mps.empty_cache()\n",
    "\n",
    "        running_loss_single_iter = loss.item()\n",
    "        running_loss += running_loss_single_iter\n",
    "\n",
    "    new_train_acc = train_correct_count / train_data_len\n",
    "    train_acc.append(new_train_acc)\n",
    "    \n",
    "    running_loss /= 1000\n",
    "    losses.append(running_loss)\n",
    "    new_val_acc = get_accuracy()\n",
    "    val_acc.append(new_val_acc)\n",
    "    epochs.append(epoch+1)\n",
    "\n",
    "    print(f'epoch {epoch+1} train running accuracy = {new_train_acc}, val_accuracy = {new_val_acc}')\n",
    "\n",
    "    print(f'[{epoch+1}/{NUM_EPOCHS}] running loss: {running_loss}\\n')\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy = 0.49466028809547424\n"
     ]
    }
   ],
   "source": [
    "print(f'validation accuracy = {get_accuracy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (0,) and (9,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_results(epochs, losses, train_acc, val_acc)\n",
      "Cell \u001b[0;32mIn [25], line 6\u001b[0m, in \u001b[0;36mplot_results\u001b[0;34m(epochs, losses, train_acc, val_acc)\u001b[0m\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mTraining Curve\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m plt\u001b[39m.\u001b[39;49mplot(epochs, losses, label\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mIterations\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pt/lib/python3.9/site-packages/matplotlib/pyplot.py:2728\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2726\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2727\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2728\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2729\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2730\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pt/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1662\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1662\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1663\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1664\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pt/lib/python3.9/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pt/lib/python3.9/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (0,) and (9,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(epochs, losses, train_acc, val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a7aaad0630ad3230b7b955ab34166b9352cf35c53f01b298fd4177c5371ed1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
