{"cells":[{"cell_type":"markdown","metadata":{"id":"5_Aw62xaLFLv"},"source":["# Imports, setting up device and seeds"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"-kQDdETmLHWI"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n"," \n","## PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data as data\n","\n","import torch.optim as optim\n","\n","from torch.utils.data import DataLoader, random_split"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["device = torch.device('mps')"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"b2OuFOjAHMLP"},"outputs":[],"source":["import os\n","import random\n","\n","def set_all_seeds(seed):\n","    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_all_seeds(42)"]},{"cell_type":"markdown","metadata":{"id":"dXiojnk_HvZA"},"source":["# Constants"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"xAKB2OBkHve9"},"outputs":[],"source":["VALIDATION_PERCENTAGE = 0.10\n","TRAIN_PATH = \"./trafic_32\""]},{"cell_type":"markdown","metadata":{"id":"sejxvF56H7N7"},"source":["# Loading data"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"V77_sxtXPa7v"},"outputs":[],"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","batch_size = 1024\n","\n","trainFolder = torchvision.datasets.ImageFolder(root=TRAIN_PATH,\n","                                               transform=transform)\n","\n","trainloader = torch.utils.data.DataLoader(trainFolder, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","# testset = torchvision.datasets.CIFAR10(root=TEST_PATH, train=False,\n","#                                        download=False, transform=transform)\n","\n","# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","#                                          shuffle=False, num_workers=2)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"7GxSaXT0H_Uh"},"outputs":[{"data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 39209\n","    Root location: ./trafic_32\n","    StandardTransform\n","Transform: Compose(\n","               ToTensor()\n","               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","           )"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["trainFolder"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"4QVDO2YxIAYt"},"outputs":[{"data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x169a1c820>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["trainloader"]},{"cell_type":"markdown","metadata":{"id":"CP3xKDyuSc5X"},"source":["# Model"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"5psxvca0PG8p"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim):\n","        super(Encoder, self).__init__()\n","\n","        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc_mean  = nn.Linear(hidden_dim, latent_dim)\n","        self.fc_var   = nn.Linear(hidden_dim, latent_dim)\n","        \n","        self.LeakyReLU = nn.LeakyReLU(0.2)\n","        \n","        self.training = True\n","        \n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x       = self.LeakyReLU(self.fc_1(x))\n","        x       = self.LeakyReLU(self.fc_2(x))\n","        mean     = self.fc_mean(x)\n","        log_var  = self.fc_var(x)                      # encoder produces mean and log of variance \n","                                                       #             (i.e., parateters of simple tractable normal distribution \"q\"\n","        \n","        return mean, log_var"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"EnKbjbvKT-fI"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim, output_dim):\n","        super(Decoder, self).__init__()\n","        self.fc_1 = nn.Linear(latent_dim, hidden_dim)\n","        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc_3 = nn.Linear(hidden_dim, output_dim)\n","        \n","        self.LeakyReLU = nn.LeakyReLU(0.2)\n","        \n","    def forward(self, x):\n","        h     = self.LeakyReLU(self.fc_1(x))\n","        h     = self.LeakyReLU(self.fc_2(h))\n","        \n","        x_hat = torch.sigmoid(self.fc_3(h))\n","        x_hat = x_hat.view([-1, 1, 28, 28])\n","        return x_hat\n","        "]},{"cell_type":"code","execution_count":22,"metadata":{"id":"tliw15bbUA96"},"outputs":[],"source":["class VAE(nn.Module):\n","    def __init__(self, x_dim, hidden_dim, latent_dim):\n","        super(VAE, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n","        self.decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n","\n","        \n","    def reparameterization(self, mean, var):\n","        eps = torch.randn_like(mean)\n","        z = eps * var + mean\n","        # z = np.random.normal(mean, var)\n","        # z = mean # Change to proper sampling\n","        return z\n","        \n","                \n","    def forward(self, x):\n","        mean, log_var = self.encoder(x)\n","        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n","        x_hat = self.decoder(z)\n","        return x_hat, mean, log_var"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"GyTykpGOSBlk"},"outputs":[],"source":["vae = VAE(latent_dim=32, hidden_dim=256, x_dim=784).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czmb6spBSboc"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"F7JiRfSSUI-l"},"source":["# Training loop"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"J7mdh9rkSbrJ"},"outputs":[],"source":["criterion = nn.MSELoss(reduction=\"sum\")\n","optimizer = optim.Adam(vae.parameters(), lr=0.001)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.99)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"h7WZQS5QUKiE"},"outputs":[{"ename":"NameError","evalue":"name 'train_loader' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [25], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      3\u001b[0m     losses_epoch \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m x, _ \u001b[39min\u001b[39;00m \u001b[39miter\u001b[39m(train_loader):\n\u001b[1;32m      5\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m         out, means, log_var \u001b[39m=\u001b[39m vae(x)\n","\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"]}],"source":["num_epochs = 30\n","for n in range(num_epochs):\n","    losses_epoch = []\n","    for x, _ in iter(train_loader):\n","        x = x.to(device)\n","        out, means, log_var = vae(x)\n","        out = out.cpu()\n","        x = x.cpu()\n","        loss = criterion(out, x) \n","        losses_epoch.append(loss.item())\n","        loss.backward()               \n","        optimizer.step()             \n","        optimizer.zero_grad()  \n","    L1_list = []\n","#     if n % 10 == 0:\n","    for x, _ in iter(test_loader):\n","        x  = x.to(device)\n","        out, _, _ = vae(x)\n","        L1_list.append(torch.mean(torch.abs(out-x)).item())\n","    print(f\"Epoch {n} loss {np.mean(np.array(losses_epoch))}, test L1 = {np.mean(L1_list)}\")\n","    scheduler.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZUs5XtdUKkZ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP0UeS0BcnTX8JMocoCFg5R","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"pt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n[Clang 13.0.1 ]"},"vscode":{"interpreter":{"hash":"3a7aaad0630ad3230b7b955ab34166b9352cf35c53f01b298fd4177c5371ed1f"}}},"nbformat":4,"nbformat_minor":0}
