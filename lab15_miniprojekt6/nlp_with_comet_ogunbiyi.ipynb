{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import comet_ml at the top of your file\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "# # Create an experiment with your api key\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"7IzkZqpQDneBnYdnl3QJMYV4K\",\n",
    "#     project_name=\"test-comet-project-name\",\n",
    "#     workspace=\"milosz-l\",\n",
    "# )\n",
    "\n",
    "# # Report multiple hyperparameters using a dictionary:\n",
    "# hyper_params = {\n",
    "#     \"learning_rate\": 0.5,\n",
    "#     \"steps\": 100000,\n",
    "#     \"batch_size\": 50,\n",
    "# }\n",
    "# experiment.log_parameters(hyper_params)\n",
    "\n",
    "# # Or report single hyperparameters:\n",
    "# hidden_layer_size = 50\n",
    "# experiment.log_parameter(\"hidden_layer_size\", hidden_layer_size)\n",
    "\n",
    "# # Long any time-series metrics:\n",
    "# train_accuracy = 3.14\n",
    "# experiment.log_metric(\"accuracy\", train_accuracy, step=0)\n",
    "\n",
    "# # Run your code and go to /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "# comet_ml.init(project_name = \"Hugging Face Text Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/milosz/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c9540141764a1c8ea237ccfdcfcbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "df = load_dataset(\"imdb\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/milosz-l/hugging-face-text-classification/b8070fa34b81493ea3c203d1148fdda6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with your api key\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"7IzkZqpQDneBnYdnl3QJMYV4K\",\n",
    "#     project_name=\"hugging-face-text-classification\",\n",
    "#     workspace=\"milosz-l\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e056f1b740742a5ac1ab6af363140a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da268401282a4852bb5c2e44798db89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a59b8489844b3aa034cae6856ab67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenize_df = df.map(tokenize_function, batched=True)\n",
    "train_df = tokenize_df[\"train\"].shuffle(seed=random_seed).select(range(200))\n",
    "test_df = tokenize_df[\"test\"].shuffle(seed=random_seed).select(range(100))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_MODEL_NAME, num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    seed=random_seed,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=10,\n",
    "    save_steps=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \n",
    "    #get global experiments\n",
    "    # experiment = comet_ml.get_global_experiment()\n",
    "    experiment = Experiment(\n",
    "    api_key=\"7IzkZqpQDneBnYdnl3QJMYV4K\",\n",
    "    project_name=\"hugging-face-text-classification\",\n",
    "    workspace=\"milosz-l\",\n",
    "    )\n",
    "    \n",
    "    #get y_true and y_preds for eval_dataset\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    #compute precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro')\n",
    "    \n",
    "    #compute accuracy score\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    #log confusion matrix\n",
    "    if experiment:\n",
    "        epoch = int(experiment.curr_epoch) if experiment.curr_epoch is not None else 0\n",
    "        experiment.set_epoch(epoch)\n",
    "        experiment.log_confusion_matrix(\n",
    "            y_true=labels,\n",
    "            y_predicted=preds,\n",
    "            labels=[\"negative\", \"postive\"]\n",
    "        )\n",
    "\n",
    "    return {\"accuracy\": acc, \n",
    "            \"f1\": f1, \n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COMET_MODE=ONLINE\n",
      "env: COMET_LOG_ASSETS=TRUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 200\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25\n",
      "  Number of trainable parameters = 66955010\n",
      "COMET ERROR: Failed to extract scalar from SummaryWriter.add_hparams()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d93adc00ed46b08b55e35cb8fb794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8da5e8871d4d559711e8078180a84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/milosz-l/hugging-face-text-classification/b8070fa34b81493ea3c203d1148fdda6\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [3] : (0.553064227104187, 0.7117539048194885)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-environment-definition : 1\n",
      "COMET INFO:     conda-info                   : 1\n",
      "COMET INFO:     conda-specification          : 1\n",
      "COMET INFO:     environment details          : 1\n",
      "COMET INFO:     filename                     : 1\n",
      "COMET INFO:     git metadata                 : 1\n",
      "COMET INFO:     installed packages           : 1\n",
      "COMET INFO:     model graph                  : 1\n",
      "COMET INFO:     notebook                     : 1\n",
      "COMET INFO:     source_code                  : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/milosz-l/hugging-face-text-classification/095e4a66ad384c86b15836e8b21c231c\n",
      "\n",
      "Saving model checkpoint to ./results/checkpoint-25\n",
      "Configuration saved in ./results/checkpoint-25/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{'eval_loss': 0.5551766753196716, 'eval_accuracy': 0.82, 'eval_f1': 0.8173701298701299, 'eval_precision': 0.8251343530384456, 'eval_recall': 0.8157366519470093, 'eval_runtime': 22.9478, 'eval_samples_per_second': 4.358, 'eval_steps_per_second': 0.567, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-25/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 105.1166, 'train_samples_per_second': 1.903, 'train_steps_per_second': 0.238, 'train_loss': 0.6340503692626953, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25, training_loss=0.6340503692626953, metrics={'train_runtime': 105.1166, 'train_samples_per_second': 1.903, 'train_steps_per_second': 0.238, 'train_loss': 0.6340503692626953, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env COMET_MODE=ONLINE\n",
    "%env COMET_LOG_ASSETS=TRUE\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=test_df,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a7aaad0630ad3230b7b955ab34166b9352cf35c53f01b298fd4177c5371ed1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
