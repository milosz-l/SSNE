{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Mainakdeb/Wine_Quality_Prediction/blob/master/Wine_Connoisseur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sX-tdVX9D9t3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piTKw4F7tYra"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "iIq7JlmpDmYb",
    "outputId": "6caf5c72-e226-448f-fa24-db89f72c996c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8      6.0  \n",
       "1      9.5      6.0  \n",
       "2     10.1      6.0  \n",
       "3      9.9      6.0  \n",
       "4      9.9      6.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', delimiter=\";\").astype(float)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYSzFF6qpuak"
   },
   "source": [
    "### Split into train, test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCqHM5XxDl9l"
   },
   "outputs": [],
   "source": [
    "train = df.iloc[:3686]\n",
    "val = df.iloc[3686:3886]\n",
    "test = df.iloc[3886:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRPWhLGhrYOt"
   },
   "source": [
    "### Split features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CS5SFLwUDl5t",
    "outputId": "0b77d1da-aacb-47e3-d567-c944f138bdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3686, 11)\n",
      "(200, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train= train.drop('quality', axis=1), train['quality']\n",
    "print(X_train.shape)\n",
    "\n",
    "X_val, y_val = val.drop('quality', axis=1), val['quality']\n",
    "print(X_val.shape)\n",
    "\n",
    "X_test, y_test = test.drop(\"quality\", axis=1), test[\"quality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xU3PZCxgsNmC"
   },
   "source": [
    "### Splitting into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AsLdat2eE2OO",
    "outputId": "7d958f9c-1709-4810-b1ff-f6ca9c53a247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 246\n"
     ]
    }
   ],
   "source": [
    "f=15 # no. of batches\n",
    "\n",
    "train_batch = np.array_split(X_train, f) \n",
    "label_batch = np.array_split(y_train, f) # 50 sections/batches\n",
    "\n",
    "val_batch = np.array_split(X_val, f)\n",
    "val_label_batch = np.array_split(y_val, f)\n",
    "\n",
    "test_batch = np.array_split(X_test,f) \n",
    "test_label_batch  = np.array_split(y_test, f)\n",
    "\n",
    "\n",
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "for i in range(len(val_batch)):\n",
    "    val_batch[i] = torch.from_numpy(val_batch[i].values).float()\n",
    "for i in range(len(val_label_batch)):\n",
    "    val_label_batch[i] = torch.from_numpy(val_label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "    \n",
    "for i in range(len(test_batch)):\n",
    "    test_batch[i] = torch.from_numpy(test_batch[i].values).float()\n",
    "for i in range(len(test_label_batch)):\n",
    "    test_label_batch[i] = torch.from_numpy(test_label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "print(\"Batch size:\", len(train_batch[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqMR4-U5sVcE"
   },
   "source": [
    "### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ml1BgIpxE2L9"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(11, 22)\n",
    "        self.fc2 = nn.Linear(22, 44)\n",
    "        self.fc3 = nn.Linear(44, 88)\n",
    "        self.fc4 = nn.Linear(88, 22)\n",
    "        self.fc5 = nn.Linear(22, 1)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()\n",
    "train_losses, val_losses = [], []\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # 0.015 87\n",
    "total_epochs=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ov0p9t03saO6"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvTIRas7E2Ku"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11740\\488095999.py:4: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for e in tnrange(epochs):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6308dbe6e944c2aaa01f7d609508d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 train_loss : 8.990810044606526 Val loss:  6.691870008044773\n",
      "Epoch : 1 train_loss : 4.179758087793986 Val loss:  3.356941644350688\n",
      "Epoch : 2 train_loss : 2.2307119528452555 Val loss:  1.8999932988484702\n",
      "Epoch : 3 train_loss : 1.3279564301172893 Val loss:  1.218381726609336\n",
      "Epoch : 4 train_loss : 0.8746140599250793 Val loss:  0.8552332267827458\n",
      "Epoch : 5 train_loss : 0.7277444163958232 Val loss:  0.7043583508001434\n",
      "Epoch : 6 train_loss : 0.7089036544164021 Val loss:  0.6789537141389317\n",
      "Epoch : 7 train_loss : 0.6916412631670634 Val loss:  0.6564252189464039\n",
      "Epoch : 8 train_loss : 0.6803492784500123 Val loss:  0.6501108598046833\n",
      "Epoch : 9 train_loss : 0.6718702554702759 Val loss:  0.6442990101046032\n",
      "Epoch : 10 train_loss : 0.664496902624766 Val loss:  0.6443719633751446\n",
      "Epoch : 11 train_loss : 0.6586488962173462 Val loss:  0.6400395150648224\n",
      "Epoch : 12 train_loss : 0.655229099591573 Val loss:  0.6388374851809607\n",
      "Epoch : 13 train_loss : 0.6510697444279988 Val loss:  0.6389461841185888\n",
      "Epoch : 14 train_loss : 0.6474645515282949 Val loss:  0.6341421441568269\n",
      "Epoch : 15 train_loss : 0.6462779621283213 Val loss:  0.6318884449534946\n",
      "Epoch : 16 train_loss : 0.642662767569224 Val loss:  0.6320393927892048\n",
      "Epoch : 17 train_loss : 0.6383835832277934 Val loss:  0.6294024667806095\n",
      "Epoch : 18 train_loss : 0.6356805165608724 Val loss:  0.6267743021580908\n",
      "Epoch : 19 train_loss : 0.6321040332317353 Val loss:  0.6252057319217258\n",
      "Epoch : 20 train_loss : 0.6296851495901744 Val loss:  0.6251566440860431\n",
      "Epoch : 21 train_loss : 0.6296518365542094 Val loss:  0.6257635081145498\n",
      "Epoch : 22 train_loss : 0.6289778093496958 Val loss:  0.6247273621294234\n",
      "Epoch : 23 train_loss : 0.6284792045752208 Val loss:  0.625687836739752\n",
      "Epoch : 24 train_loss : 0.6258965373039246 Val loss:  0.6254902960194482\n",
      "Epoch : 25 train_loss : 0.6250795304775238 Val loss:  0.6294606070054901\n",
      "Epoch : 26 train_loss : 0.6215734978516897 Val loss:  0.6325505963630146\n",
      "Epoch : 27 train_loss : 0.6208114326000214 Val loss:  0.6341880679130554\n",
      "Epoch : 28 train_loss : 0.6215713699658711 Val loss:  0.6357981358302963\n",
      "Epoch : 29 train_loss : 0.6203950544198354 Val loss:  0.6353289768099785\n",
      "Epoch : 30 train_loss : 0.6189784685770671 Val loss:  0.6356520183549986\n",
      "Epoch : 31 train_loss : 0.6196712732315064 Val loss:  0.6371143978171878\n",
      "Epoch : 32 train_loss : 0.6186971724033355 Val loss:  0.6366980759633912\n",
      "Epoch : 33 train_loss : 0.6154785633087159 Val loss:  0.6352199196153216\n",
      "Epoch : 34 train_loss : 0.6185389856497446 Val loss:  0.6378688323497772\n",
      "Epoch : 35 train_loss : 0.6143103082974751 Val loss:  0.6353585550520154\n",
      "Epoch : 36 train_loss : 0.6138879517714183 Val loss:  0.6347266209125519\n",
      "Epoch : 37 train_loss : 0.6137462457021078 Val loss:  0.6352989852097299\n",
      "Epoch : 38 train_loss : 0.6113612751166025 Val loss:  0.6355291516251035\n",
      "Epoch : 39 train_loss : 0.6129432519276937 Val loss:  0.6352922338578436\n",
      "Epoch : 40 train_loss : 0.6096702516078949 Val loss:  0.6336422164241473\n",
      "Epoch : 41 train_loss : 0.6111259559790293 Val loss:  0.6349592590000894\n",
      "Epoch : 42 train_loss : 0.6082151850064595 Val loss:  0.6330969853202502\n",
      "Epoch : 43 train_loss : 0.6089232782522838 Val loss:  0.6333419626288943\n",
      "Epoch : 44 train_loss : 0.6076168576876323 Val loss:  0.6328170716431406\n",
      "Epoch : 45 train_loss : 0.6079926749070486 Val loss:  0.6329848321941164\n",
      "Epoch : 46 train_loss : 0.6058253427346547 Val loss:  0.6322809903158082\n",
      "Epoch : 47 train_loss : 0.6063743432362875 Val loss:  0.6322978123691347\n",
      "Epoch : 48 train_loss : 0.6043579657872518 Val loss:  0.6325498022635777\n",
      "Epoch : 49 train_loss : 0.603834847609202 Val loss:  0.6321126736534967\n",
      "Epoch : 50 train_loss : 0.6037424047787984 Val loss:  0.6326006864176856\n",
      "Epoch : 51 train_loss : 0.6045921206474304 Val loss:  0.6320795138345825\n",
      "Epoch : 52 train_loss : 0.6018545031547546 Val loss:  0.6314941164851189\n",
      "Epoch : 53 train_loss : 0.6043223063151042 Val loss:  0.631527427136898\n",
      "Epoch : 54 train_loss : 0.6008216420809428 Val loss:  0.6327248701453209\n",
      "Epoch : 55 train_loss : 0.6036041418711344 Val loss:  0.6317352326379881\n",
      "Epoch : 56 train_loss : 0.60215918024381 Val loss:  0.6259683271249136\n",
      "Epoch : 57 train_loss : 0.6022171954313914 Val loss:  0.6264534524414275\n",
      "Epoch : 58 train_loss : 0.6028733770052592 Val loss:  0.6284669759538438\n",
      "Epoch : 59 train_loss : 0.6011679728825887 Val loss:  0.6283785502115885\n",
      "Epoch : 60 train_loss : 0.5985584278901418 Val loss:  0.6297767938839064\n",
      "Epoch : 61 train_loss : 0.5996054371198019 Val loss:  0.63294755693939\n",
      "Epoch : 62 train_loss : 0.5985086898008982 Val loss:  0.6302270720071264\n",
      "Epoch : 63 train_loss : 0.5984146138032277 Val loss:  0.6306321015291744\n",
      "Epoch : 64 train_loss : 0.5980013132095336 Val loss:  0.6290836413039101\n",
      "Epoch : 65 train_loss : 0.6092022081216176 Val loss:  0.6323158406383462\n",
      "Epoch : 66 train_loss : 0.6006034334500631 Val loss:  0.6404208240244124\n",
      "Epoch : 67 train_loss : 0.6017453273137411 Val loss:  0.6268426267968283\n",
      "Epoch : 68 train_loss : 0.5907014906406403 Val loss:  0.6279255761040582\n",
      "Epoch : 69 train_loss : 0.5975595275561015 Val loss:  0.6288090940978791\n",
      "Epoch : 70 train_loss : 0.5891319572925567 Val loss:  0.6213340822524495\n",
      "Epoch : 71 train_loss : 0.5964228967825572 Val loss:  0.6279284842809041\n",
      "Epoch : 72 train_loss : 0.5906629780928294 Val loss:  0.623259729511208\n",
      "Epoch : 73 train_loss : 0.5962752560774486 Val loss:  0.627519195344713\n",
      "Epoch : 74 train_loss : 0.5890074829260509 Val loss:  0.6237721862726742\n",
      "Epoch : 75 train_loss : 0.5948715686798096 Val loss:  0.6269828795062171\n",
      "Epoch : 76 train_loss : 0.587427963813146 Val loss:  0.6224689453840255\n",
      "Epoch : 77 train_loss : 0.5960377931594849 Val loss:  0.6274058506555028\n",
      "Epoch : 78 train_loss : 0.5869958023230235 Val loss:  0.6212963079081641\n",
      "Epoch : 79 train_loss : 0.5943852404753367 Val loss:  0.6269938599069913\n",
      "Epoch : 80 train_loss : 0.5867945631345113 Val loss:  0.6218767037325434\n",
      "Epoch : 81 train_loss : 0.5937453071276347 Val loss:  0.6281514566474491\n",
      "Epoch : 82 train_loss : 0.5889308114846548 Val loss:  0.6245234415266249\n",
      "Epoch : 83 train_loss : 0.5873186945915222 Val loss:  0.6233287292718886\n",
      "Epoch : 84 train_loss : 0.5912609835465749 Val loss:  0.6301030151049296\n",
      "Epoch : 85 train_loss : 0.5880380709966023 Val loss:  0.6277784009774526\n",
      "Epoch : 86 train_loss : 0.5878198762734731 Val loss:  0.6279808444115851\n",
      "Epoch : 87 train_loss : 0.5878757079442342 Val loss:  0.6260012401805983\n",
      "Epoch : 88 train_loss : 0.5852036257584889 Val loss:  0.6216783285472128\n",
      "Epoch : 89 train_loss : 0.5899909416834513 Val loss:  0.6299622456563844\n",
      "Epoch : 90 train_loss : 0.5849465628465017 Val loss:  0.6244657394289971\n",
      "Epoch : 91 train_loss : 0.5870936751365662 Val loss:  0.6257733603980806\n",
      "Epoch : 92 train_loss : 0.5838052491346996 Val loss:  0.6243217005332311\n",
      "Epoch : 93 train_loss : 0.5888427337010701 Val loss:  0.6304249343938297\n",
      "Epoch : 94 train_loss : 0.5825582683086395 Val loss:  0.6238331849045224\n",
      "Epoch : 95 train_loss : 0.5878815710544586 Val loss:  0.6285993404520883\n",
      "Epoch : 96 train_loss : 0.5836133321126302 Val loss:  0.6273593219783571\n",
      "Epoch : 97 train_loss : 0.5827947894732157 Val loss:  0.6243213927083545\n",
      "Epoch : 98 train_loss : 0.5878088136514028 Val loss:  0.6274317169189453\n",
      "Epoch : 99 train_loss : 0.5844620545705159 Val loss:  0.6179250698950555\n",
      "Epoch : 100 train_loss : 0.8295868535836538 Val loss:  0.8525439308749304\n",
      "Epoch : 101 train_loss : 0.657607352733612 Val loss:  0.6442341237597995\n",
      "Epoch : 102 train_loss : 0.5980543434619904 Val loss:  0.634724973142147\n",
      "Epoch : 103 train_loss : 0.5735457301139831 Val loss:  0.6076250006755193\n",
      "Epoch : 104 train_loss : 0.5857377688090006 Val loss:  0.6257342910435465\n",
      "Epoch : 105 train_loss : 0.583122585217158 Val loss:  0.6239314019348886\n",
      "Epoch : 106 train_loss : 0.5794344266255697 Val loss:  0.626172866622607\n",
      "Epoch : 107 train_loss : 0.5779634475708008 Val loss:  0.6180002173119121\n",
      "Epoch : 108 train_loss : 0.5790013949076335 Val loss:  0.623927515215344\n",
      "Epoch : 109 train_loss : 0.5792244772116343 Val loss:  0.617386165757974\n",
      "Epoch : 110 train_loss : 0.575867352883021 Val loss:  0.6165853859318627\n",
      "Epoch : 111 train_loss : 0.5803281247615815 Val loss:  0.6240599111053678\n",
      "Epoch : 112 train_loss : 0.5765507241090139 Val loss:  0.6207808476686478\n",
      "Epoch : 113 train_loss : 0.5805194596449534 Val loss:  0.6238601327273581\n",
      "Epoch : 114 train_loss : 0.5769575635592142 Val loss:  0.6213992321160104\n",
      "Epoch : 115 train_loss : 0.578919517993927 Val loss:  0.6232073865334193\n",
      "Epoch : 116 train_loss : 0.5780516664187113 Val loss:  0.6206876733236844\n",
      "Epoch : 117 train_loss : 0.5797403891881306 Val loss:  0.6247981099618806\n",
      "Epoch : 118 train_loss : 0.5780870338280996 Val loss:  0.6224438699748781\n",
      "Epoch : 119 train_loss : 0.5771066089471181 Val loss:  0.6190080905622907\n",
      "Epoch : 120 train_loss : 0.5812818805376688 Val loss:  0.6253042873740197\n",
      "Epoch : 121 train_loss : 0.5773576478163401 Val loss:  0.6180484530991978\n",
      "Epoch : 122 train_loss : 0.5774624586105347 Val loss:  0.6217447410689461\n",
      "Epoch : 123 train_loss : 0.5782072961330413 Val loss:  0.6224047833349969\n",
      "Epoch : 124 train_loss : 0.5779825150966644 Val loss:  0.6213753986358642\n",
      "Epoch : 125 train_loss : 0.5786274472872416 Val loss:  0.6195471400684781\n",
      "Epoch : 126 train_loss : 0.5762442072232564 Val loss:  0.619031931857268\n",
      "Epoch : 127 train_loss : 0.5806413372357686 Val loss:  0.6231632773743736\n",
      "Epoch : 128 train_loss : 0.5791546901067098 Val loss:  0.6229347145888541\n",
      "Epoch : 129 train_loss : 0.5802571793397268 Val loss:  0.6213255245818031\n",
      "Epoch : 130 train_loss : 0.577229110399882 Val loss:  0.6205350597699483\n",
      "Epoch : 131 train_loss : 0.5817183136940003 Val loss:  0.6263839182919926\n",
      "Epoch : 132 train_loss : 0.5784907778104146 Val loss:  0.6177601593732833\n",
      "Epoch : 133 train_loss : 0.5810126105944315 Val loss:  0.6235693208376567\n",
      "Epoch : 134 train_loss : 0.5769454638163248 Val loss:  0.6233387078510391\n",
      "Epoch : 135 train_loss : 0.581044469277064 Val loss:  0.6270818846093283\n",
      "Epoch : 136 train_loss : 0.5819474935531617 Val loss:  0.6222382612691985\n",
      "Epoch : 137 train_loss : 0.5802596509456635 Val loss:  0.6264322640167342\n",
      "Epoch : 138 train_loss : 0.5833904723326365 Val loss:  0.6286993353565534\n",
      "Epoch : 139 train_loss : 0.5828941067059835 Val loss:  0.6238056087493896\n",
      "Epoch : 140 train_loss : 0.5826365351676941 Val loss:  0.6309867974453502\n",
      "Epoch : 141 train_loss : 0.5837870935599009 Val loss:  0.6263298533360164\n",
      "Epoch : 142 train_loss : 0.5848608334859212 Val loss:  0.6327856442663404\n",
      "Epoch : 143 train_loss : 0.5868167797724406 Val loss:  0.631627093023724\n",
      "Epoch : 144 train_loss : 0.5877249836921692 Val loss:  0.6344278107749092\n",
      "Epoch : 145 train_loss : 0.5906978865464528 Val loss:  0.6355448518196741\n",
      "Epoch : 146 train_loss : 0.5907212694485983 Val loss:  0.6363992151286867\n",
      "Epoch : 147 train_loss : 0.5952958722909292 Val loss:  0.6402864550881915\n",
      "Epoch : 148 train_loss : 0.5989274978637695 Val loss:  0.6445282100306617\n",
      "Epoch : 149 train_loss : 0.6034216960271199 Val loss:  0.641043168703715\n",
      "Epoch : 150 train_loss : 0.6074234247207642 Val loss:  0.6511282606257333\n",
      "Epoch : 151 train_loss : 0.61286541223526 Val loss:  0.649521806107627\n",
      "Epoch : 152 train_loss : 0.6167854309082031 Val loss:  0.6542520122064485\n",
      "Epoch : 153 train_loss : 0.6163836300373078 Val loss:  0.6488044484787516\n",
      "Epoch : 154 train_loss : 0.6132503132025401 Val loss:  0.6484094628360536\n",
      "Epoch : 155 train_loss : 0.6064741810162863 Val loss:  0.6387850231925647\n",
      "Epoch : 156 train_loss : 0.6002060830593109 Val loss:  0.637142310043176\n",
      "Epoch : 157 train_loss : 0.5915028035640717 Val loss:  0.6271902011500464\n",
      "Epoch : 158 train_loss : 0.5885688205560048 Val loss:  0.6268268936541346\n",
      "Epoch : 159 train_loss : 0.5822715938091279 Val loss:  0.6191133890880478\n",
      "Epoch : 160 train_loss : 0.580461593468984 Val loss:  0.6239239230089717\n",
      "Epoch : 161 train_loss : 0.5798089385032654 Val loss:  0.6173245805501938\n",
      "Epoch : 162 train_loss : 0.5817949136098226 Val loss:  0.6260430047247144\n",
      "Epoch : 163 train_loss : 0.579976350069046 Val loss:  0.6161548135677973\n",
      "Epoch : 164 train_loss : 0.5812424878279369 Val loss:  0.6207101269563039\n",
      "Epoch : 165 train_loss : 0.5823732276757558 Val loss:  0.6194120924009217\n",
      "Epoch : 166 train_loss : 0.5817616283893585 Val loss:  0.6216773333483272\n",
      "Epoch : 167 train_loss : 0.5791967173417409 Val loss:  0.6140137148234579\n",
      "Epoch : 168 train_loss : 0.5783623317877452 Val loss:  0.6195597698291143\n",
      "Epoch : 169 train_loss : 0.5775985201199849 Val loss:  0.6124082145426009\n",
      "Epoch : 170 train_loss : 0.5788392424583435 Val loss:  0.619852276775572\n",
      "Epoch : 171 train_loss : 0.5749274849891662 Val loss:  0.6108943849139743\n",
      "Epoch : 172 train_loss : 0.574467808008194 Val loss:  0.6163057606750064\n",
      "Epoch : 173 train_loss : 0.5739645779132843 Val loss:  0.6083609469731648\n",
      "Epoch : 174 train_loss : 0.5745973269144694 Val loss:  0.6205666624837451\n",
      "Epoch : 175 train_loss : 0.5742498318354289 Val loss:  0.6116290328900019\n",
      "Epoch : 176 train_loss : 0.5762038807074229 Val loss:  0.6160204874806934\n",
      "Epoch : 177 train_loss : 0.5742868324120839 Val loss:  0.6113035215271844\n",
      "Epoch : 178 train_loss : 0.5740966081619263 Val loss:  0.6144648347629441\n",
      "Epoch : 179 train_loss : 0.5730616350968679 Val loss:  0.6079437891642253\n",
      "Epoch : 180 train_loss : 0.5743632892767588 Val loss:  0.6161241821779145\n",
      "Epoch : 181 train_loss : 0.5728931923707327 Val loss:  0.6080488806300692\n",
      "Epoch : 182 train_loss : 0.5733596424261729 Val loss:  0.6165780400236448\n",
      "Epoch : 183 train_loss : 0.572142505645752 Val loss:  0.6055565177400907\n",
      "Epoch : 184 train_loss : 0.5727156360944112 Val loss:  0.6187622038854493\n",
      "Epoch : 185 train_loss : 0.5710010687510173 Val loss:  0.6067299495802986\n",
      "Epoch : 186 train_loss : 0.5723941544691722 Val loss:  0.6149754279520777\n",
      "Epoch : 187 train_loss : 0.5718634068965912 Val loss:  0.6088687012261815\n",
      "Epoch : 188 train_loss : 0.5721232950687408 Val loss:  0.6144681771596273\n",
      "Epoch : 189 train_loss : 0.5720454891522725 Val loss:  0.6087739330530167\n",
      "Epoch : 190 train_loss : 0.5716245889663696 Val loss:  0.6126207673549652\n",
      "Epoch : 191 train_loss : 0.5718463321526845 Val loss:  0.6066363762815794\n",
      "Epoch : 192 train_loss : 0.5711532553037008 Val loss:  0.616093511250284\n",
      "Epoch : 193 train_loss : 0.5691932837168375 Val loss:  0.6036732169985771\n",
      "Epoch : 194 train_loss : 0.5695296823978424 Val loss:  0.615287195874585\n",
      "Epoch : 195 train_loss : 0.56881369749705 Val loss:  0.6053688577810924\n",
      "Epoch : 196 train_loss : 0.5697565734386444 Val loss:  0.6206481383575333\n",
      "Epoch : 197 train_loss : 0.5714907864729564 Val loss:  0.6064017181595166\n",
      "Epoch : 198 train_loss : 0.5717482010523478 Val loss:  0.6225341541899575\n",
      "Epoch : 199 train_loss : 0.5692243874073029 Val loss:  0.6062031390269598\n",
      "Epoch : 200 train_loss : 0.5691696107387543 Val loss:  0.6170289904210302\n",
      "Epoch : 201 train_loss : 0.5679460088411967 Val loss:  0.6055663163132138\n",
      "Epoch : 202 train_loss : 0.569218369325002 Val loss:  0.61453382667568\n",
      "Epoch : 203 train_loss : 0.5694457709789276 Val loss:  0.6021557722820176\n",
      "Epoch : 204 train_loss : 0.5668778995672862 Val loss:  0.6116740919484033\n",
      "Epoch : 205 train_loss : 0.5655323843161265 Val loss:  0.6039708660377396\n",
      "Epoch : 206 train_loss : 0.5615407208601634 Val loss:  0.6252432508601083\n",
      "Epoch : 207 train_loss : 0.5911631306012471 Val loss:  0.6317312507165803\n",
      "Epoch : 208 train_loss : 0.5699306905269623 Val loss:  0.601012077430884\n",
      "Epoch : 209 train_loss : 0.5725938101609548 Val loss:  0.6267931315965123\n",
      "Epoch : 210 train_loss : 0.575935943921407 Val loss:  0.6127634913722674\n",
      "Epoch : 211 train_loss : 0.5741625885168712 Val loss:  0.6253458205196593\n",
      "Epoch : 212 train_loss : 0.5694128612677256 Val loss:  0.6018561569518514\n",
      "Epoch : 213 train_loss : 0.5670552353064219 Val loss:  0.6228452577193578\n",
      "Epoch : 214 train_loss : 0.5641216437021891 Val loss:  0.6000139947070016\n",
      "Epoch : 215 train_loss : 0.5658676505088807 Val loss:  0.6176141217019823\n",
      "Epoch : 216 train_loss : 0.5670234402020772 Val loss:  0.6014631668395467\n",
      "Epoch : 217 train_loss : 0.5653564512729645 Val loss:  0.6223116299841139\n",
      "Epoch : 218 train_loss : 0.5644442081451416 Val loss:  0.602158988846673\n",
      "Epoch : 219 train_loss : 0.5656689405441284 Val loss:  0.6231779009103775\n",
      "Epoch : 220 train_loss : 0.567536199092865 Val loss:  0.6022318474120564\n",
      "Epoch : 221 train_loss : 0.5672611256440481 Val loss:  0.6246976650754611\n",
      "Epoch : 222 train_loss : 0.5660475750764211 Val loss:  0.601285195681784\n",
      "Epoch : 223 train_loss : 0.5679582297801972 Val loss:  0.6246268098884158\n",
      "Epoch : 224 train_loss : 0.5667747656504313 Val loss:  0.5995987071924739\n",
      "Epoch : 225 train_loss : 0.5651586373647054 Val loss:  0.6230716461274358\n",
      "Epoch : 226 train_loss : 0.5626308023929596 Val loss:  0.600972157716751\n",
      "Epoch : 227 train_loss : 0.5642633418242137 Val loss:  0.6178869275583162\n",
      "Epoch : 228 train_loss : 0.5635254144668579 Val loss:  0.5980081409878202\n",
      "Epoch : 229 train_loss : 0.5635642111301422 Val loss:  0.6278665083977911\n",
      "Epoch : 230 train_loss : 0.5609101533889771 Val loss:  0.5995518237683508\n",
      "Epoch : 231 train_loss : 0.5628280142943064 Val loss:  0.6196102509895961\n",
      "Epoch : 232 train_loss : 0.5645267009735108 Val loss:  0.5990389459662968\n",
      "Epoch : 233 train_loss : 0.5644390722115834 Val loss:  0.6223083255688349\n",
      "Epoch : 234 train_loss : 0.5641342202822367 Val loss:  0.6050432030359904\n",
      "Epoch : 235 train_loss : 0.5646047413349151 Val loss:  0.6163030431336828\n",
      "Epoch : 236 train_loss : 0.5610226412614187 Val loss:  0.6004641141825252\n",
      "Epoch : 237 train_loss : 0.5634860674540202 Val loss:  0.619383736550808\n",
      "Epoch : 238 train_loss : 0.563108911116918 Val loss:  0.5994797200957934\n",
      "Epoch : 239 train_loss : 0.5648056924343109 Val loss:  0.6315611428022385\n",
      "Epoch : 240 train_loss : 0.5613491892814636 Val loss:  0.5984094056487083\n",
      "Epoch : 241 train_loss : 0.5654546896616618 Val loss:  0.6240268358588218\n",
      "Epoch : 242 train_loss : 0.5609054466088613 Val loss:  0.601637034184403\n",
      "Epoch : 243 train_loss : 0.5636188566684723 Val loss:  0.6206599933240149\n",
      "Epoch : 244 train_loss : 0.5618862609068552 Val loss:  0.5977369793256123\n",
      "Epoch : 245 train_loss : 0.5649851322174072 Val loss:  0.6323893990781572\n",
      "Epoch : 246 train_loss : 0.5618458211421966 Val loss:  0.6010452271170087\n",
      "Epoch : 247 train_loss : 0.5606429576873779 Val loss:  0.6178431380126211\n",
      "Epoch : 248 train_loss : 0.5597429076830546 Val loss:  0.5976090847452481\n",
      "Epoch : 249 train_loss : 0.5665642619132996 Val loss:  0.6226241261760393\n",
      "Epoch : 250 train_loss : 0.5641340970993042 Val loss:  0.6017012739843792\n",
      "Epoch : 251 train_loss : 0.5627907991409302 Val loss:  0.6294267220960723\n",
      "Epoch : 252 train_loss : 0.5581166466077169 Val loss:  0.5972951135039329\n",
      "Epoch : 253 train_loss : 0.5643767098585765 Val loss:  0.6319497176673677\n",
      "Epoch : 254 train_loss : 0.5632953882217407 Val loss:  0.5932877906163533\n",
      "Epoch : 255 train_loss : 0.5644595146179199 Val loss:  0.6367477666007149\n",
      "Epoch : 256 train_loss : 0.5589546899000803 Val loss:  0.6050183447202048\n",
      "Epoch : 257 train_loss : 0.5629739205042521 Val loss:  0.6194009970956379\n",
      "Epoch : 258 train_loss : 0.5585182348887126 Val loss:  0.5959024467402034\n",
      "Epoch : 259 train_loss : 0.561678026119868 Val loss:  0.631331177022722\n",
      "Epoch : 260 train_loss : 0.5572352310021719 Val loss:  0.5959636310074063\n",
      "Epoch : 261 train_loss : 0.5622442305088043 Val loss:  0.6235939956042501\n",
      "Epoch : 262 train_loss : 0.5628522912661235 Val loss:  0.6063275585240788\n",
      "Epoch : 263 train_loss : 0.5617421785990397 Val loss:  0.6251907787720363\n",
      "Epoch : 264 train_loss : 0.5654691874980926 Val loss:  0.6109082116352187\n",
      "Epoch : 265 train_loss : 0.5650920490423839 Val loss:  0.6250892510347896\n",
      "Epoch : 266 train_loss : 0.5601585706075033 Val loss:  0.5969192608859805\n",
      "Epoch : 267 train_loss : 0.5638767838478088 Val loss:  0.6337518975138664\n",
      "Epoch : 268 train_loss : 0.5602882186571757 Val loss:  0.5993954779373275\n",
      "Epoch : 269 train_loss : 0.5580504794915517 Val loss:  0.6202078104350301\n",
      "Epoch : 270 train_loss : 0.5561973432699839 Val loss:  0.5988616430428293\n",
      "Epoch : 271 train_loss : 0.5625979244709015 Val loss:  0.6285136215554343\n",
      "Epoch : 272 train_loss : 0.5631813168525696 Val loss:  0.6004639843437406\n",
      "Epoch : 273 train_loss : 0.5598932683467865 Val loss:  0.6209002152085304\n",
      "Epoch : 274 train_loss : 0.5540514330069224 Val loss:  0.5948492959141731\n",
      "Epoch : 275 train_loss : 0.5631034155686696 Val loss:  0.6334476842814021\n",
      "Epoch : 276 train_loss : 0.5594984034697215 Val loss:  0.5966013952758578\n",
      "Epoch : 277 train_loss : 0.5633694330851237 Val loss:  0.6234628070063061\n",
      "Epoch : 278 train_loss : 0.5608558932940165 Val loss:  0.6058011215262943\n",
      "Epoch : 279 train_loss : 0.5567182223002116 Val loss:  0.6142257877190908\n",
      "Epoch : 280 train_loss : 0.5562194069226583 Val loss:  0.6132737488879098\n",
      "Epoch : 281 train_loss : 0.5549147566159566 Val loss:  0.6169479160176383\n",
      "Epoch : 282 train_loss : 0.5540600260098775 Val loss:  0.6029367195566496\n",
      "Epoch : 283 train_loss : 0.5554373661677042 Val loss:  0.6202118787831731\n",
      "Epoch : 284 train_loss : 0.5562959889570872 Val loss:  0.6012047199408213\n",
      "Epoch : 285 train_loss : 0.5602773030598959 Val loss:  0.6216284980045425\n",
      "Epoch : 286 train_loss : 0.5556668718655904 Val loss:  0.603390998740991\n",
      "Epoch : 287 train_loss : 0.5584563712279002 Val loss:  0.6228419559531742\n",
      "Epoch : 288 train_loss : 0.5566010316212971 Val loss:  0.6093888516558541\n",
      "Epoch : 289 train_loss : 0.5517212708791097 Val loss:  0.6103550937771798\n",
      "Epoch : 290 train_loss : 0.5533731599648793 Val loss:  0.6117421179347567\n",
      "Epoch : 291 train_loss : 0.555757627884547 Val loss:  0.6062336150142882\n",
      "Epoch : 292 train_loss : 0.5576777140299479 Val loss:  0.6233005696203974\n",
      "Epoch : 293 train_loss : 0.5528611044088999 Val loss:  0.6024279402361975\n",
      "Epoch : 294 train_loss : 0.5553434093793234 Val loss:  0.6129698203669653\n",
      "Epoch : 295 train_loss : 0.5565137326717376 Val loss:  0.6131170920199818\n",
      "Epoch : 296 train_loss : 0.5538876156012217 Val loss:  0.6263473481933276\n",
      "Epoch : 297 train_loss : 0.5557671248912811 Val loss:  0.6109249560700523\n",
      "Epoch : 298 train_loss : 0.5513421734174092 Val loss:  0.6110790321562025\n",
      "Epoch : 299 train_loss : 0.5549478948116302 Val loss:  0.6052395383516948\n",
      "Epoch : 300 train_loss : 0.5582549691200256 Val loss:  0.6393478602833219\n",
      "Epoch : 301 train_loss : 0.5559739708900452 Val loss:  0.6021371886134148\n",
      "Epoch : 302 train_loss : 0.5578438639640808 Val loss:  0.624520300063822\n",
      "Epoch : 303 train_loss : 0.5562778055667877 Val loss:  0.6092330190870497\n",
      "Epoch : 304 train_loss : 0.5505204677581788 Val loss:  0.618525927695963\n",
      "Epoch : 305 train_loss : 0.5546168824036916 Val loss:  0.604036200079653\n",
      "Epoch : 306 train_loss : 0.5602985362211863 Val loss:  0.6409931093454361\n",
      "Epoch : 307 train_loss : 0.5586335857709249 Val loss:  0.6015870893994967\n",
      "Epoch : 308 train_loss : 0.5572104136149089 Val loss:  0.6104219614466031\n",
      "Epoch : 309 train_loss : 0.5543087244033813 Val loss:  0.5975517196456591\n",
      "Epoch : 310 train_loss : 0.5565209607283275 Val loss:  0.6232768021027247\n",
      "Epoch : 311 train_loss : 0.5541778246561686 Val loss:  0.5981802386045456\n",
      "Epoch : 312 train_loss : 0.5593810300032298 Val loss:  0.6312823834353023\n",
      "Epoch : 313 train_loss : 0.5558237334092458 Val loss:  0.5950297511617343\n",
      "Epoch : 314 train_loss : 0.5594839493433634 Val loss:  0.6485548157493273\n",
      "Epoch : 315 train_loss : 0.5566889802614848 Val loss:  0.6019187691145473\n",
      "Epoch : 316 train_loss : 0.5607134858767192 Val loss:  0.6235909351044231\n",
      "Epoch : 317 train_loss : 0.5534411092599233 Val loss:  0.594577341708872\n",
      "Epoch : 318 train_loss : 0.5561023533344269 Val loss:  0.6251831057667732\n",
      "Epoch : 319 train_loss : 0.5498398224512736 Val loss:  0.5953240818447536\n",
      "Epoch : 320 train_loss : 0.5539385239283244 Val loss:  0.6074281154738532\n",
      "Epoch : 321 train_loss : 0.5535521725813548 Val loss:  0.5928874906897545\n",
      "Epoch : 322 train_loss : 0.5535224179426829 Val loss:  0.619575571152899\n",
      "Epoch : 323 train_loss : 0.5529790341854095 Val loss:  0.5890654953320822\n",
      "Epoch : 324 train_loss : 0.5583569188912709 Val loss:  0.6273190206289292\n",
      "Epoch : 325 train_loss : 0.556191752354304 Val loss:  0.5997330540749761\n",
      "Epoch : 326 train_loss : 0.5580971956253051 Val loss:  0.6257675370242861\n",
      "Epoch : 327 train_loss : 0.5549845596154531 Val loss:  0.5954701299137539\n",
      "Epoch : 328 train_loss : 0.5534731944402059 Val loss:  0.6047371744116147\n",
      "Epoch : 329 train_loss : 0.5525324940681458 Val loss:  0.5840963152050972\n",
      "Epoch : 330 train_loss : 0.5574008226394653 Val loss:  0.6261684548192554\n",
      "Epoch : 331 train_loss : 0.5552671869595845 Val loss:  0.597149611181683\n",
      "Epoch : 332 train_loss : 0.5575528144836426 Val loss:  0.6324694004654885\n",
      "Epoch : 333 train_loss : 0.5488857865333557 Val loss:  0.5871019445856411\n",
      "Epoch : 334 train_loss : 0.5515263001124064 Val loss:  0.6114858265386688\n",
      "Epoch : 335 train_loss : 0.5538884282112122 Val loss:  0.5924380983908971\n",
      "Epoch : 336 train_loss : 0.5676768084367116 Val loss:  0.6291387352678511\n",
      "Epoch : 337 train_loss : 0.5600370248158772 Val loss:  0.5931659051444795\n",
      "Epoch : 338 train_loss : 0.5638882696628571 Val loss:  0.6245591392450862\n",
      "Epoch : 339 train_loss : 0.5523039519786834 Val loss:  0.6037067999773555\n",
      "Epoch : 340 train_loss : 0.5590445538361867 Val loss:  0.6187778885496987\n",
      "Epoch : 341 train_loss : 0.5516577661037445 Val loss:  0.5919899778895908\n",
      "Epoch : 342 train_loss : 0.5544907788435618 Val loss:  0.6177746229039298\n",
      "Epoch : 343 train_loss : 0.5473477363586425 Val loss:  0.601902241508166\n",
      "Epoch : 344 train_loss : 0.5472841322422027 Val loss:  0.6031125871340434\n",
      "Epoch : 345 train_loss : 0.5490466793378194 Val loss:  0.6167146702607472\n",
      "Epoch : 346 train_loss : 0.5483508805433909 Val loss:  0.6017597684264183\n",
      "Epoch : 347 train_loss : 0.55515562693278 Val loss:  0.6128438639309671\n",
      "Epoch : 348 train_loss : 0.5518535872300466 Val loss:  0.5934250922997794\n",
      "Epoch : 349 train_loss : 0.5527547597885132 Val loss:  0.6218085941672326\n",
      "Epoch : 350 train_loss : 0.5460242827733358 Val loss:  0.5851272215445836\n",
      "Epoch : 351 train_loss : 0.5532652139663696 Val loss:  0.6201356233490838\n",
      "Epoch : 352 train_loss : 0.5471959114074707 Val loss:  0.6014179021451208\n",
      "Epoch : 353 train_loss : 0.5463783264160156 Val loss:  0.6073846044805314\n",
      "Epoch : 354 train_loss : 0.5471604208151499 Val loss:  0.6051641176806556\n",
      "Epoch : 355 train_loss : 0.5462185442447662 Val loss:  0.612483664519257\n",
      "Epoch : 356 train_loss : 0.5488254606723786 Val loss:  0.6079649155338606\n",
      "Epoch : 357 train_loss : 0.5492648323376973 Val loss:  0.601016391283936\n",
      "Epoch : 358 train_loss : 0.5472262084484101 Val loss:  0.5960107524527444\n",
      "Epoch : 359 train_loss : 0.5490996400515239 Val loss:  0.594281015197436\n",
      "Epoch : 360 train_loss : 0.5523652156194051 Val loss:  0.6292484197682805\n",
      "Epoch : 361 train_loss : 0.5509919087092082 Val loss:  0.6001047431760363\n",
      "Epoch : 362 train_loss : 0.5525717735290527 Val loss:  0.6137436922391256\n",
      "Epoch : 363 train_loss : 0.5466275850931803 Val loss:  0.5976864800850551\n",
      "Epoch : 364 train_loss : 0.5474360843499502 Val loss:  0.6096840623352263\n",
      "Epoch : 365 train_loss : 0.5502644538879394 Val loss:  0.5960664524303543\n",
      "Epoch : 366 train_loss : 0.5519900699456533 Val loss:  0.6184248248736064\n",
      "Epoch : 367 train_loss : 0.5494969666004181 Val loss:  0.5929147304428948\n",
      "Epoch : 368 train_loss : 0.5497634212176005 Val loss:  0.6097471973962254\n",
      "Epoch : 369 train_loss : 0.5440170546372731 Val loss:  0.5970322525832388\n",
      "Epoch : 370 train_loss : 0.5495038072268168 Val loss:  0.6068630750311745\n",
      "Epoch : 371 train_loss : 0.5520428081353506 Val loss:  0.593959776626693\n",
      "Epoch : 372 train_loss : 0.5615753293037414 Val loss:  0.6591702356934548\n",
      "Epoch : 373 train_loss : 0.5614872097969055 Val loss:  0.5720576549569766\n",
      "Epoch : 374 train_loss : 0.5568418999512991 Val loss:  0.6093363853295645\n",
      "Epoch : 375 train_loss : 0.5547975500424703 Val loss:  0.5896927495466339\n",
      "Epoch : 376 train_loss : 0.5762780606746674 Val loss:  0.6408532426423498\n",
      "Epoch : 377 train_loss : 0.6047376751899719 Val loss:  0.628084659245279\n",
      "Epoch : 378 train_loss : 0.5585780580838521 Val loss:  0.6116692459583283\n",
      "Epoch : 379 train_loss : 0.5476154704888662 Val loss:  0.5820396092534066\n",
      "Epoch : 380 train_loss : 0.5523152152697245 Val loss:  0.6402503397398525\n",
      "Epoch : 381 train_loss : 0.5437867224216462 Val loss:  0.5883799906902843\n",
      "Epoch : 382 train_loss : 0.5432241598765055 Val loss:  0.605021985106998\n",
      "Epoch : 383 train_loss : 0.5412295560042063 Val loss:  0.6057081701689296\n",
      "Epoch : 384 train_loss : 0.5434611876805623 Val loss:  0.5954885062244203\n",
      "Epoch : 385 train_loss : 0.5494512259960175 Val loss:  0.6172791249222225\n",
      "Epoch : 386 train_loss : 0.5459887087345123 Val loss:  0.6009159374568198\n",
      "Epoch : 387 train_loss : 0.5523913820584615 Val loss:  0.620215966105461\n",
      "Epoch : 388 train_loss : 0.5437894344329834 Val loss:  0.6023234236571524\n",
      "Epoch : 389 train_loss : 0.5449724872907002 Val loss:  0.6261796683073044\n",
      "Epoch : 390 train_loss : 0.5453695098559062 Val loss:  0.5873622141281765\n",
      "Epoch : 391 train_loss : 0.5568503558635711 Val loss:  0.6500117745333248\n",
      "Epoch : 392 train_loss : 0.5551045298576355 Val loss:  0.5774666688508457\n",
      "Epoch : 393 train_loss : 0.552242660522461 Val loss:  0.6326136793030632\n",
      "Epoch : 394 train_loss : 0.5492041011651357 Val loss:  0.5911299007799891\n",
      "Epoch : 395 train_loss : 0.5422031859556834 Val loss:  0.5965878250201544\n",
      "Epoch : 396 train_loss : 0.5453971068064372 Val loss:  0.6027474494112862\n",
      "Epoch : 397 train_loss : 0.5464614967505137 Val loss:  0.6016872281167241\n",
      "Epoch : 398 train_loss : 0.545620185136795 Val loss:  0.6167846847573916\n",
      "Epoch : 399 train_loss : 0.5464629471302033 Val loss:  0.6224472702211804\n",
      "Epoch : 400 train_loss : 0.545563938220342 Val loss:  0.6082161191105843\n",
      "Epoch : 401 train_loss : 0.5413346131642659 Val loss:  0.5996656067172687\n",
      "Epoch : 402 train_loss : 0.5421231826146443 Val loss:  0.6054421730836232\n",
      "Epoch : 403 train_loss : 0.5474841495354971 Val loss:  0.593745524055428\n",
      "Epoch : 404 train_loss : 0.5485482215881348 Val loss:  0.603920144935449\n",
      "Epoch : 405 train_loss : 0.5511071542898814 Val loss:  0.6034450849228434\n",
      "Epoch : 406 train_loss : 0.5448959807554881 Val loss:  0.5991781730122037\n",
      "Epoch : 407 train_loss : 0.5479527672131856 Val loss:  0.628072426782714\n",
      "Epoch : 408 train_loss : 0.5441811601320903 Val loss:  0.6062351384427812\n",
      "Epoch : 409 train_loss : 0.5444787561893463 Val loss:  0.5908246232072512\n",
      "Epoch : 410 train_loss : 0.5466613034407298 Val loss:  0.6118969339463446\n",
      "Epoch : 411 train_loss : 0.544975491364797 Val loss:  0.5970401580797302\n",
      "Epoch : 412 train_loss : 0.5500874618689219 Val loss:  0.6161361600955327\n",
      "Epoch : 413 train_loss : 0.5449225624402364 Val loss:  0.5933902175890075\n",
      "Epoch : 414 train_loss : 0.544836715857188 Val loss:  0.6097205711735619\n",
      "Epoch : 415 train_loss : 0.55551042954127 Val loss:  0.5975832575559615\n",
      "Epoch : 416 train_loss : 0.5585557798544566 Val loss:  0.5823776122927666\n",
      "Epoch : 417 train_loss : 0.5535300453503926 Val loss:  0.6072412161363496\n",
      "Epoch : 418 train_loss : 0.5510192294915517 Val loss:  0.593236245976554\n",
      "Epoch : 419 train_loss : 0.5544173876444499 Val loss:  0.653438309331735\n",
      "Epoch : 420 train_loss : 0.5503957688808441 Val loss:  0.5883302556475004\n",
      "Epoch : 421 train_loss : 0.5562200268109639 Val loss:  0.6434228771924972\n",
      "Epoch : 422 train_loss : 0.5492867708206177 Val loss:  0.6076409654484854\n",
      "Epoch : 423 train_loss : 0.5450411637624105 Val loss:  0.6253552712003391\n",
      "Epoch : 424 train_loss : 0.5429543872674306 Val loss:  0.5849908007515802\n",
      "Epoch : 425 train_loss : 0.5458291451136271 Val loss:  0.6121969100832939\n",
      "Epoch : 426 train_loss : 0.5415913959344228 Val loss:  0.6065884339809418\n",
      "Epoch : 427 train_loss : 0.5376978119214376 Val loss:  0.6076008137398295\n",
      "Epoch : 428 train_loss : 0.5447440087795258 Val loss:  0.6055076087514559\n",
      "Epoch : 429 train_loss : 0.5423691312472025 Val loss:  0.5963985021246804\n",
      "Epoch : 430 train_loss : 0.5446173946062723 Val loss:  0.6272924150692093\n",
      "Epoch : 431 train_loss : 0.5458482027053833 Val loss:  0.5793752564324274\n",
      "Epoch : 432 train_loss : 0.5563907444477081 Val loss:  0.6559750175144938\n",
      "Epoch : 433 train_loss : 0.5587559680143992 Val loss:  0.581168213420444\n",
      "Epoch : 434 train_loss : 0.5593600730101268 Val loss:  0.6174611059824625\n",
      "Epoch : 435 train_loss : 0.5448532978693644 Val loss:  0.5903059420651859\n",
      "Epoch : 436 train_loss : 0.5448461631933849 Val loss:  0.6116086935997009\n",
      "Epoch : 437 train_loss : 0.5398198346296946 Val loss:  0.5948736360006862\n",
      "Epoch : 438 train_loss : 0.5413151542345683 Val loss:  0.6164547585778767\n",
      "Epoch : 439 train_loss : 0.5407281676928203 Val loss:  0.6045967738495933\n",
      "Epoch : 440 train_loss : 0.5375818212827047 Val loss:  0.6011663024624189\n",
      "Epoch : 441 train_loss : 0.5434731443723043 Val loss:  0.6152098212639491\n",
      "Epoch : 442 train_loss : 0.5399045010407766 Val loss:  0.6051283445954323\n",
      "Epoch : 443 train_loss : 0.5388604323069255 Val loss:  0.6121296786599689\n",
      "Epoch : 444 train_loss : 0.54459627866745 Val loss:  0.615847620997164\n",
      "Epoch : 445 train_loss : 0.542721152305603 Val loss:  0.6082736863692602\n",
      "Epoch : 446 train_loss : 0.5484196245670319 Val loss:  0.6241025382942624\n",
      "Epoch : 447 train_loss : 0.5381002942721049 Val loss:  0.5908660293287702\n",
      "Epoch : 448 train_loss : 0.5414321343104045 Val loss:  0.6030572085248099\n",
      "Epoch : 449 train_loss : 0.5410357316335043 Val loss:  0.6029190840323767\n",
      "Epoch : 450 train_loss : 0.538882048924764 Val loss:  0.599328860839208\n",
      "Epoch : 451 train_loss : 0.540468978881836 Val loss:  0.607068380051189\n",
      "Epoch : 452 train_loss : 0.5391096969445547 Val loss:  0.6017605672942268\n",
      "Epoch : 453 train_loss : 0.5363361895084381 Val loss:  0.5864774024486542\n",
      "Epoch : 454 train_loss : 0.5474538703759512 Val loss:  0.6070230741302172\n",
      "Epoch : 455 train_loss : 0.5474000136057536 Val loss:  0.610968058374193\n",
      "Epoch : 456 train_loss : 0.5393649975458781 Val loss:  0.6085796983705626\n",
      "Epoch : 457 train_loss : 0.5374342620372772 Val loss:  0.6059656841887369\n",
      "Epoch : 458 train_loss : 0.5367684721946716 Val loss:  0.6107197751932674\n",
      "Epoch : 459 train_loss : 0.5342262387275696 Val loss:  0.6084839791721769\n",
      "Epoch : 460 train_loss : 0.5361279129981995 Val loss:  0.6042692083451483\n",
      "Epoch : 461 train_loss : 0.5341040233771006 Val loss:  0.6049233206775454\n",
      "Epoch : 462 train_loss : 0.53294531305631 Val loss:  0.5989375808172757\n",
      "Epoch : 463 train_loss : 0.5406026740868887 Val loss:  0.6041646770636241\n",
      "Epoch : 464 train_loss : 0.5396458089351654 Val loss:  0.6135790764292081\n",
      "Epoch : 465 train_loss : 0.544762009382248 Val loss:  0.6042440559135543\n",
      "Epoch : 466 train_loss : 0.5498124341169993 Val loss:  0.6128140812781122\n",
      "Epoch : 467 train_loss : 0.5425813178221385 Val loss:  0.6185857837067711\n",
      "Epoch : 468 train_loss : 0.5536283830801646 Val loss:  0.6233341340223948\n",
      "Epoch : 469 train_loss : 0.5526489237944285 Val loss:  0.5842901056342654\n",
      "Epoch : 470 train_loss : 0.5518701473871866 Val loss:  0.6084447229570813\n",
      "Epoch : 471 train_loss : 0.5504863460858663 Val loss:  0.6214442976978091\n",
      "Epoch : 472 train_loss : 0.5406982839107514 Val loss:  0.6037713539600372\n",
      "Epoch : 473 train_loss : 0.5449051558971405 Val loss:  0.6111575102143817\n",
      "Epoch : 474 train_loss : 0.5420417149861654 Val loss:  0.6061651899417241\n",
      "Epoch : 475 train_loss : 0.5349699318408966 Val loss:  0.6130553204814593\n",
      "Epoch : 476 train_loss : 0.5356184621651967 Val loss:  0.6137617231739891\n",
      "Epoch : 477 train_loss : 0.5352725307146708 Val loss:  0.600806533826722\n",
      "Epoch : 478 train_loss : 0.5330468853314717 Val loss:  0.6031038122375807\n",
      "Epoch : 479 train_loss : 0.5307141522566478 Val loss:  0.6146436895264519\n",
      "Epoch : 480 train_loss : 0.5323520064353943 Val loss:  0.6062552880578571\n",
      "Epoch : 481 train_loss : 0.5356223940849304 Val loss:  0.6194960155420833\n",
      "Epoch : 482 train_loss : 0.5387914478778839 Val loss:  0.6115375800265207\n",
      "Epoch : 483 train_loss : 0.5436649362246195 Val loss:  0.608699922296736\n",
      "Epoch : 484 train_loss : 0.5405061324437459 Val loss:  0.602258338464631\n",
      "Epoch : 485 train_loss : 0.5400439361731212 Val loss:  0.6153155359625816\n",
      "Epoch : 486 train_loss : 0.5418341279029846 Val loss:  0.595729173289405\n",
      "Epoch : 487 train_loss : 0.5583766758441925 Val loss:  0.6168708285358218\n",
      "Epoch : 488 train_loss : 0.546814109881719 Val loss:  0.5959843940205044\n",
      "Epoch : 489 train_loss : 0.5418438255786896 Val loss:  0.639934680097633\n",
      "Epoch : 490 train_loss : 0.542151673634847 Val loss:  0.6080276538266075\n",
      "Epoch : 491 train_loss : 0.5432168861230214 Val loss:  0.6451221842567125\n",
      "Epoch : 492 train_loss : 0.6110159556070963 Val loss:  0.6916130166252454\n",
      "Epoch : 493 train_loss : 0.5614136417706808 Val loss:  0.6004502067797713\n",
      "Epoch : 494 train_loss : 0.5529624780019124 Val loss:  0.6144458706180255\n",
      "Epoch : 495 train_loss : 0.5394673844178518 Val loss:  0.60488876024882\n",
      "Epoch : 496 train_loss : 0.5343454718589783 Val loss:  0.6025071677565575\n",
      "Epoch : 497 train_loss : 0.5315210680166881 Val loss:  0.6087896529171203\n",
      "Epoch : 498 train_loss : 0.5315121074517568 Val loss:  0.6022085443470213\n",
      "Epoch : 499 train_loss : 0.5302343765894572 Val loss:  0.6163251401980718\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "total_epochs+=epochs\n",
    "\n",
    "for e in tnrange(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    val_loss_1=0\n",
    "    val_loss_sum = 0\n",
    "    \n",
    "    for i in range(len(train_batch)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = criterion(output, label_batch[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for j in range(len(val_batch)):\n",
    "                \n",
    "                val_output = model(val_batch[j])\n",
    "                val_loss =  criterion(val_output, val_label_batch[j])\n",
    "                val_loss_1+=val_loss.item()\n",
    "        val_loss_sum=val_loss_1/len(val_batch)\n",
    "        \n",
    "        \n",
    "    print(\"Epoch :\", e, \"train_loss :\", train_loss/len(train_batch), \"Val loss: \", val_loss_sum/len(val_batch))    \n",
    "    val_losses.append(val_loss_sum/len(val_batch))    \n",
    "    train_losses.append(train_loss/len(train_batch))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGxKLiV0suA8"
   },
   "source": [
    "### Training vs validation loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "g3uZMDqrE2Iu",
    "outputId": "b4191f91-8f2e-4ff4-d86b-b1b2b58dc15b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19d7756efa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frm=10 # does not \n",
    "plt.plot(train_losses[frm:], label='Training loss')\n",
    "plt.plot(val_losses[frm:], label='Validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdxctDz8tECj"
   },
   "source": [
    "### Performance on test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CRm4OS3IRq22",
    "outputId": "0a7b802e-e68a-4af0-dd6b-25a64bef8bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.23715415019763 %\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "i=0\n",
    "res=[]\n",
    "with torch.no_grad():\n",
    "    for batch in test_batch :\n",
    "        for j in range(len(batch)):\n",
    "            x = model(batch[j])\n",
    "            res.append(round(x.item()))\n",
    "\n",
    "true_labels = list(test[\"quality\"])\n",
    "\n",
    "for i in range(len(res)):\n",
    "    if res[i]==int(true_labels[i]):\n",
    "        correct+=1\n",
    "        \n",
    "print(\"Accuracy:\", 100 * (correct/len(res)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Wine_Connoisseur.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
